{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIWPG5P967sp",
        "outputId": "c1bec66d-9423-4398-b6c3-f7d6b9e6b02c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (2.6.4)\n",
            "Requirement already satisfied: mujoco in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.10/dist-packages (2.9.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco) (1.6.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco) (3.1.7)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.0.7)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.5.3)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (10.0.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2023.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.1.8)\n",
            "Requirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.28.1)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.3.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.19.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.11.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.9.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (13.7.0)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (0.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (6.1.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco) (3.17.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.15.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (2.16.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (1.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->ray[rllib]) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym matplotlib glfw mujoco ray[rllib]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpL72ezg3POw",
        "outputId": "4c4ec5a3-0ab5-40a5-87b4-17ba9ff108b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/flax/configurations.py:42: DeprecationWarning: jax.config.define_bool_state is deprecated. Please use other libraries for configuration instead.\n",
            "  return jax_config.define_bool_state('flax_' + name, default, help)\n",
            "/usr/local/lib/python3.10/dist-packages/flax/linen/activation.py:36: DeprecationWarning: jax.nn.normalize is deprecated. Use jax.nn.standardize instead.\n",
            "  from jax.nn import normalize\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "2023-12-21 16:49:09,222\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2023-12-21 16:49:10,952\tWARNING deprecation.py:50 -- DeprecationWarning: `rllib/algorithms/simple_q/` has been deprecated. Use `rllib_contrib/simple_q/` instead. This will raise an error in the future!\n",
            "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:483: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "`UnifiedLogger` will be removed in Ray 2.7.\n",
            "  return UnifiedLogger(config, logdir, loggers=None)\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "2023-12-21 16:49:12,747\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "agent_timesteps_total: 100\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.011599063873291016\n",
            "  StateBufferConnector_ms: 0.007033348083496094\n",
            "  ViewRequirementAgentConnector_ms: 1.1858105659484863\n",
            "counters:\n",
            "  num_agent_steps_sampled: 100\n",
            "  num_agent_steps_trained: 0\n",
            "  num_env_steps_sampled: 100\n",
            "  num_env_steps_trained: 0\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-49-13\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -42.27276631577412\n",
            "episode_reward_mean: -47.61032826383281\n",
            "episode_reward_min: -52.9478902118915\n",
            "episodes_this_iter: 2\n",
            "episodes_total: 2\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  learner: {}\n",
            "  num_agent_steps_sampled: 100\n",
            "  num_agent_steps_trained: 0\n",
            "  num_env_steps_sampled: 100\n",
            "  num_env_steps_trained: 0\n",
            "iterations_since_restore: 1\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 100\n",
            "num_agent_steps_trained: 0\n",
            "num_env_steps_sampled: 100\n",
            "num_env_steps_sampled_this_iter: 100\n",
            "num_env_steps_sampled_throughput_per_sec: 95.10089055959666\n",
            "num_env_steps_trained: 0\n",
            "num_env_steps_trained_this_iter: 0\n",
            "num_env_steps_trained_throughput_per_sec: 0.0\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 0\n",
            "perf:\n",
            "  cpu_util_percent: 93.0\n",
            "  ram_util_percent: 16.1\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.7143634380680499\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 1.0705017807460067\n",
            "  mean_inference_ms: 5.8355048151299505\n",
            "  mean_raw_obs_processing_ms: 2.148642398343228\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.011599063873291016\n",
            "    StateBufferConnector_ms: 0.007033348083496094\n",
            "    ViewRequirementAgentConnector_ms: 1.1858105659484863\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -42.27276631577412\n",
            "  episode_reward_mean: -47.61032826383281\n",
            "  episode_reward_min: -52.9478902118915\n",
            "  episodes_this_iter: 2\n",
            "  hist_stats:\n",
            "    episode_lengths:\n",
            "    - 50\n",
            "    - 50\n",
            "    episode_reward:\n",
            "    - -52.9478902118915\n",
            "    - -42.27276631577412\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.7143634380680499\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 1.0705017807460067\n",
            "    mean_inference_ms: 5.8355048151299505\n",
            "    mean_raw_obs_processing_ms: 2.148642398343228\n",
            "time_since_restore: 1.0519967079162598\n",
            "time_this_iter_s: 1.0519967079162598\n",
            "time_total_s: 1.0519967079162598\n",
            "timers:\n",
            "  sample_time_ms: 505.649\n",
            "  training_iteration_time_ms: 525.722\n",
            "timestamp: 1703177353\n",
            "timesteps_total: 100\n",
            "training_iteration: 1\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 300\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.012461344401041666\n",
            "  StateBufferConnector_ms: 0.007720788319905599\n",
            "  ViewRequirementAgentConnector_ms: 0.5163033803304037\n",
            "counters:\n",
            "  num_agent_steps_sampled: 300\n",
            "  num_agent_steps_trained: 0\n",
            "  num_env_steps_sampled: 300\n",
            "  num_env_steps_trained: 0\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-49-15\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -35.809440612150325\n",
            "episode_reward_mean: -44.29166832993449\n",
            "episode_reward_min: -52.9478902118915\n",
            "episodes_this_iter: 4\n",
            "episodes_total: 6\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  learner: {}\n",
            "  num_agent_steps_sampled: 300\n",
            "  num_agent_steps_trained: 0\n",
            "  num_env_steps_sampled: 300\n",
            "  num_env_steps_trained: 0\n",
            "iterations_since_restore: 2\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 300\n",
            "num_agent_steps_trained: 0\n",
            "num_env_steps_sampled: 300\n",
            "num_env_steps_sampled_this_iter: 200\n",
            "num_env_steps_sampled_throughput_per_sec: 176.03772994940647\n",
            "num_env_steps_trained: 0\n",
            "num_env_steps_trained_this_iter: 0\n",
            "num_env_steps_trained_throughput_per_sec: 0.0\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 0\n",
            "perf:\n",
            "  cpu_util_percent: 100.0\n",
            "  ram_util_percent: 16.2\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.5992843687992462\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.7198833759699733\n",
            "  mean_inference_ms: 4.848762798843659\n",
            "  mean_raw_obs_processing_ms: 1.5727947931057642\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.012461344401041666\n",
            "    StateBufferConnector_ms: 0.007720788319905599\n",
            "    ViewRequirementAgentConnector_ms: 0.5163033803304037\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -35.809440612150325\n",
            "  episode_reward_mean: -44.29166832993449\n",
            "  episode_reward_min: -52.9478902118915\n",
            "  episodes_this_iter: 4\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-52.9478902118915, -42.27276631577412, -35.809440612150325, -42.78687249875137,\n",
            "      -48.226936123851296, -43.70610421718831]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.5992843687992462\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.7198833759699733\n",
            "    mean_inference_ms: 4.848762798843659\n",
            "    mean_raw_obs_processing_ms: 1.5727947931057642\n",
            "time_since_restore: 2.188556432723999\n",
            "time_this_iter_s: 1.1365597248077393\n",
            "time_total_s: 2.188556432723999\n",
            "timers:\n",
            "  sample_time_ms: 344.736\n",
            "  training_iteration_time_ms: 364.575\n",
            "timestamp: 1703177355\n",
            "timesteps_total: 300\n",
            "training_iteration: 2\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 450\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.02357959747314453\n",
            "  StateBufferConnector_ms: 0.0076664818657769095\n",
            "  ViewRequirementAgentConnector_ms: 0.7121827867296007\n",
            "counters:\n",
            "  num_agent_steps_sampled: 450\n",
            "  num_agent_steps_trained: 0\n",
            "  num_env_steps_sampled: 450\n",
            "  num_env_steps_trained: 0\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-49-16\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -28.847951561839505\n",
            "episode_reward_mean: -42.78435424312201\n",
            "episode_reward_min: -52.9478902118915\n",
            "episodes_this_iter: 3\n",
            "episodes_total: 9\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  learner: {}\n",
            "  num_agent_steps_sampled: 450\n",
            "  num_agent_steps_trained: 0\n",
            "  num_env_steps_sampled: 450\n",
            "  num_env_steps_trained: 0\n",
            "iterations_since_restore: 3\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 450\n",
            "num_agent_steps_trained: 0\n",
            "num_env_steps_sampled: 450\n",
            "num_env_steps_sampled_this_iter: 150\n",
            "num_env_steps_sampled_throughput_per_sec: 118.61375065656088\n",
            "num_env_steps_trained: 0\n",
            "num_env_steps_trained_this_iter: 0\n",
            "num_env_steps_trained_throughput_per_sec: 0.0\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 0\n",
            "perf:\n",
            "  cpu_util_percent: 100.0\n",
            "  ram_util_percent: 16.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.6131951233276899\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.6742131279162726\n",
            "  mean_inference_ms: 4.736280943922972\n",
            "  mean_raw_obs_processing_ms: 1.5074546130421096\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.02357959747314453\n",
            "    StateBufferConnector_ms: 0.0076664818657769095\n",
            "    ViewRequirementAgentConnector_ms: 0.7121827867296007\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -28.847951561839505\n",
            "  episode_reward_mean: -42.78435424312201\n",
            "  episode_reward_min: -52.9478902118915\n",
            "  episodes_this_iter: 3\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-52.9478902118915, -42.27276631577412, -35.809440612150325, -42.78687249875137,\n",
            "      -48.226936123851296, -43.70610421718831, -28.847951561839505, -50.226912924731465,\n",
            "      -40.234313721920174]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.6131951233276899\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.6742131279162726\n",
            "    mean_inference_ms: 4.736280943922972\n",
            "    mean_raw_obs_processing_ms: 1.5074546130421096\n",
            "time_since_restore: 3.4536778926849365\n",
            "time_this_iter_s: 1.2651214599609375\n",
            "time_total_s: 3.4536778926849365\n",
            "timers:\n",
            "  sample_time_ms: 362.551\n",
            "  training_iteration_time_ms: 383.553\n",
            "timestamp: 1703177356\n",
            "timesteps_total: 450\n",
            "training_iteration: 3\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 650\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.019566829387958232\n",
            "  StateBufferConnector_ms: 0.007400145897498498\n",
            "  ViewRequirementAgentConnector_ms: 0.5627265343299279\n",
            "counters:\n",
            "  num_agent_steps_sampled: 650\n",
            "  num_agent_steps_trained: 0\n",
            "  num_env_steps_sampled: 650\n",
            "  num_env_steps_trained: 0\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-49-17\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -28.847951561839505\n",
            "episode_reward_mean: -41.984763273770305\n",
            "episode_reward_min: -52.9478902118915\n",
            "episodes_this_iter: 4\n",
            "episodes_total: 13\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  learner: {}\n",
            "  num_agent_steps_sampled: 650\n",
            "  num_agent_steps_trained: 0\n",
            "  num_env_steps_sampled: 650\n",
            "  num_env_steps_trained: 0\n",
            "iterations_since_restore: 4\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 650\n",
            "num_agent_steps_trained: 0\n",
            "num_env_steps_sampled: 650\n",
            "num_env_steps_sampled_this_iter: 200\n",
            "num_env_steps_sampled_throughput_per_sec: 189.2271506392044\n",
            "num_env_steps_trained: 0\n",
            "num_env_steps_trained_this_iter: 0\n",
            "num_env_steps_trained_throughput_per_sec: 0.0\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 0\n",
            "perf:\n",
            "  cpu_util_percent: 100.0\n",
            "  ram_util_percent: 16.5\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.6245285199595597\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.6267129251164208\n",
            "  mean_inference_ms: 4.556110108701903\n",
            "  mean_raw_obs_processing_ms: 1.3968260691104901\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.019566829387958232\n",
            "    StateBufferConnector_ms: 0.007400145897498498\n",
            "    ViewRequirementAgentConnector_ms: 0.5627265343299279\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -28.847951561839505\n",
            "  episode_reward_mean: -41.984763273770305\n",
            "  episode_reward_min: -52.9478902118915\n",
            "  episodes_this_iter: 4\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-52.9478902118915, -42.27276631577412, -35.809440612150325, -42.78687249875137,\n",
            "      -48.226936123851296, -43.70610421718831, -28.847951561839505, -50.226912924731465,\n",
            "      -40.234313721920174, -39.47923562372334, -44.223342210371115, -36.07944203858264,\n",
            "      -40.960714498238794]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.6245285199595597\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.6267129251164208\n",
            "    mean_inference_ms: 4.556110108701903\n",
            "    mean_raw_obs_processing_ms: 1.3968260691104901\n",
            "time_since_restore: 4.511234283447266\n",
            "time_this_iter_s: 1.057556390762329\n",
            "time_total_s: 4.511234283447266\n",
            "timers:\n",
            "  sample_time_ms: 298.018\n",
            "  training_iteration_time_ms: 314.881\n",
            "timestamp: 1703177357\n",
            "timesteps_total: 650\n",
            "training_iteration: 4\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 1200\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.013500452041625977\n",
            "  StateBufferConnector_ms: 0.006363789240519206\n",
            "  ViewRequirementAgentConnector_ms: 0.35899778207143146\n",
            "counters:\n",
            "  num_agent_steps_sampled: 1200\n",
            "  num_agent_steps_trained: 0\n",
            "  num_env_steps_sampled: 1200\n",
            "  num_env_steps_trained: 0\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-49-18\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -3.9582265024007572\n",
            "episode_reward_mean: -36.60504141109596\n",
            "episode_reward_min: -52.9478902118915\n",
            "episodes_this_iter: 11\n",
            "episodes_total: 24\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  learner: {}\n",
            "  num_agent_steps_sampled: 1200\n",
            "  num_agent_steps_trained: 0\n",
            "  num_env_steps_sampled: 1200\n",
            "  num_env_steps_trained: 0\n",
            "iterations_since_restore: 5\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 1200\n",
            "num_agent_steps_trained: 0\n",
            "num_env_steps_sampled: 1200\n",
            "num_env_steps_sampled_this_iter: 550\n",
            "num_env_steps_sampled_throughput_per_sec: 503.51821686230505\n",
            "num_env_steps_trained: 0\n",
            "num_env_steps_trained_this_iter: 0\n",
            "num_env_steps_trained_throughput_per_sec: 0.0\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 0\n",
            "perf:\n",
            "  cpu_util_percent: 69.85\n",
            "  ram_util_percent: 16.7\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.5486164407206445\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.5011762985586767\n",
            "  mean_inference_ms: 3.7461616643035565\n",
            "  mean_raw_obs_processing_ms: 1.1090282207656081\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.013500452041625977\n",
            "    StateBufferConnector_ms: 0.006363789240519206\n",
            "    ViewRequirementAgentConnector_ms: 0.35899778207143146\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -3.9582265024007572\n",
            "  episode_reward_mean: -36.60504141109596\n",
            "  episode_reward_min: -52.9478902118915\n",
            "  episodes_this_iter: 11\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-52.9478902118915, -42.27276631577412, -35.809440612150325, -42.78687249875137,\n",
            "      -48.226936123851296, -43.70610421718831, -28.847951561839505, -50.226912924731465,\n",
            "      -40.234313721920174, -39.47923562372334, -44.223342210371115, -36.07944203858264,\n",
            "      -40.960714498238794, -36.65349946581253, -36.631519261006545, -43.150293781754364,\n",
            "      -42.43728396156587, -47.21215085656128, -38.07798161304542, -40.451651080648176,\n",
            "      -15.697504751592119, -17.585187740726337, -10.863772292175815, -3.9582265024007572]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.5486164407206445\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.5011762985586767\n",
            "    mean_inference_ms: 3.7461616643035565\n",
            "    mean_raw_obs_processing_ms: 1.1090282207656081\n",
            "time_since_restore: 5.604034423828125\n",
            "time_this_iter_s: 1.0928001403808594\n",
            "time_total_s: 5.604034423828125\n",
            "timers:\n",
            "  sample_time_ms: 95.314\n",
            "  training_iteration_time_ms: 99.056\n",
            "timestamp: 1703177358\n",
            "timesteps_total: 1200\n",
            "training_iteration: 5\n",
            "trial_id: default\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-21 16:49:19,345\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        0.2850484848022461, 0.362335205078125, 0.41675758361816406, 0.680537223815918,\n",
            "        0.029700279235839844, 0.3155384063720703, 0.34917640686035156, 0.2590975761413574,\n",
            "        0.12436771392822266, 0.0751352310180664, 0.23122692108154297, 0.14208221435546875]\n",
            "  num_agent_steps_sampled: 176200\n",
            "  num_agent_steps_trained: 111808\n",
            "  num_env_steps_sampled: 176200\n",
            "  num_env_steps_trained: 111808\n",
            "  num_target_updates: 3494\n",
            "iterations_since_restore: 469\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 176200\n",
            "num_agent_steps_trained: 111808\n",
            "num_env_steps_sampled: 176200\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 412.8444304163777\n",
            "num_env_steps_trained: 111808\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 264.2204354664817\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 56.1\n",
            "  ram_util_percent: 21.3\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25626566573701015\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.178844542877193\n",
            "  mean_inference_ms: 1.2832058710987702\n",
            "  mean_raw_obs_processing_ms: 0.3677303602294124\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006700754165649414\n",
            "    StateBufferConnector_ms: 0.004994630813598633\n",
            "    ViewRequirementAgentConnector_ms: 0.1337721347808838\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.811581609157007\n",
            "  episode_reward_mean: -11.914582306861902\n",
            "  episode_reward_min: -16.993743828477378\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-9.717843117719587, -13.394068957508035, -12.683179536208144,\n",
            "      -13.089148054321203, -10.705591648197682, -16.952923787397527, -10.742126619697098,\n",
            "      -14.648132466439728, -15.76087054139353, -11.804668023846114, -16.993743828477378,\n",
            "      -9.565128331620596, -11.60695901336051, -10.209699493321528, -11.521016841041092,\n",
            "      -9.765619552766264, -10.335814635511916, -11.348333679015605, -11.324197577346911,\n",
            "      -9.842232063947094, -13.638772331845436, -11.438809784845374, -11.967304029631906,\n",
            "      -9.213103155094196, -10.369781693478643, -14.120277943498312, -9.026239131910971,\n",
            "      -14.08479564300754, -12.338829960743684, -11.849552979526734, -10.370395815430019,\n",
            "      -11.753742817566552, -10.630773726226511, -14.263831154005054, -13.846620150316324,\n",
            "      -12.126319905868984, -10.867226134647142, -11.885070226640817, -10.952615943210938,\n",
            "      -12.397296039616156, -14.348876664953949, -10.469279430335924, -12.550867437519926,\n",
            "      -12.948255258634058, -12.844806230633733, -9.869071947825871, -14.799725681950145,\n",
            "      -15.217906908851694, -13.396100065550439, -9.825872026926868, -12.80790931670308,\n",
            "      -8.460180194190139, -7.895667880116643, -14.9559007999179, -12.04587172709983,\n",
            "      -11.500113366012776, -8.628475998121827, -12.302732576037618, -14.797011253282088,\n",
            "      -9.374144805207553, -11.005801486326542, -12.01544815007982, -10.224082892379098,\n",
            "      -12.709105769385685, -7.890350068098079, -12.43601158759242, -14.151613620031116,\n",
            "      -9.542861251244908, -12.12814643800162, -15.546500978140484, -10.666178171774728,\n",
            "      -9.10352998221069, -12.95702785772355, -14.82836910969135, -8.426565987531134,\n",
            "      -13.837848810127497, -11.868366860923198, -15.840938386980476, -14.775156967801474,\n",
            "      -10.829532179532166, -16.576630769717077, -15.19246877064802, -11.267033004655092,\n",
            "      -11.865422840929678, -13.46048762823312, -12.277969371310444, -9.1464936144288,\n",
            "      -11.182424475530023, -11.477762184825, -8.604758302135476, -13.49267102762119,\n",
            "      -11.071449216866686, -11.112009692183571, -12.712559331198094, -6.811581609157007,\n",
            "      -13.413294460526679, -7.215254320872949, -10.456773102052791, -12.208323821574268,\n",
            "      -12.938000680027411]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25626566573701015\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.178844542877193\n",
            "    mean_inference_ms: 1.2832058710987702\n",
            "    mean_raw_obs_processing_ms: 0.3677303602294124\n",
            "time_since_restore: 501.6307442188263\n",
            "time_this_iter_s: 1.0910706520080566\n",
            "time_total_s: 501.6307442188263\n",
            "timers:\n",
            "  learn_throughput: 2301.618\n",
            "  learn_time_ms: 13.903\n",
            "  load_throughput: 178647.315\n",
            "  load_time_ms: 0.179\n",
            "  sample_time_ms: 89.986\n",
            "  synch_weights_time_ms: 0.026\n",
            "  training_iteration_time_ms: 121.492\n",
            "timestamp: 1703177877\n",
            "timesteps_total: 176200\n",
            "training_iteration: 469\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 176600\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006728410720825195\n",
            "  StateBufferConnector_ms: 0.00501561164855957\n",
            "  ViewRequirementAgentConnector_ms: 0.13559937477111816\n",
            "counters:\n",
            "  last_target_update_ts: 176600\n",
            "  num_agent_steps_sampled: 176600\n",
            "  num_agent_steps_trained: 112064\n",
            "  num_env_steps_sampled: 176600\n",
            "  num_env_steps_trained: 112064\n",
            "  num_target_updates: 3502\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-57-58\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.794629504820062\n",
            "episode_reward_min: -16.993743828477378\n",
            "episodes_this_iter: 8\n",
            "episodes_total: 3532\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 176600\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1744.71875\n",
            "      learner_stats:\n",
            "        actor_loss: -20.55973243713379\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1399688720703125\n",
            "        alpha_value: 0.7058840990066528\n",
            "        critic_loss: 0.04511760175228119\n",
            "        grad_gnorm: 3.272911310195923\n",
            "        log_alpha_value: -0.3483041822910309\n",
            "        max_q: 22.99579429626465\n",
            "        mean_q: 19.923545837402344\n",
            "        min_q: 16.816913604736328\n",
            "        policy_t: -0.19034284353256226\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.20284083485603333\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3502.0\n",
            "      td_error: [0.14852142333984375, 0.17759037017822266, 0.4665374755859375, 0.1144723892211914,\n",
            "        0.07150650024414062, 1.193436622619629, 0.1732330322265625, 0.1689157485961914,\n",
            "        0.0728616714477539, 0.295867919921875, 0.1552143096923828, 0.03923797607421875,\n",
            "        0.10234355926513672, 0.3527841567993164, 0.0239715576171875, 0.22056293487548828,\n",
            "        0.39232349395751953, 0.1196298599243164, 0.034974098205566406, 0.09699630737304688,\n",
            "        0.4179859161376953, 0.20442676544189453, 0.07319068908691406, 0.1241302490234375,\n",
            "        0.3603782653808594, 0.22737884521484375, 0.07091426849365234, 0.18593215942382812,\n",
            "        0.07855701446533203, 0.11121082305908203, 0.10918235778808594, 0.10663795471191406]\n",
            "  num_agent_steps_sampled: 176600\n",
            "  num_agent_steps_trained: 112064\n",
            "  num_env_steps_sampled: 176600\n",
            "  num_env_steps_trained: 112064\n",
            "  num_target_updates: 3502\n",
            "iterations_since_restore: 470\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 176600\n",
            "num_agent_steps_trained: 112064\n",
            "num_env_steps_sampled: 176600\n",
            "num_env_steps_sampled_this_iter: 400\n",
            "num_env_steps_sampled_throughput_per_sec: 378.03789658880225\n",
            "num_env_steps_trained: 112064\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_env_steps_trained_throughput_per_sec: 241.94425381683345\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 54.35\n",
            "  ram_util_percent: 21.3\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2562437127058621\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17883589756331753\n",
            "  mean_inference_ms: 1.2830368840522643\n",
            "  mean_raw_obs_processing_ms: 0.367694504750215\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006728410720825195\n",
            "    StateBufferConnector_ms: 0.00501561164855957\n",
            "    ViewRequirementAgentConnector_ms: 0.13559937477111816\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.794629504820062\n",
            "  episode_reward_min: -16.993743828477378\n",
            "  episodes_this_iter: 8\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-15.76087054139353, -11.804668023846114, -16.993743828477378,\n",
            "      -9.565128331620596, -11.60695901336051, -10.209699493321528, -11.521016841041092,\n",
            "      -9.765619552766264, -10.335814635511916, -11.348333679015605, -11.324197577346911,\n",
            "      -9.842232063947094, -13.638772331845436, -11.438809784845374, -11.967304029631906,\n",
            "      -9.213103155094196, -10.369781693478643, -14.120277943498312, -9.026239131910971,\n",
            "      -14.08479564300754, -12.338829960743684, -11.849552979526734, -10.370395815430019,\n",
            "      -11.753742817566552, -10.630773726226511, -14.263831154005054, -13.846620150316324,\n",
            "      -12.126319905868984, -10.867226134647142, -11.885070226640817, -10.952615943210938,\n",
            "      -12.397296039616156, -14.348876664953949, -10.469279430335924, -12.550867437519926,\n",
            "      -12.948255258634058, -12.844806230633733, -9.869071947825871, -14.799725681950145,\n",
            "      -15.217906908851694, -13.396100065550439, -9.825872026926868, -12.80790931670308,\n",
            "      -8.460180194190139, -7.895667880116643, -14.9559007999179, -12.04587172709983,\n",
            "      -11.500113366012776, -8.628475998121827, -12.302732576037618, -14.797011253282088,\n",
            "      -9.374144805207553, -11.005801486326542, -12.01544815007982, -10.224082892379098,\n",
            "      -12.709105769385685, -7.890350068098079, -12.43601158759242, -14.151613620031116,\n",
            "      -9.542861251244908, -12.12814643800162, -15.546500978140484, -10.666178171774728,\n",
            "      -9.10352998221069, -12.95702785772355, -14.82836910969135, -8.426565987531134,\n",
            "      -13.837848810127497, -11.868366860923198, -15.840938386980476, -14.775156967801474,\n",
            "      -10.829532179532166, -16.576630769717077, -15.19246877064802, -11.267033004655092,\n",
            "      -11.865422840929678, -13.46048762823312, -12.277969371310444, -9.1464936144288,\n",
            "      -11.182424475530023, -11.477762184825, -8.604758302135476, -13.49267102762119,\n",
            "      -11.071449216866686, -11.112009692183571, -12.712559331198094, -6.811581609157007,\n",
            "      -13.413294460526679, -7.215254320872949, -10.456773102052791, -12.208323821574268,\n",
            "      -12.938000680027411, -12.95868617175637, -11.88083777302189, -11.762136603418604,\n",
            "      -6.705977731703049, -14.08938714283293, -9.150013534006034, -9.439512493063452,\n",
            "      -13.951182533502232]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2562437127058621\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17883589756331753\n",
            "    mean_inference_ms: 1.2830368840522643\n",
            "    mean_raw_obs_processing_ms: 0.367694504750215\n",
            "time_since_restore: 502.68996119499207\n",
            "time_this_iter_s: 1.0592169761657715\n",
            "time_total_s: 502.68996119499207\n",
            "timers:\n",
            "  learn_throughput: 2202.168\n",
            "  learn_time_ms: 14.531\n",
            "  load_throughput: 178102.081\n",
            "  load_time_ms: 0.18\n",
            "  sample_time_ms: 96.6\n",
            "  synch_weights_time_ms: 0.025\n",
            "  training_iteration_time_ms: 128.2\n",
            "timestamp: 1703177878\n",
            "timesteps_total: 176600\n",
            "training_iteration: 470\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 177050\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006669521331787109\n",
            "  StateBufferConnector_ms: 0.0049860477447509766\n",
            "  ViewRequirementAgentConnector_ms: 0.13425970077514648\n",
            "counters:\n",
            "  last_target_update_ts: 177050\n",
            "  num_agent_steps_sampled: 177050\n",
            "  num_agent_steps_trained: 112352\n",
            "  num_env_steps_sampled: 177050\n",
            "  num_env_steps_trained: 112352\n",
            "  num_target_updates: 3511\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-57-59\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.65781988729156\n",
            "episode_reward_min: -16.576630769717077\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3541\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 177050\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1560.5625\n",
            "      learner_stats:\n",
            "        actor_loss: -19.859928131103516\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1394857168197632\n",
            "        alpha_value: 0.7052616477012634\n",
            "        critic_loss: 0.08232714235782623\n",
            "        grad_gnorm: 3.2632594108581543\n",
            "        log_alpha_value: -0.34918639063835144\n",
            "        max_q: 21.97166633605957\n",
            "        mean_q: 19.13994026184082\n",
            "        min_q: 14.395684242248535\n",
            "        policy_t: -0.0673929825425148\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.3121069669723511\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3511.0\n",
            "      td_error: [0.18526363372802734, 0.24053096771240234, 0.06716156005859375, 0.08630180358886719,\n",
            "        0.3316164016723633, 0.10374832153320312, 0.1986684799194336, 0.41290950775146484,\n",
            "        0.1439046859741211, 0.29000091552734375, 0.4044628143310547, 0.4951362609863281,\n",
            "        0.11952972412109375, 0.5471878051757812, 0.4482240676879883, 0.2911500930786133,\n",
            "        0.19446277618408203, 0.24256420135498047, 0.1910238265991211, 0.37579917907714844,\n",
            "        0.4900484085083008, 0.13074398040771484, 0.1386585235595703, 1.622645378112793,\n",
            "        0.04608917236328125, 0.3411245346069336, 0.2443075180053711, 0.6319360733032227,\n",
            "        0.25041961669921875, 0.12894535064697266, 0.34163951873779297, 0.2512178421020508]\n",
            "  num_agent_steps_sampled: 177050\n",
            "  num_agent_steps_trained: 112352\n",
            "  num_env_steps_sampled: 177050\n",
            "  num_env_steps_trained: 112352\n",
            "  num_target_updates: 3511\n",
            "iterations_since_restore: 471\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 177050\n",
            "num_agent_steps_trained: 112352\n",
            "num_env_steps_sampled: 177050\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 408.82068556813573\n",
            "num_env_steps_trained: 112352\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 261.64523876360687\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 56.1\n",
            "  ram_util_percent: 21.3\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.256216009187716\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17882158039165588\n",
            "  mean_inference_ms: 1.2828203879740496\n",
            "  mean_raw_obs_processing_ms: 0.36764194664580496\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006669521331787109\n",
            "    StateBufferConnector_ms: 0.0049860477447509766\n",
            "    ViewRequirementAgentConnector_ms: 0.13425970077514648\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.65781988729156\n",
            "  episode_reward_min: -16.576630769717077\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-11.348333679015605, -11.324197577346911, -9.842232063947094,\n",
            "      -13.638772331845436, -11.438809784845374, -11.967304029631906, -9.213103155094196,\n",
            "      -10.369781693478643, -14.120277943498312, -9.026239131910971, -14.08479564300754,\n",
            "      -12.338829960743684, -11.849552979526734, -10.370395815430019, -11.753742817566552,\n",
            "      -10.630773726226511, -14.263831154005054, -13.846620150316324, -12.126319905868984,\n",
            "      -10.867226134647142, -11.885070226640817, -10.952615943210938, -12.397296039616156,\n",
            "      -14.348876664953949, -10.469279430335924, -12.550867437519926, -12.948255258634058,\n",
            "      -12.844806230633733, -9.869071947825871, -14.799725681950145, -15.217906908851694,\n",
            "      -13.396100065550439, -9.825872026926868, -12.80790931670308, -8.460180194190139,\n",
            "      -7.895667880116643, -14.9559007999179, -12.04587172709983, -11.500113366012776,\n",
            "      -8.628475998121827, -12.302732576037618, -14.797011253282088, -9.374144805207553,\n",
            "      -11.005801486326542, -12.01544815007982, -10.224082892379098, -12.709105769385685,\n",
            "      -7.890350068098079, -12.43601158759242, -14.151613620031116, -9.542861251244908,\n",
            "      -12.12814643800162, -15.546500978140484, -10.666178171774728, -9.10352998221069,\n",
            "      -12.95702785772355, -14.82836910969135, -8.426565987531134, -13.837848810127497,\n",
            "      -11.868366860923198, -15.840938386980476, -14.775156967801474, -10.829532179532166,\n",
            "      -16.576630769717077, -15.19246877064802, -11.267033004655092, -11.865422840929678,\n",
            "      -13.46048762823312, -12.277969371310444, -9.1464936144288, -11.182424475530023,\n",
            "      -11.477762184825, -8.604758302135476, -13.49267102762119, -11.071449216866686,\n",
            "      -11.112009692183571, -12.712559331198094, -6.811581609157007, -13.413294460526679,\n",
            "      -7.215254320872949, -10.456773102052791, -12.208323821574268, -12.938000680027411,\n",
            "      -12.95868617175637, -11.88083777302189, -11.762136603418604, -6.705977731703049,\n",
            "      -14.08938714283293, -9.150013534006034, -9.439512493063452, -13.951182533502232,\n",
            "      -12.05665465774798, -10.608063036244266, -14.777214783631942, -8.130244594842775,\n",
            "      -9.36960500636909, -6.872272505312526, -10.766278472460701, -7.929009053458728,\n",
            "      -13.373216398421135]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.256216009187716\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17882158039165588\n",
            "    mean_inference_ms: 1.2828203879740496\n",
            "    mean_raw_obs_processing_ms: 0.36764194664580496\n",
            "time_since_restore: 503.7917568683624\n",
            "time_this_iter_s: 1.1017956733703613\n",
            "time_total_s: 503.7917568683624\n",
            "timers:\n",
            "  learn_throughput: 2361.496\n",
            "  learn_time_ms: 13.551\n",
            "  load_throughput: 162216.253\n",
            "  load_time_ms: 0.197\n",
            "  sample_time_ms: 93.132\n",
            "  synch_weights_time_ms: 0.024\n",
            "  training_iteration_time_ms: 124.051\n",
            "timestamp: 1703177879\n",
            "timesteps_total: 177050\n",
            "training_iteration: 471\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 177400\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006679058074951172\n",
            "  StateBufferConnector_ms: 0.004942893981933594\n",
            "  ViewRequirementAgentConnector_ms: 0.1327681541442871\n",
            "counters:\n",
            "  last_target_update_ts: 177400\n",
            "  num_agent_steps_sampled: 177400\n",
            "  num_agent_steps_trained: 112576\n",
            "  num_env_steps_sampled: 177400\n",
            "  num_env_steps_trained: 112576\n",
            "  num_target_updates: 3518\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-00\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.636753175955427\n",
            "episode_reward_min: -16.576630769717077\n",
            "episodes_this_iter: 7\n",
            "episodes_total: 3548\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 177400\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1879.40625\n",
            "      learner_stats:\n",
            "        actor_loss: -20.290225982666016\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.182592749595642\n",
            "        alpha_value: 0.7047761678695679\n",
            "        critic_loss: 0.05475971847772598\n",
            "        grad_gnorm: 3.3800435066223145\n",
            "        log_alpha_value: -0.3498750329017639\n",
            "        max_q: 21.898515701293945\n",
            "        mean_q: 19.525325775146484\n",
            "        min_q: 17.160423278808594\n",
            "        policy_t: 0.008038721978664398\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.2394755482673645\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3518.0\n",
            "      td_error: [0.37519073486328125, 0.2811126708984375, 0.29015350341796875, 0.12151908874511719,\n",
            "        0.13655948638916016, 0.9986104965209961, 0.1874523162841797, 0.07761383056640625,\n",
            "        0.19404125213623047, 0.06187152862548828, 0.8281021118164062, 0.023131370544433594,\n",
            "        0.5153274536132812, 0.0749969482421875, 0.45110607147216797, 0.22472763061523438,\n",
            "        0.27121829986572266, 0.11562919616699219, 0.050566673278808594, 0.38973331451416016,\n",
            "        0.18702316284179688, 0.15515613555908203, 0.17053794860839844, 0.24105167388916016,\n",
            "        0.02364063262939453, 0.15544509887695312, 0.3144550323486328, 0.29293060302734375,\n",
            "        0.010048866271972656, 0.15879249572753906, 0.20519351959228516, 0.08027839660644531]\n",
            "  num_agent_steps_sampled: 177400\n",
            "  num_agent_steps_trained: 112576\n",
            "  num_env_steps_sampled: 177400\n",
            "  num_env_steps_trained: 112576\n",
            "  num_target_updates: 3518\n",
            "iterations_since_restore: 472\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 177400\n",
            "num_agent_steps_trained: 112576\n",
            "num_env_steps_sampled: 177400\n",
            "num_env_steps_sampled_this_iter: 350\n",
            "num_env_steps_sampled_throughput_per_sec: 339.8269158874178\n",
            "num_env_steps_trained: 112576\n",
            "num_env_steps_trained_this_iter: 224\n",
            "num_env_steps_trained_throughput_per_sec: 217.48922616794738\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 224\n",
            "perf:\n",
            "  cpu_util_percent: 65.25\n",
            "  ram_util_percent: 21.3\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25619261109223596\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17880663050584125\n",
            "  mean_inference_ms: 1.2826383776421\n",
            "  mean_raw_obs_processing_ms: 0.36759478253048966\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006679058074951172\n",
            "    StateBufferConnector_ms: 0.004942893981933594\n",
            "    ViewRequirementAgentConnector_ms: 0.1327681541442871\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.636753175955427\n",
            "  episode_reward_min: -16.576630769717077\n",
            "  episodes_this_iter: 7\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-10.369781693478643, -14.120277943498312, -9.026239131910971,\n",
            "      -14.08479564300754, -12.338829960743684, -11.849552979526734, -10.370395815430019,\n",
            "      -11.753742817566552, -10.630773726226511, -14.263831154005054, -13.846620150316324,\n",
            "      -12.126319905868984, -10.867226134647142, -11.885070226640817, -10.952615943210938,\n",
            "      -12.397296039616156, -14.348876664953949, -10.469279430335924, -12.550867437519926,\n",
            "      -12.948255258634058, -12.844806230633733, -9.869071947825871, -14.799725681950145,\n",
            "      -15.217906908851694, -13.396100065550439, -9.825872026926868, -12.80790931670308,\n",
            "      -8.460180194190139, -7.895667880116643, -14.9559007999179, -12.04587172709983,\n",
            "      -11.500113366012776, -8.628475998121827, -12.302732576037618, -14.797011253282088,\n",
            "      -9.374144805207553, -11.005801486326542, -12.01544815007982, -10.224082892379098,\n",
            "      -12.709105769385685, -7.890350068098079, -12.43601158759242, -14.151613620031116,\n",
            "      -9.542861251244908, -12.12814643800162, -15.546500978140484, -10.666178171774728,\n",
            "      -9.10352998221069, -12.95702785772355, -14.82836910969135, -8.426565987531134,\n",
            "      -13.837848810127497, -11.868366860923198, -15.840938386980476, -14.775156967801474,\n",
            "      -10.829532179532166, -16.576630769717077, -15.19246877064802, -11.267033004655092,\n",
            "      -11.865422840929678, -13.46048762823312, -12.277969371310444, -9.1464936144288,\n",
            "      -11.182424475530023, -11.477762184825, -8.604758302135476, -13.49267102762119,\n",
            "      -11.071449216866686, -11.112009692183571, -12.712559331198094, -6.811581609157007,\n",
            "      -13.413294460526679, -7.215254320872949, -10.456773102052791, -12.208323821574268,\n",
            "      -12.938000680027411, -12.95868617175637, -11.88083777302189, -11.762136603418604,\n",
            "      -6.705977731703049, -14.08938714283293, -9.150013534006034, -9.439512493063452,\n",
            "      -13.951182533502232, -12.05665465774798, -10.608063036244266, -14.777214783631942,\n",
            "      -8.130244594842775, -9.36960500636909, -6.872272505312526, -10.766278472460701,\n",
            "      -7.929009053458728, -13.373216398421135, -12.094972978989182, -10.533361932837124,\n",
            "      -9.861880684544484, -14.76786107475355, -9.637541311276973, -12.111276001648935,\n",
            "      -7.659187504063094]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25619261109223596\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17880663050584125\n",
            "    mean_inference_ms: 1.2826383776421\n",
            "    mean_raw_obs_processing_ms: 0.36759478253048966\n",
            "time_since_restore: 504.8232522010803\n",
            "time_this_iter_s: 1.0314953327178955\n",
            "time_total_s: 504.8232522010803\n",
            "timers:\n",
            "  learn_throughput: 2050.625\n",
            "  learn_time_ms: 15.605\n",
            "  load_throughput: 152312.447\n",
            "  load_time_ms: 0.21\n",
            "  sample_time_ms: 101.149\n",
            "  synch_weights_time_ms: 0.027\n",
            "  training_iteration_time_ms: 138.402\n",
            "timestamp: 1703177880\n",
            "timesteps_total: 177400\n",
            "training_iteration: 472\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 177750\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006586313247680664\n",
            "  StateBufferConnector_ms: 0.004919528961181641\n",
            "  ViewRequirementAgentConnector_ms: 0.1323380470275879\n",
            "counters:\n",
            "  last_target_update_ts: 177750\n",
            "  num_agent_steps_sampled: 177750\n",
            "  num_agent_steps_trained: 112800\n",
            "  num_env_steps_sampled: 177750\n",
            "  num_env_steps_trained: 112800\n",
            "  num_target_updates: 3525\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-02\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.561225089911458\n",
            "episode_reward_min: -16.576630769717077\n",
            "episodes_this_iter: 7\n",
            "episodes_total: 3555\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 177750\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1680.40625\n",
            "      learner_stats:\n",
            "        actor_loss: -19.61967658996582\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1189395189285278\n",
            "        alpha_value: 0.7042895555496216\n",
            "        critic_loss: 0.15671831369400024\n",
            "        grad_gnorm: 3.1918113231658936\n",
            "        log_alpha_value: -0.35056567192077637\n",
            "        max_q: 23.133037567138672\n",
            "        mean_q: 18.852718353271484\n",
            "        min_q: 6.494699478149414\n",
            "        policy_t: -0.20578637719154358\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.378347247838974\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3525.0\n",
            "      td_error: [0.06858253479003906, 0.015498161315917969, 0.20901870727539062, 0.045073509216308594,\n",
            "        0.036128997802734375, 0.45072078704833984, 0.22513484954833984, 0.3190336227416992,\n",
            "        0.035015106201171875, 0.5239086151123047, 0.1495189666748047, 0.6708450317382812,\n",
            "        0.3916597366333008, 0.07088947296142578, 0.07656574249267578, 0.5148496627807617,\n",
            "        1.7613694667816162, 0.3492422103881836, 0.327146053314209, 0.6832265853881836,\n",
            "        0.32773399353027344, 0.04926586151123047, 0.012217521667480469, 0.12448501586914062,\n",
            "        0.18631458282470703, 0.7574548721313477, 0.1704273223876953, 1.8719501495361328,\n",
            "        0.14114952087402344, 0.03742504119873047, 1.2409582138061523, 0.26430225372314453]\n",
            "  num_agent_steps_sampled: 177750\n",
            "  num_agent_steps_trained: 112800\n",
            "  num_env_steps_sampled: 177750\n",
            "  num_env_steps_trained: 112800\n",
            "  num_target_updates: 3525\n",
            "iterations_since_restore: 473\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 177750\n",
            "num_agent_steps_trained: 112800\n",
            "num_env_steps_sampled: 177750\n",
            "num_env_steps_sampled_this_iter: 350\n",
            "num_env_steps_sampled_throughput_per_sec: 294.1854145591486\n",
            "num_env_steps_trained: 112800\n",
            "num_env_steps_trained_this_iter: 224\n",
            "num_env_steps_trained_throughput_per_sec: 188.27866531785514\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 224\n",
            "perf:\n",
            "  cpu_util_percent: 99.65\n",
            "  ram_util_percent: 21.3\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2561703500095568\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17879087248408596\n",
            "  mean_inference_ms: 1.282457088309237\n",
            "  mean_raw_obs_processing_ms: 0.3675451271944088\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006586313247680664\n",
            "    StateBufferConnector_ms: 0.004919528961181641\n",
            "    ViewRequirementAgentConnector_ms: 0.1323380470275879\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.561225089911458\n",
            "  episode_reward_min: -16.576630769717077\n",
            "  episodes_this_iter: 7\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-11.753742817566552, -10.630773726226511, -14.263831154005054,\n",
            "      -13.846620150316324, -12.126319905868984, -10.867226134647142, -11.885070226640817,\n",
            "      -10.952615943210938, -12.397296039616156, -14.348876664953949, -10.469279430335924,\n",
            "      -12.550867437519926, -12.948255258634058, -12.844806230633733, -9.869071947825871,\n",
            "      -14.799725681950145, -15.217906908851694, -13.396100065550439, -9.825872026926868,\n",
            "      -12.80790931670308, -8.460180194190139, -7.895667880116643, -14.9559007999179,\n",
            "      -12.04587172709983, -11.500113366012776, -8.628475998121827, -12.302732576037618,\n",
            "      -14.797011253282088, -9.374144805207553, -11.005801486326542, -12.01544815007982,\n",
            "      -10.224082892379098, -12.709105769385685, -7.890350068098079, -12.43601158759242,\n",
            "      -14.151613620031116, -9.542861251244908, -12.12814643800162, -15.546500978140484,\n",
            "      -10.666178171774728, -9.10352998221069, -12.95702785772355, -14.82836910969135,\n",
            "      -8.426565987531134, -13.837848810127497, -11.868366860923198, -15.840938386980476,\n",
            "      -14.775156967801474, -10.829532179532166, -16.576630769717077, -15.19246877064802,\n",
            "      -11.267033004655092, -11.865422840929678, -13.46048762823312, -12.277969371310444,\n",
            "      -9.1464936144288, -11.182424475530023, -11.477762184825, -8.604758302135476,\n",
            "      -13.49267102762119, -11.071449216866686, -11.112009692183571, -12.712559331198094,\n",
            "      -6.811581609157007, -13.413294460526679, -7.215254320872949, -10.456773102052791,\n",
            "      -12.208323821574268, -12.938000680027411, -12.95868617175637, -11.88083777302189,\n",
            "      -11.762136603418604, -6.705977731703049, -14.08938714283293, -9.150013534006034,\n",
            "      -9.439512493063452, -13.951182533502232, -12.05665465774798, -10.608063036244266,\n",
            "      -14.777214783631942, -8.130244594842775, -9.36960500636909, -6.872272505312526,\n",
            "      -10.766278472460701, -7.929009053458728, -13.373216398421135, -12.094972978989182,\n",
            "      -10.533361932837124, -9.861880684544484, -14.76786107475355, -9.637541311276973,\n",
            "      -12.111276001648935, -7.659187504063094, -10.654072144021406, -10.662708288246701,\n",
            "      -12.157111678172402, -11.036723399105998, -9.017724652339835, -10.383344214753725,\n",
            "      -10.695380186558769]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2561703500095568\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17879087248408596\n",
            "    mean_inference_ms: 1.282457088309237\n",
            "    mean_raw_obs_processing_ms: 0.3675451271944088\n",
            "time_since_restore: 506.0145950317383\n",
            "time_this_iter_s: 1.191342830657959\n",
            "time_total_s: 506.0145950317383\n",
            "timers:\n",
            "  learn_throughput: 1733.781\n",
            "  learn_time_ms: 18.457\n",
            "  load_throughput: 119783.782\n",
            "  load_time_ms: 0.267\n",
            "  sample_time_ms: 123.469\n",
            "  synch_weights_time_ms: 0.031\n",
            "  training_iteration_time_ms: 168.432\n",
            "timestamp: 1703177882\n",
            "timesteps_total: 177750\n",
            "training_iteration: 473\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 178050\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.0065593719482421875\n",
            "  StateBufferConnector_ms: 0.004991054534912109\n",
            "  ViewRequirementAgentConnector_ms: 0.13432097434997559\n",
            "counters:\n",
            "  last_target_update_ts: 178050\n",
            "  num_agent_steps_sampled: 178050\n",
            "  num_agent_steps_trained: 112992\n",
            "  num_env_steps_sampled: 178050\n",
            "  num_env_steps_trained: 112992\n",
            "  num_target_updates: 3531\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-03\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.483249977011983\n",
            "episode_reward_min: -16.576630769717077\n",
            "episodes_this_iter: 6\n",
            "episodes_total: 3561\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 178050\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1919.46875\n",
            "      learner_stats:\n",
            "        actor_loss: -19.496566772460938\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1051385402679443\n",
            "        alpha_value: 0.7038753032684326\n",
            "        critic_loss: 0.08457823097705841\n",
            "        grad_gnorm: 3.1471614837646484\n",
            "        log_alpha_value: -0.35115405917167664\n",
            "        max_q: 21.91353988647461\n",
            "        mean_q: 18.804691314697266\n",
            "        min_q: 13.394657135009766\n",
            "        policy_t: -0.22320543229579926\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.3070189952850342\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3531.0\n",
            "      td_error: [0.16353893280029297, 0.4344453811645508, 0.6962671279907227, 0.3103160858154297,\n",
            "        0.36063098907470703, 0.12220001220703125, 0.17345333099365234, 0.1342182159423828,\n",
            "        0.08310890197753906, 0.5540437698364258, 0.1298828125, 0.02723217010498047,\n",
            "        0.7953839302062988, 0.17344093322753906, 0.29295921325683594, 0.1812915802001953,\n",
            "        0.16614437103271484, 0.19781208038330078, 0.08290767669677734, 0.16628456115722656,\n",
            "        0.48482227325439453, 0.1031808853149414, 0.22567272186279297, 0.11302661895751953,\n",
            "        0.4910287857055664, 0.7642831802368164, 0.9391851425170898, 0.5839681625366211,\n",
            "        0.12972450256347656, 0.04732704162597656, 0.10579299926757812, 0.5910329818725586]\n",
            "  num_agent_steps_sampled: 178050\n",
            "  num_agent_steps_trained: 112992\n",
            "  num_env_steps_sampled: 178050\n",
            "  num_env_steps_trained: 112992\n",
            "  num_target_updates: 3531\n",
            "iterations_since_restore: 474\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 178050\n",
            "num_agent_steps_trained: 112992\n",
            "num_env_steps_sampled: 178050\n",
            "num_env_steps_sampled_this_iter: 300\n",
            "num_env_steps_sampled_throughput_per_sec: 296.83028893208996\n",
            "num_env_steps_trained: 112992\n",
            "num_env_steps_trained_this_iter: 192\n",
            "num_env_steps_trained_throughput_per_sec: 189.97138491653757\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 192\n",
            "perf:\n",
            "  cpu_util_percent: 98.6\n",
            "  ram_util_percent: 21.3\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25615076159227285\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17877635984743734\n",
            "  mean_inference_ms: 1.2822941213253218\n",
            "  mean_raw_obs_processing_ms: 0.36750166109467114\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.0065593719482421875\n",
            "    StateBufferConnector_ms: 0.004991054534912109\n",
            "    ViewRequirementAgentConnector_ms: 0.13432097434997559\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.483249977011983\n",
            "  episode_reward_min: -16.576630769717077\n",
            "  episodes_this_iter: 6\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-11.885070226640817, -10.952615943210938, -12.397296039616156,\n",
            "      -14.348876664953949, -10.469279430335924, -12.550867437519926, -12.948255258634058,\n",
            "      -12.844806230633733, -9.869071947825871, -14.799725681950145, -15.217906908851694,\n",
            "      -13.396100065550439, -9.825872026926868, -12.80790931670308, -8.460180194190139,\n",
            "      -7.895667880116643, -14.9559007999179, -12.04587172709983, -11.500113366012776,\n",
            "      -8.628475998121827, -12.302732576037618, -14.797011253282088, -9.374144805207553,\n",
            "      -11.005801486326542, -12.01544815007982, -10.224082892379098, -12.709105769385685,\n",
            "      -7.890350068098079, -12.43601158759242, -14.151613620031116, -9.542861251244908,\n",
            "      -12.12814643800162, -15.546500978140484, -10.666178171774728, -9.10352998221069,\n",
            "      -12.95702785772355, -14.82836910969135, -8.426565987531134, -13.837848810127497,\n",
            "      -11.868366860923198, -15.840938386980476, -14.775156967801474, -10.829532179532166,\n",
            "      -16.576630769717077, -15.19246877064802, -11.267033004655092, -11.865422840929678,\n",
            "      -13.46048762823312, -12.277969371310444, -9.1464936144288, -11.182424475530023,\n",
            "      -11.477762184825, -8.604758302135476, -13.49267102762119, -11.071449216866686,\n",
            "      -11.112009692183571, -12.712559331198094, -6.811581609157007, -13.413294460526679,\n",
            "      -7.215254320872949, -10.456773102052791, -12.208323821574268, -12.938000680027411,\n",
            "      -12.95868617175637, -11.88083777302189, -11.762136603418604, -6.705977731703049,\n",
            "      -14.08938714283293, -9.150013534006034, -9.439512493063452, -13.951182533502232,\n",
            "      -12.05665465774798, -10.608063036244266, -14.777214783631942, -8.130244594842775,\n",
            "      -9.36960500636909, -6.872272505312526, -10.766278472460701, -7.929009053458728,\n",
            "      -13.373216398421135, -12.094972978989182, -10.533361932837124, -9.861880684544484,\n",
            "      -14.76786107475355, -9.637541311276973, -12.111276001648935, -7.659187504063094,\n",
            "      -10.654072144021406, -10.662708288246701, -12.157111678172402, -11.036723399105998,\n",
            "      -9.017724652339835, -10.383344214753725, -10.695380186558769, -9.479772956160454,\n",
            "      -9.285665126804158, -10.612019896672653, -10.867272274875523, -14.075178824852696,\n",
            "      -11.371093519317604]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25615076159227285\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17877635984743734\n",
            "    mean_inference_ms: 1.2822941213253218\n",
            "    mean_raw_obs_processing_ms: 0.36750166109467114\n",
            "time_since_restore: 507.02679920196533\n",
            "time_this_iter_s: 1.0122041702270508\n",
            "time_total_s: 507.02679920196533\n",
            "timers:\n",
            "  learn_throughput: 1658.155\n",
            "  learn_time_ms: 19.299\n",
            "  load_throughput: 110186.133\n",
            "  load_time_ms: 0.29\n",
            "  sample_time_ms: 123.741\n",
            "  synch_weights_time_ms: 0.037\n",
            "  training_iteration_time_ms: 169.615\n",
            "timestamp: 1703177883\n",
            "timesteps_total: 178050\n",
            "training_iteration: 474\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 178350\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006610870361328125\n",
            "  StateBufferConnector_ms: 0.0050258636474609375\n",
            "  ViewRequirementAgentConnector_ms: 0.1352066993713379\n",
            "counters:\n",
            "  last_target_update_ts: 178350\n",
            "  num_agent_steps_sampled: 178350\n",
            "  num_agent_steps_trained: 113184\n",
            "  num_env_steps_sampled: 178350\n",
            "  num_env_steps_trained: 113184\n",
            "  num_target_updates: 3537\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-04\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.417446541661409\n",
            "episode_reward_min: -16.576630769717077\n",
            "episodes_this_iter: 6\n",
            "episodes_total: 3567\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 178350\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1809.5625\n",
            "      learner_stats:\n",
            "        actor_loss: -19.870542526245117\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1074864864349365\n",
            "        alpha_value: 0.7034623622894287\n",
            "        critic_loss: 0.08648693561553955\n",
            "        grad_gnorm: 3.1485865116119385\n",
            "        log_alpha_value: -0.35174086689949036\n",
            "        max_q: 21.019821166992188\n",
            "        mean_q: 19.142467498779297\n",
            "        min_q: 16.24433708190918\n",
            "        policy_t: -0.17400309443473816\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.3092733323574066\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3537.0\n",
            "      td_error: [0.32601070404052734, 0.8423519134521484, 0.23405742645263672, 0.03710174560546875,\n",
            "        0.3876018524169922, 0.7800683975219727, 0.27414894104003906, 0.2292346954345703,\n",
            "        0.07286643981933594, 0.2575063705444336, 0.4466667175292969, 0.039589881896972656,\n",
            "        0.4805583953857422, 0.05239582061767578, 0.41796207427978516, 0.06769466400146484,\n",
            "        0.055068016052246094, 0.036957740783691406, 1.0957326889038086, 0.14697647094726562,\n",
            "        0.5756196975708008, 0.1325225830078125, 0.009967803955078125, 0.27797889709472656,\n",
            "        0.5735073089599609, 0.19241714477539062, 0.19892597198486328, 0.39251708984375,\n",
            "        0.8235435485839844, 0.04347515106201172, 0.05218315124511719, 0.3435373306274414]\n",
            "  num_agent_steps_sampled: 178350\n",
            "  num_agent_steps_trained: 113184\n",
            "  num_env_steps_sampled: 178350\n",
            "  num_env_steps_trained: 113184\n",
            "  num_target_updates: 3537\n",
            "iterations_since_restore: 475\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 178350\n",
            "num_agent_steps_trained: 113184\n",
            "num_env_steps_sampled: 178350\n",
            "num_env_steps_sampled_this_iter: 300\n",
            "num_env_steps_sampled_throughput_per_sec: 289.8802896381182\n",
            "num_env_steps_trained: 113184\n",
            "num_env_steps_trained_this_iter: 192\n",
            "num_env_steps_trained_throughput_per_sec: 185.52338536839565\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 192\n",
            "perf:\n",
            "  cpu_util_percent: 99.65\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2561339701358507\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.1787649910163687\n",
            "  mean_inference_ms: 1.2821490381166023\n",
            "  mean_raw_obs_processing_ms: 0.36746317619594676\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006610870361328125\n",
            "    StateBufferConnector_ms: 0.0050258636474609375\n",
            "    ViewRequirementAgentConnector_ms: 0.1352066993713379\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.417446541661409\n",
            "  episode_reward_min: -16.576630769717077\n",
            "  episodes_this_iter: 6\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-12.948255258634058, -12.844806230633733, -9.869071947825871,\n",
            "      -14.799725681950145, -15.217906908851694, -13.396100065550439, -9.825872026926868,\n",
            "      -12.80790931670308, -8.460180194190139, -7.895667880116643, -14.9559007999179,\n",
            "      -12.04587172709983, -11.500113366012776, -8.628475998121827, -12.302732576037618,\n",
            "      -14.797011253282088, -9.374144805207553, -11.005801486326542, -12.01544815007982,\n",
            "      -10.224082892379098, -12.709105769385685, -7.890350068098079, -12.43601158759242,\n",
            "      -14.151613620031116, -9.542861251244908, -12.12814643800162, -15.546500978140484,\n",
            "      -10.666178171774728, -9.10352998221069, -12.95702785772355, -14.82836910969135,\n",
            "      -8.426565987531134, -13.837848810127497, -11.868366860923198, -15.840938386980476,\n",
            "      -14.775156967801474, -10.829532179532166, -16.576630769717077, -15.19246877064802,\n",
            "      -11.267033004655092, -11.865422840929678, -13.46048762823312, -12.277969371310444,\n",
            "      -9.1464936144288, -11.182424475530023, -11.477762184825, -8.604758302135476,\n",
            "      -13.49267102762119, -11.071449216866686, -11.112009692183571, -12.712559331198094,\n",
            "      -6.811581609157007, -13.413294460526679, -7.215254320872949, -10.456773102052791,\n",
            "      -12.208323821574268, -12.938000680027411, -12.95868617175637, -11.88083777302189,\n",
            "      -11.762136603418604, -6.705977731703049, -14.08938714283293, -9.150013534006034,\n",
            "      -9.439512493063452, -13.951182533502232, -12.05665465774798, -10.608063036244266,\n",
            "      -14.777214783631942, -8.130244594842775, -9.36960500636909, -6.872272505312526,\n",
            "      -10.766278472460701, -7.929009053458728, -13.373216398421135, -12.094972978989182,\n",
            "      -10.533361932837124, -9.861880684544484, -14.76786107475355, -9.637541311276973,\n",
            "      -12.111276001648935, -7.659187504063094, -10.654072144021406, -10.662708288246701,\n",
            "      -12.157111678172402, -11.036723399105998, -9.017724652339835, -10.383344214753725,\n",
            "      -10.695380186558769, -9.479772956160454, -9.285665126804158, -10.612019896672653,\n",
            "      -10.867272274875523, -14.075178824852696, -11.371093519317604, -12.521139836904533,\n",
            "      -12.017776959414775, -9.161326716075497, -12.177414047615514, -10.367973658502486,\n",
            "      -9.778030988707343]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2561339701358507\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1787649910163687\n",
            "    mean_inference_ms: 1.2821490381166023\n",
            "    mean_raw_obs_processing_ms: 0.36746317619594676\n",
            "time_since_restore: 508.0632622241974\n",
            "time_this_iter_s: 1.0364630222320557\n",
            "time_total_s: 508.0632622241974\n",
            "timers:\n",
            "  learn_throughput: 1634.577\n",
            "  learn_time_ms: 19.577\n",
            "  load_throughput: 110786.404\n",
            "  load_time_ms: 0.289\n",
            "  sample_time_ms: 125.401\n",
            "  synch_weights_time_ms: 0.04\n",
            "  training_iteration_time_ms: 172.318\n",
            "timestamp: 1703177884\n",
            "timesteps_total: 178350\n",
            "training_iteration: 475\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 178750\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.0066263675689697266\n",
            "  StateBufferConnector_ms: 0.0050547122955322266\n",
            "  ViewRequirementAgentConnector_ms: 0.1360337734222412\n",
            "counters:\n",
            "  last_target_update_ts: 178750\n",
            "  num_agent_steps_sampled: 178750\n",
            "  num_agent_steps_trained: 113440\n",
            "  num_env_steps_sampled: 178750\n",
            "  num_env_steps_trained: 113440\n",
            "  num_target_updates: 3545\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-05\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.428349378260966\n",
            "episode_reward_min: -16.576630769717077\n",
            "episodes_this_iter: 8\n",
            "episodes_total: 3575\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 178750\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 2046.59375\n",
            "      learner_stats:\n",
            "        actor_loss: -20.028413772583008\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1738793849945068\n",
            "        alpha_value: 0.7029068470001221\n",
            "        critic_loss: 0.09046623855829239\n",
            "        grad_gnorm: 3.329861640930176\n",
            "        log_alpha_value: -0.35253092646598816\n",
            "        max_q: 22.706684112548828\n",
            "        mean_q: 19.291194915771484\n",
            "        min_q: 14.837983131408691\n",
            "        policy_t: -0.051690127700567245\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.31073179841041565\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3545.0\n",
            "      td_error: [0.27908992767333984, 0.4306507110595703, 0.04142475128173828, 0.17282772064208984,\n",
            "        0.6439580917358398, 0.08707141876220703, 0.12661170959472656, 0.1199655532836914,\n",
            "        0.14573383331298828, 0.18332576751708984, 0.3147296905517578, 0.13080072402954102,\n",
            "        0.3125753402709961, 0.20208740234375, 0.3805351257324219, 0.3517932891845703,\n",
            "        0.2351818084716797, 0.45825767517089844, 0.0909128189086914, 0.1057577133178711,\n",
            "        0.26229381561279297, 1.4128236770629883, 0.6060256958007812, 1.027104377746582,\n",
            "        0.20581817626953125, 0.6530122756958008, 0.13883209228515625, 0.1796426773071289,\n",
            "        0.0967092514038086, 0.1064310073852539, 0.13114261627197266, 0.3102903366088867]\n",
            "  num_agent_steps_sampled: 178750\n",
            "  num_agent_steps_trained: 113440\n",
            "  num_env_steps_sampled: 178750\n",
            "  num_env_steps_trained: 113440\n",
            "  num_target_updates: 3545\n",
            "iterations_since_restore: 476\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 178750\n",
            "num_agent_steps_trained: 113440\n",
            "num_env_steps_sampled: 178750\n",
            "num_env_steps_sampled_this_iter: 400\n",
            "num_env_steps_sampled_throughput_per_sec: 365.02871107382373\n",
            "num_env_steps_trained: 113440\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_env_steps_trained_throughput_per_sec: 233.6183750872472\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 90.0\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25611446901798474\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.1787544611386813\n",
            "  mean_inference_ms: 1.2819729207294628\n",
            "  mean_raw_obs_processing_ms: 0.36741507047273025\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.0066263675689697266\n",
            "    StateBufferConnector_ms: 0.0050547122955322266\n",
            "    ViewRequirementAgentConnector_ms: 0.1360337734222412\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.428349378260966\n",
            "  episode_reward_min: -16.576630769717077\n",
            "  episodes_this_iter: 8\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-8.460180194190139, -7.895667880116643, -14.9559007999179, -12.04587172709983,\n",
            "      -11.500113366012776, -8.628475998121827, -12.302732576037618, -14.797011253282088,\n",
            "      -9.374144805207553, -11.005801486326542, -12.01544815007982, -10.224082892379098,\n",
            "      -12.709105769385685, -7.890350068098079, -12.43601158759242, -14.151613620031116,\n",
            "      -9.542861251244908, -12.12814643800162, -15.546500978140484, -10.666178171774728,\n",
            "      -9.10352998221069, -12.95702785772355, -14.82836910969135, -8.426565987531134,\n",
            "      -13.837848810127497, -11.868366860923198, -15.840938386980476, -14.775156967801474,\n",
            "      -10.829532179532166, -16.576630769717077, -15.19246877064802, -11.267033004655092,\n",
            "      -11.865422840929678, -13.46048762823312, -12.277969371310444, -9.1464936144288,\n",
            "      -11.182424475530023, -11.477762184825, -8.604758302135476, -13.49267102762119,\n",
            "      -11.071449216866686, -11.112009692183571, -12.712559331198094, -6.811581609157007,\n",
            "      -13.413294460526679, -7.215254320872949, -10.456773102052791, -12.208323821574268,\n",
            "      -12.938000680027411, -12.95868617175637, -11.88083777302189, -11.762136603418604,\n",
            "      -6.705977731703049, -14.08938714283293, -9.150013534006034, -9.439512493063452,\n",
            "      -13.951182533502232, -12.05665465774798, -10.608063036244266, -14.777214783631942,\n",
            "      -8.130244594842775, -9.36960500636909, -6.872272505312526, -10.766278472460701,\n",
            "      -7.929009053458728, -13.373216398421135, -12.094972978989182, -10.533361932837124,\n",
            "      -9.861880684544484, -14.76786107475355, -9.637541311276973, -12.111276001648935,\n",
            "      -7.659187504063094, -10.654072144021406, -10.662708288246701, -12.157111678172402,\n",
            "      -11.036723399105998, -9.017724652339835, -10.383344214753725, -10.695380186558769,\n",
            "      -9.479772956160454, -9.285665126804158, -10.612019896672653, -10.867272274875523,\n",
            "      -14.075178824852696, -11.371093519317604, -12.521139836904533, -12.017776959414775,\n",
            "      -9.161326716075497, -12.177414047615514, -10.367973658502486, -9.778030988707343,\n",
            "      -12.485545017073939, -12.502553909244916, -14.804113064086984, -10.320976753197233,\n",
            "      -10.589360505275465, -13.457686499889114, -15.188115648480595, -13.451579699783416]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25611446901798474\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1787544611386813\n",
            "    mean_inference_ms: 1.2819729207294628\n",
            "    mean_raw_obs_processing_ms: 0.36741507047273025\n",
            "time_since_restore: 509.16017961502075\n",
            "time_this_iter_s: 1.0969173908233643\n",
            "time_total_s: 509.16017961502075\n",
            "timers:\n",
            "  learn_throughput: 1979.995\n",
            "  learn_time_ms: 16.162\n",
            "  load_throughput: 147297.77\n",
            "  load_time_ms: 0.217\n",
            "  sample_time_ms: 109.502\n",
            "  synch_weights_time_ms: 0.03\n",
            "  training_iteration_time_ms: 145.748\n",
            "timestamp: 1703177885\n",
            "timesteps_total: 178750\n",
            "training_iteration: 476\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 179200\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006594657897949219\n",
            "  StateBufferConnector_ms: 0.005056619644165039\n",
            "  ViewRequirementAgentConnector_ms: 0.13559579849243164\n",
            "counters:\n",
            "  last_target_update_ts: 179200\n",
            "  num_agent_steps_sampled: 179200\n",
            "  num_agent_steps_trained: 113728\n",
            "  num_env_steps_sampled: 179200\n",
            "  num_env_steps_trained: 113728\n",
            "  num_target_updates: 3554\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-06\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.67158159321114\n",
            "episode_reward_min: -16.576630769717077\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3584\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 179200\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1681.25\n",
            "      learner_stats:\n",
            "        actor_loss: -20.538928985595703\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.084675669670105\n",
            "        alpha_value: 0.7022863030433655\n",
            "        critic_loss: 0.019909001886844635\n",
            "        grad_gnorm: 3.0691351890563965\n",
            "        log_alpha_value: -0.3534141480922699\n",
            "        max_q: 25.382553100585938\n",
            "        mean_q: 19.857501983642578\n",
            "        min_q: 17.486797332763672\n",
            "        policy_t: -0.23143240809440613\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.15623807907104492\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3554.0\n",
            "      td_error: [0.28920459747314453, 0.1411571502685547, 0.0686788558959961, 0.016300201416015625,\n",
            "        0.22129440307617188, 0.036873817443847656, 0.18017578125, 0.20215225219726562,\n",
            "        0.20711326599121094, 0.14375782012939453, 0.045703887939453125, 0.0046558380126953125,\n",
            "        0.12054729461669922, 0.033186912536621094, 0.06733989715576172, 0.1170206069946289,\n",
            "        0.046097755432128906, 0.2350006103515625, 0.37641239166259766, 0.04051780700683594,\n",
            "        0.11005878448486328, 0.17886066436767578, 0.2788267135620117, 0.3515129089355469,\n",
            "        0.07903480529785156, 0.3426980972290039, 0.044643402099609375, 0.3657197952270508,\n",
            "        0.015400886535644531, 0.3064546585083008, 0.06392478942871094, 0.26929187774658203]\n",
            "  num_agent_steps_sampled: 179200\n",
            "  num_agent_steps_trained: 113728\n",
            "  num_env_steps_sampled: 179200\n",
            "  num_env_steps_trained: 113728\n",
            "  num_target_updates: 3554\n",
            "iterations_since_restore: 477\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 179200\n",
            "num_agent_steps_trained: 113728\n",
            "num_env_steps_sampled: 179200\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 415.2901418590615\n",
            "num_env_steps_trained: 113728\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 265.7856907897994\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 55.4\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25609153609485974\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17874149600192787\n",
            "  mean_inference_ms: 1.2817713735473706\n",
            "  mean_raw_obs_processing_ms: 0.3673583754702453\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006594657897949219\n",
            "    StateBufferConnector_ms: 0.005056619644165039\n",
            "    ViewRequirementAgentConnector_ms: 0.13559579849243164\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.67158159321114\n",
            "  episode_reward_min: -16.576630769717077\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-11.005801486326542, -12.01544815007982, -10.224082892379098,\n",
            "      -12.709105769385685, -7.890350068098079, -12.43601158759242, -14.151613620031116,\n",
            "      -9.542861251244908, -12.12814643800162, -15.546500978140484, -10.666178171774728,\n",
            "      -9.10352998221069, -12.95702785772355, -14.82836910969135, -8.426565987531134,\n",
            "      -13.837848810127497, -11.868366860923198, -15.840938386980476, -14.775156967801474,\n",
            "      -10.829532179532166, -16.576630769717077, -15.19246877064802, -11.267033004655092,\n",
            "      -11.865422840929678, -13.46048762823312, -12.277969371310444, -9.1464936144288,\n",
            "      -11.182424475530023, -11.477762184825, -8.604758302135476, -13.49267102762119,\n",
            "      -11.071449216866686, -11.112009692183571, -12.712559331198094, -6.811581609157007,\n",
            "      -13.413294460526679, -7.215254320872949, -10.456773102052791, -12.208323821574268,\n",
            "      -12.938000680027411, -12.95868617175637, -11.88083777302189, -11.762136603418604,\n",
            "      -6.705977731703049, -14.08938714283293, -9.150013534006034, -9.439512493063452,\n",
            "      -13.951182533502232, -12.05665465774798, -10.608063036244266, -14.777214783631942,\n",
            "      -8.130244594842775, -9.36960500636909, -6.872272505312526, -10.766278472460701,\n",
            "      -7.929009053458728, -13.373216398421135, -12.094972978989182, -10.533361932837124,\n",
            "      -9.861880684544484, -14.76786107475355, -9.637541311276973, -12.111276001648935,\n",
            "      -7.659187504063094, -10.654072144021406, -10.662708288246701, -12.157111678172402,\n",
            "      -11.036723399105998, -9.017724652339835, -10.383344214753725, -10.695380186558769,\n",
            "      -9.479772956160454, -9.285665126804158, -10.612019896672653, -10.867272274875523,\n",
            "      -14.075178824852696, -11.371093519317604, -12.521139836904533, -12.017776959414775,\n",
            "      -9.161326716075497, -12.177414047615514, -10.367973658502486, -9.778030988707343,\n",
            "      -12.485545017073939, -12.502553909244916, -14.804113064086984, -10.320976753197233,\n",
            "      -10.589360505275465, -13.457686499889114, -15.188115648480595, -13.451579699783416,\n",
            "      -13.847447318744019, -15.62213315314085, -14.898431366057483, -14.950176911069281,\n",
            "      -12.878792173470302, -14.191468213559183, -14.482777466918787, -11.96868688718963,\n",
            "      -11.44340660485428]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25609153609485974\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17874149600192787\n",
            "    mean_inference_ms: 1.2817713735473706\n",
            "    mean_raw_obs_processing_ms: 0.3673583754702453\n",
            "time_since_restore: 510.24493408203125\n",
            "time_this_iter_s: 1.084754467010498\n",
            "time_total_s: 510.24493408203125\n",
            "timers:\n",
            "  learn_throughput: 2295.859\n",
            "  learn_time_ms: 13.938\n",
            "  load_throughput: 179339.562\n",
            "  load_time_ms: 0.178\n",
            "  sample_time_ms: 88.879\n",
            "  synch_weights_time_ms: 0.025\n",
            "  training_iteration_time_ms: 119.865\n",
            "timestamp: 1703177886\n",
            "timesteps_total: 179200\n",
            "training_iteration: 477\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 179600\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006746768951416016\n",
            "  StateBufferConnector_ms: 0.005075216293334961\n",
            "  ViewRequirementAgentConnector_ms: 0.13649964332580566\n",
            "counters:\n",
            "  last_target_update_ts: 179600\n",
            "  num_agent_steps_sampled: 179600\n",
            "  num_agent_steps_trained: 113984\n",
            "  num_env_steps_sampled: 179600\n",
            "  num_env_steps_trained: 113984\n",
            "  num_target_updates: 3562\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-07\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.71183353584396\n",
            "episode_reward_min: -16.576630769717077\n",
            "episodes_this_iter: 8\n",
            "episodes_total: 3592\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 179600\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1833.28125\n",
            "      learner_stats:\n",
            "        actor_loss: -20.34326171875\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1026935577392578\n",
            "        alpha_value: 0.7017360329627991\n",
            "        critic_loss: 0.04650234058499336\n",
            "        grad_gnorm: 3.1132123470306396\n",
            "        log_alpha_value: -0.35419800877571106\n",
            "        max_q: 22.250438690185547\n",
            "        mean_q: 19.785064697265625\n",
            "        min_q: 16.11876678466797\n",
            "        policy_t: -0.2186480611562729\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.24601536989212036\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3562.0\n",
            "      td_error: [0.14235687255859375, 0.7659053802490234, 0.07664299011230469, 0.051189422607421875,\n",
            "        0.1550445556640625, 0.49784374237060547, 0.36901378631591797, 0.11631488800048828,\n",
            "        0.4464225769042969, 0.1148996353149414, 0.2533378601074219, 0.13828659057617188,\n",
            "        0.048381805419921875, 0.4568338394165039, 0.34023094177246094, 0.044859886169433594,\n",
            "        0.3553133010864258, 0.3954582214355469, 0.17534828186035156, 0.24367427825927734,\n",
            "        0.1099691390991211, 0.25232887268066406, 0.45732688903808594, 0.10485267639160156,\n",
            "        0.39258575439453125, 0.21251869201660156, 0.37973690032958984, 0.14512252807617188,\n",
            "        0.2155323028564453, 0.21899890899658203, 0.013113975524902344, 0.1830463409423828]\n",
            "  num_agent_steps_sampled: 179600\n",
            "  num_agent_steps_trained: 113984\n",
            "  num_env_steps_sampled: 179600\n",
            "  num_env_steps_trained: 113984\n",
            "  num_target_updates: 3562\n",
            "iterations_since_restore: 478\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 179600\n",
            "num_agent_steps_trained: 113984\n",
            "num_env_steps_sampled: 179600\n",
            "num_env_steps_sampled_this_iter: 400\n",
            "num_env_steps_sampled_throughput_per_sec: 389.2770936454935\n",
            "num_env_steps_trained: 113984\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_env_steps_trained_throughput_per_sec: 249.13733993311584\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 55.95\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25607379908952527\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17873142626607283\n",
            "  mean_inference_ms: 1.2816099430223198\n",
            "  mean_raw_obs_processing_ms: 0.36731002793734163\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006746768951416016\n",
            "    StateBufferConnector_ms: 0.005075216293334961\n",
            "    ViewRequirementAgentConnector_ms: 0.13649964332580566\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.71183353584396\n",
            "  episode_reward_min: -16.576630769717077\n",
            "  episodes_this_iter: 8\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-12.12814643800162, -15.546500978140484, -10.666178171774728,\n",
            "      -9.10352998221069, -12.95702785772355, -14.82836910969135, -8.426565987531134,\n",
            "      -13.837848810127497, -11.868366860923198, -15.840938386980476, -14.775156967801474,\n",
            "      -10.829532179532166, -16.576630769717077, -15.19246877064802, -11.267033004655092,\n",
            "      -11.865422840929678, -13.46048762823312, -12.277969371310444, -9.1464936144288,\n",
            "      -11.182424475530023, -11.477762184825, -8.604758302135476, -13.49267102762119,\n",
            "      -11.071449216866686, -11.112009692183571, -12.712559331198094, -6.811581609157007,\n",
            "      -13.413294460526679, -7.215254320872949, -10.456773102052791, -12.208323821574268,\n",
            "      -12.938000680027411, -12.95868617175637, -11.88083777302189, -11.762136603418604,\n",
            "      -6.705977731703049, -14.08938714283293, -9.150013534006034, -9.439512493063452,\n",
            "      -13.951182533502232, -12.05665465774798, -10.608063036244266, -14.777214783631942,\n",
            "      -8.130244594842775, -9.36960500636909, -6.872272505312526, -10.766278472460701,\n",
            "      -7.929009053458728, -13.373216398421135, -12.094972978989182, -10.533361932837124,\n",
            "      -9.861880684544484, -14.76786107475355, -9.637541311276973, -12.111276001648935,\n",
            "      -7.659187504063094, -10.654072144021406, -10.662708288246701, -12.157111678172402,\n",
            "      -11.036723399105998, -9.017724652339835, -10.383344214753725, -10.695380186558769,\n",
            "      -9.479772956160454, -9.285665126804158, -10.612019896672653, -10.867272274875523,\n",
            "      -14.075178824852696, -11.371093519317604, -12.521139836904533, -12.017776959414775,\n",
            "      -9.161326716075497, -12.177414047615514, -10.367973658502486, -9.778030988707343,\n",
            "      -12.485545017073939, -12.502553909244916, -14.804113064086984, -10.320976753197233,\n",
            "      -10.589360505275465, -13.457686499889114, -15.188115648480595, -13.451579699783416,\n",
            "      -13.847447318744019, -15.62213315314085, -14.898431366057483, -14.950176911069281,\n",
            "      -12.878792173470302, -14.191468213559183, -14.482777466918787, -11.96868688718963,\n",
            "      -11.44340660485428, -12.448758889616082, -12.990992413430979, -9.288925906749954,\n",
            "      -12.958628622625204, -12.106768301855102, -13.467092482188749, -9.248766661875116,\n",
            "      -11.490535810078455]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25607379908952527\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17873142626607283\n",
            "    mean_inference_ms: 1.2816099430223198\n",
            "    mean_raw_obs_processing_ms: 0.36731002793734163\n",
            "time_since_restore: 511.2737340927124\n",
            "time_this_iter_s: 1.0288000106811523\n",
            "time_total_s: 511.2737340927124\n",
            "timers:\n",
            "  learn_throughput: 2240.813\n",
            "  learn_time_ms: 14.281\n",
            "  load_throughput: 181547.042\n",
            "  load_time_ms: 0.176\n",
            "  sample_time_ms: 94.792\n",
            "  synch_weights_time_ms: 0.029\n",
            "  training_iteration_time_ms: 126.804\n",
            "timestamp: 1703177887\n",
            "timesteps_total: 179600\n",
            "training_iteration: 478\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 180050\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006756782531738281\n",
            "  StateBufferConnector_ms: 0.0050661563873291016\n",
            "  ViewRequirementAgentConnector_ms: 0.13660001754760742\n",
            "counters:\n",
            "  last_target_update_ts: 180050\n",
            "  num_agent_steps_sampled: 180050\n",
            "  num_agent_steps_trained: 114272\n",
            "  num_env_steps_sampled: 180050\n",
            "  num_env_steps_trained: 114272\n",
            "  num_target_updates: 3571\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-08\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.840130436791215\n",
            "episode_reward_min: -16.576630769717077\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3601\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 180050\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1855.34375\n",
            "      learner_stats:\n",
            "        actor_loss: -20.205768585205078\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1298811435699463\n",
            "        alpha_value: 0.7011231184005737\n",
            "        critic_loss: 0.040465377271175385\n",
            "        grad_gnorm: 3.1821210384368896\n",
            "        log_alpha_value: -0.3550717532634735\n",
            "        max_q: 23.59117317199707\n",
            "        mean_q: 19.483232498168945\n",
            "        min_q: 16.57839584350586\n",
            "        policy_t: -0.19394895434379578\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.22697550058364868\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3571.0\n",
            "      td_error: [0.23880386352539062, 0.0941629409790039, 0.2693510055541992, 0.15519142150878906,\n",
            "        0.2620563507080078, 0.1571979522705078, 0.4242362976074219, 0.03332328796386719,\n",
            "        0.5259733200073242, 0.054787635803222656, 0.11087894439697266, 0.21866416931152344,\n",
            "        0.0818166732788086, 0.19404315948486328, 0.27042579650878906, 0.3951845169067383,\n",
            "        0.1790637969970703, 0.12191009521484375, 0.12038230895996094, 0.46570777893066406,\n",
            "        0.6230220794677734, 0.09485721588134766, 0.3351869583129883, 0.14708518981933594,\n",
            "        0.11773967742919922, 0.022591590881347656, 0.21238136291503906, 0.1537628173828125,\n",
            "        0.5458440780639648, 0.24396419525146484, 0.07232379913330078, 0.32129573822021484]\n",
            "  num_agent_steps_sampled: 180050\n",
            "  num_agent_steps_trained: 114272\n",
            "  num_env_steps_sampled: 180050\n",
            "  num_env_steps_trained: 114272\n",
            "  num_target_updates: 3571\n",
            "iterations_since_restore: 479\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 180050\n",
            "num_agent_steps_trained: 114272\n",
            "num_env_steps_sampled: 180050\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 415.7176716652104\n",
            "num_env_steps_trained: 114272\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 266.0593098657347\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 56.7\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25605366352788367\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.1787194377513918\n",
            "  mean_inference_ms: 1.2814258616720273\n",
            "  mean_raw_obs_processing_ms: 0.367255328480582\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006756782531738281\n",
            "    StateBufferConnector_ms: 0.0050661563873291016\n",
            "    ViewRequirementAgentConnector_ms: 0.13660001754760742\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.840130436791215\n",
            "  episode_reward_min: -16.576630769717077\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-15.840938386980476, -14.775156967801474, -10.829532179532166,\n",
            "      -16.576630769717077, -15.19246877064802, -11.267033004655092, -11.865422840929678,\n",
            "      -13.46048762823312, -12.277969371310444, -9.1464936144288, -11.182424475530023,\n",
            "      -11.477762184825, -8.604758302135476, -13.49267102762119, -11.071449216866686,\n",
            "      -11.112009692183571, -12.712559331198094, -6.811581609157007, -13.413294460526679,\n",
            "      -7.215254320872949, -10.456773102052791, -12.208323821574268, -12.938000680027411,\n",
            "      -12.95868617175637, -11.88083777302189, -11.762136603418604, -6.705977731703049,\n",
            "      -14.08938714283293, -9.150013534006034, -9.439512493063452, -13.951182533502232,\n",
            "      -12.05665465774798, -10.608063036244266, -14.777214783631942, -8.130244594842775,\n",
            "      -9.36960500636909, -6.872272505312526, -10.766278472460701, -7.929009053458728,\n",
            "      -13.373216398421135, -12.094972978989182, -10.533361932837124, -9.861880684544484,\n",
            "      -14.76786107475355, -9.637541311276973, -12.111276001648935, -7.659187504063094,\n",
            "      -10.654072144021406, -10.662708288246701, -12.157111678172402, -11.036723399105998,\n",
            "      -9.017724652339835, -10.383344214753725, -10.695380186558769, -9.479772956160454,\n",
            "      -9.285665126804158, -10.612019896672653, -10.867272274875523, -14.075178824852696,\n",
            "      -11.371093519317604, -12.521139836904533, -12.017776959414775, -9.161326716075497,\n",
            "      -12.177414047615514, -10.367973658502486, -9.778030988707343, -12.485545017073939,\n",
            "      -12.502553909244916, -14.804113064086984, -10.320976753197233, -10.589360505275465,\n",
            "      -13.457686499889114, -15.188115648480595, -13.451579699783416, -13.847447318744019,\n",
            "      -15.62213315314085, -14.898431366057483, -14.950176911069281, -12.878792173470302,\n",
            "      -14.191468213559183, -14.482777466918787, -11.96868688718963, -11.44340660485428,\n",
            "      -12.448758889616082, -12.990992413430979, -9.288925906749954, -12.958628622625204,\n",
            "      -12.106768301855102, -13.467092482188749, -9.248766661875116, -11.490535810078455,\n",
            "      -13.725009139429709, -13.4130069295191, -11.602683942103303, -14.50602504996919,\n",
            "      -12.681462746674505, -14.656985864996866, -12.660956971998862, -12.582657047159453,\n",
            "      -16.363436598998874]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25605366352788367\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1787194377513918\n",
            "    mean_inference_ms: 1.2814258616720273\n",
            "    mean_raw_obs_processing_ms: 0.367255328480582\n",
            "time_since_restore: 512.3573920726776\n",
            "time_this_iter_s: 1.08365797996521\n",
            "time_total_s: 512.3573920726776\n",
            "timers:\n",
            "  learn_throughput: 2417.262\n",
            "  learn_time_ms: 13.238\n",
            "  load_throughput: 188006.343\n",
            "  load_time_ms: 0.17\n",
            "  sample_time_ms: 91.27\n",
            "  synch_weights_time_ms: 0.026\n",
            "  training_iteration_time_ms: 121.869\n",
            "timestamp: 1703177888\n",
            "timesteps_total: 180050\n",
            "training_iteration: 479\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 180500\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006760358810424805\n",
            "  StateBufferConnector_ms: 0.005086183547973633\n",
            "  ViewRequirementAgentConnector_ms: 0.1369633674621582\n",
            "counters:\n",
            "  last_target_update_ts: 180500\n",
            "  num_agent_steps_sampled: 180500\n",
            "  num_agent_steps_trained: 114560\n",
            "  num_env_steps_sampled: 180500\n",
            "  num_env_steps_trained: 114560\n",
            "  num_target_updates: 3580\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-09\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.730909508800691\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3610\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 180500\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 2040.84375\n",
            "      learner_stats:\n",
            "        actor_loss: -20.331668853759766\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1170272827148438\n",
            "        alpha_value: 0.700510561466217\n",
            "        critic_loss: 0.04341437667608261\n",
            "        grad_gnorm: 3.1381947994232178\n",
            "        log_alpha_value: -0.35594579577445984\n",
            "        max_q: 21.65951156616211\n",
            "        mean_q: 19.602130889892578\n",
            "        min_q: 12.608197212219238\n",
            "        policy_t: -0.16628743708133698\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.22825640439987183\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3580.0\n",
            "      td_error: [0.3155326843261719, 0.2746849060058594, 0.10718822479248047, 0.3200397491455078,\n",
            "        0.14510822296142578, 0.0526123046875, 0.07226753234863281, 0.16321563720703125,\n",
            "        0.02842998504638672, 0.10626029968261719, 0.2676115036010742, 0.513519287109375,\n",
            "        0.07616710662841797, 0.24073505401611328, 0.29165077209472656, 0.08418464660644531,\n",
            "        0.1807098388671875, 0.2476787567138672, 0.1438884735107422, 0.6248254776000977,\n",
            "        0.362823486328125, 0.3941965103149414, 0.1595163345336914, 0.2838020324707031,\n",
            "        0.20742225646972656, 0.11382293701171875, 0.3713855743408203, 0.11765766143798828,\n",
            "        0.22171974182128906, 0.19372940063476562, 0.0646371841430664, 0.5571813583374023]\n",
            "  num_agent_steps_sampled: 180500\n",
            "  num_agent_steps_trained: 114560\n",
            "  num_env_steps_sampled: 180500\n",
            "  num_env_steps_trained: 114560\n",
            "  num_target_updates: 3580\n",
            "iterations_since_restore: 480\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 180500\n",
            "num_agent_steps_trained: 114560\n",
            "num_env_steps_sampled: 180500\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 413.94471267985267\n",
            "num_env_steps_trained: 114560\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 264.9246161151057\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 55.05\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2560327557728039\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.1787077223552376\n",
            "  mean_inference_ms: 1.2812391848457003\n",
            "  mean_raw_obs_processing_ms: 0.3671992532757724\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006760358810424805\n",
            "    StateBufferConnector_ms: 0.005086183547973633\n",
            "    ViewRequirementAgentConnector_ms: 0.1369633674621582\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.730909508800691\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-9.1464936144288, -11.182424475530023, -11.477762184825, -8.604758302135476,\n",
            "      -13.49267102762119, -11.071449216866686, -11.112009692183571, -12.712559331198094,\n",
            "      -6.811581609157007, -13.413294460526679, -7.215254320872949, -10.456773102052791,\n",
            "      -12.208323821574268, -12.938000680027411, -12.95868617175637, -11.88083777302189,\n",
            "      -11.762136603418604, -6.705977731703049, -14.08938714283293, -9.150013534006034,\n",
            "      -9.439512493063452, -13.951182533502232, -12.05665465774798, -10.608063036244266,\n",
            "      -14.777214783631942, -8.130244594842775, -9.36960500636909, -6.872272505312526,\n",
            "      -10.766278472460701, -7.929009053458728, -13.373216398421135, -12.094972978989182,\n",
            "      -10.533361932837124, -9.861880684544484, -14.76786107475355, -9.637541311276973,\n",
            "      -12.111276001648935, -7.659187504063094, -10.654072144021406, -10.662708288246701,\n",
            "      -12.157111678172402, -11.036723399105998, -9.017724652339835, -10.383344214753725,\n",
            "      -10.695380186558769, -9.479772956160454, -9.285665126804158, -10.612019896672653,\n",
            "      -10.867272274875523, -14.075178824852696, -11.371093519317604, -12.521139836904533,\n",
            "      -12.017776959414775, -9.161326716075497, -12.177414047615514, -10.367973658502486,\n",
            "      -9.778030988707343, -12.485545017073939, -12.502553909244916, -14.804113064086984,\n",
            "      -10.320976753197233, -10.589360505275465, -13.457686499889114, -15.188115648480595,\n",
            "      -13.451579699783416, -13.847447318744019, -15.62213315314085, -14.898431366057483,\n",
            "      -14.950176911069281, -12.878792173470302, -14.191468213559183, -14.482777466918787,\n",
            "      -11.96868688718963, -11.44340660485428, -12.448758889616082, -12.990992413430979,\n",
            "      -9.288925906749954, -12.958628622625204, -12.106768301855102, -13.467092482188749,\n",
            "      -9.248766661875116, -11.490535810078455, -13.725009139429709, -13.4130069295191,\n",
            "      -11.602683942103303, -14.50602504996919, -12.681462746674505, -14.656985864996866,\n",
            "      -12.660956971998862, -12.582657047159453, -16.363436598998874, -13.530862506330191,\n",
            "      -15.575662822289955, -14.478654851222322, -13.271152354473637, -10.200992463005514,\n",
            "      -11.172904321321422, -8.452631306227014, -10.387820214495893, -14.092866281389728]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2560327557728039\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1787077223552376\n",
            "    mean_inference_ms: 1.2812391848457003\n",
            "    mean_raw_obs_processing_ms: 0.3671992532757724\n",
            "time_since_restore: 513.4456720352173\n",
            "time_this_iter_s: 1.0882799625396729\n",
            "time_total_s: 513.4456720352173\n",
            "timers:\n",
            "  learn_throughput: 2440.784\n",
            "  learn_time_ms: 13.111\n",
            "  load_throughput: 166833.72\n",
            "  load_time_ms: 0.192\n",
            "  sample_time_ms: 89.231\n",
            "  synch_weights_time_ms: 0.025\n",
            "  training_iteration_time_ms: 121.032\n",
            "timestamp: 1703177889\n",
            "timesteps_total: 180500\n",
            "training_iteration: 480\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 180950\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006778240203857422\n",
            "  StateBufferConnector_ms: 0.005131959915161133\n",
            "  ViewRequirementAgentConnector_ms: 0.13748550415039062\n",
            "counters:\n",
            "  last_target_update_ts: 180950\n",
            "  num_agent_steps_sampled: 180950\n",
            "  num_agent_steps_trained: 114848\n",
            "  num_env_steps_sampled: 180950\n",
            "  num_env_steps_trained: 114848\n",
            "  num_target_updates: 3589\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-10\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.705977731703049\n",
            "episode_reward_mean: -11.724790432436171\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3619\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 180950\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1377.34375\n",
            "      learner_stats:\n",
            "        actor_loss: -20.471357345581055\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1226273775100708\n",
            "        alpha_value: 0.6998932361602783\n",
            "        critic_loss: 0.028794799000024796\n",
            "        grad_gnorm: 3.1461353302001953\n",
            "        log_alpha_value: -0.3568274676799774\n",
            "        max_q: 22.14851188659668\n",
            "        mean_q: 19.812854766845703\n",
            "        min_q: 15.754068374633789\n",
            "        policy_t: -0.16333259642124176\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.19043001532554626\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3589.0\n",
            "      td_error: [0.18547534942626953, 0.2931842803955078, 0.009227752685546875, 0.0829782485961914,\n",
            "        0.1997232437133789, 0.3513936996459961, 0.201385498046875, 0.22405433654785156,\n",
            "        0.4114418029785156, 0.24431896209716797, 0.169281005859375, 0.4415426254272461,\n",
            "        0.12085247039794922, 0.27677440643310547, 0.07604598999023438, 0.019559860229492188,\n",
            "        0.14506149291992188, 0.027230262756347656, 0.029211997985839844, 0.3212738037109375,\n",
            "        0.1539630889892578, 0.3295869827270508, 0.10793495178222656, 0.07548141479492188,\n",
            "        0.07396411895751953, 0.23330307006835938, 0.16075897216796875, 0.3054084777832031,\n",
            "        0.027976036071777344, 0.5014772415161133, 0.130645751953125, 0.16324329376220703]\n",
            "  num_agent_steps_sampled: 180950\n",
            "  num_agent_steps_trained: 114848\n",
            "  num_env_steps_sampled: 180950\n",
            "  num_env_steps_trained: 114848\n",
            "  num_target_updates: 3589\n",
            "iterations_since_restore: 481\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 180950\n",
            "num_agent_steps_trained: 114848\n",
            "num_env_steps_sampled: 180950\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 417.5877245673923\n",
            "num_env_steps_trained: 114848\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 267.25614372313106\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 56.4\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2560107614280952\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.1786961686199404\n",
            "  mean_inference_ms: 1.2810477415020949\n",
            "  mean_raw_obs_processing_ms: 0.36714371896913484\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006778240203857422\n",
            "    StateBufferConnector_ms: 0.005131959915161133\n",
            "    ViewRequirementAgentConnector_ms: 0.13748550415039062\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.705977731703049\n",
            "  episode_reward_mean: -11.724790432436171\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-13.413294460526679, -7.215254320872949, -10.456773102052791,\n",
            "      -12.208323821574268, -12.938000680027411, -12.95868617175637, -11.88083777302189,\n",
            "      -11.762136603418604, -6.705977731703049, -14.08938714283293, -9.150013534006034,\n",
            "      -9.439512493063452, -13.951182533502232, -12.05665465774798, -10.608063036244266,\n",
            "      -14.777214783631942, -8.130244594842775, -9.36960500636909, -6.872272505312526,\n",
            "      -10.766278472460701, -7.929009053458728, -13.373216398421135, -12.094972978989182,\n",
            "      -10.533361932837124, -9.861880684544484, -14.76786107475355, -9.637541311276973,\n",
            "      -12.111276001648935, -7.659187504063094, -10.654072144021406, -10.662708288246701,\n",
            "      -12.157111678172402, -11.036723399105998, -9.017724652339835, -10.383344214753725,\n",
            "      -10.695380186558769, -9.479772956160454, -9.285665126804158, -10.612019896672653,\n",
            "      -10.867272274875523, -14.075178824852696, -11.371093519317604, -12.521139836904533,\n",
            "      -12.017776959414775, -9.161326716075497, -12.177414047615514, -10.367973658502486,\n",
            "      -9.778030988707343, -12.485545017073939, -12.502553909244916, -14.804113064086984,\n",
            "      -10.320976753197233, -10.589360505275465, -13.457686499889114, -15.188115648480595,\n",
            "      -13.451579699783416, -13.847447318744019, -15.62213315314085, -14.898431366057483,\n",
            "      -14.950176911069281, -12.878792173470302, -14.191468213559183, -14.482777466918787,\n",
            "      -11.96868688718963, -11.44340660485428, -12.448758889616082, -12.990992413430979,\n",
            "      -9.288925906749954, -12.958628622625204, -12.106768301855102, -13.467092482188749,\n",
            "      -9.248766661875116, -11.490535810078455, -13.725009139429709, -13.4130069295191,\n",
            "      -11.602683942103303, -14.50602504996919, -12.681462746674505, -14.656985864996866,\n",
            "      -12.660956971998862, -12.582657047159453, -16.363436598998874, -13.530862506330191,\n",
            "      -15.575662822289955, -14.478654851222322, -13.271152354473637, -10.200992463005514,\n",
            "      -11.172904321321422, -8.452631306227014, -10.387820214495893, -14.092866281389728,\n",
            "      -9.29984178372595, -10.216686893826026, -9.931663013262247, -12.783466943021752,\n",
            "      -13.781827702686751, -11.60078731948264, -10.372853254784438, -6.955653676378858,\n",
            "      -10.057021230324885]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2560107614280952\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1786961686199404\n",
            "    mean_inference_ms: 1.2810477415020949\n",
            "    mean_raw_obs_processing_ms: 0.36714371896913484\n",
            "time_since_restore: 514.524619102478\n",
            "time_this_iter_s: 1.0789470672607422\n",
            "time_total_s: 514.524619102478\n",
            "timers:\n",
            "  learn_throughput: 2292.803\n",
            "  learn_time_ms: 13.957\n",
            "  load_throughput: 174876.519\n",
            "  load_time_ms: 0.183\n",
            "  sample_time_ms: 87.972\n",
            "  synch_weights_time_ms: 0.024\n",
            "  training_iteration_time_ms: 118.881\n",
            "timestamp: 1703177890\n",
            "timesteps_total: 180950\n",
            "training_iteration: 481\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 181400\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006777048110961914\n",
            "  StateBufferConnector_ms: 0.005131959915161133\n",
            "  ViewRequirementAgentConnector_ms: 0.13715672492980957\n",
            "counters:\n",
            "  last_target_update_ts: 181400\n",
            "  num_agent_steps_sampled: 181400\n",
            "  num_agent_steps_trained: 115136\n",
            "  num_env_steps_sampled: 181400\n",
            "  num_env_steps_trained: 115136\n",
            "  num_target_updates: 3598\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-12\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.872272505312526\n",
            "episode_reward_mean: -11.654217521508512\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3628\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 181400\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1817.65625\n",
            "      learner_stats:\n",
            "        actor_loss: -20.243043899536133\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1284784078598022\n",
            "        alpha_value: 0.6992751359939575\n",
            "        critic_loss: 0.037313513457775116\n",
            "        grad_gnorm: 3.1547207832336426\n",
            "        log_alpha_value: -0.3577110171318054\n",
            "        max_q: 22.262632369995117\n",
            "        mean_q: 19.50346565246582\n",
            "        min_q: 13.80198860168457\n",
            "        policy_t: -0.22487474977970123\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.20742711424827576\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3598.0\n",
            "      td_error: [0.5960836410522461, 0.08164024353027344, 0.24779033660888672, 0.26453399658203125,\n",
            "        0.10581207275390625, 0.24049854278564453, 0.1796426773071289, 0.4894380569458008,\n",
            "        0.17829418182373047, 0.25426197052001953, 0.45446205139160156, 0.056883811950683594,\n",
            "        0.38480091094970703, 0.40587329864501953, 0.07819080352783203, 0.09310722351074219,\n",
            "        0.2010488510131836, 0.16295242309570312, 0.5678367614746094, 0.08055782318115234,\n",
            "        0.10443401336669922, 0.22225284576416016, 0.17183399200439453, 0.2561178207397461,\n",
            "        0.13013935089111328, 0.05884742736816406, 0.13284873962402344, 0.04803276062011719,\n",
            "        0.0829010009765625, 0.06557083129882812, 0.04632282257080078, 0.1946563720703125]\n",
            "  num_agent_steps_sampled: 181400\n",
            "  num_agent_steps_trained: 115136\n",
            "  num_env_steps_sampled: 181400\n",
            "  num_env_steps_trained: 115136\n",
            "  num_target_updates: 3598\n",
            "iterations_since_restore: 482\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 181400\n",
            "num_agent_steps_trained: 115136\n",
            "num_env_steps_sampled: 181400\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 420.6545983600273\n",
            "num_env_steps_trained: 115136\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 269.2189429504175\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 54.8\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25598793382952867\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.178683330445203\n",
            "  mean_inference_ms: 1.280845883566158\n",
            "  mean_raw_obs_processing_ms: 0.36708338745210595\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006777048110961914\n",
            "    StateBufferConnector_ms: 0.005131959915161133\n",
            "    ViewRequirementAgentConnector_ms: 0.13715672492980957\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.872272505312526\n",
            "  episode_reward_mean: -11.654217521508512\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-14.08938714283293, -9.150013534006034, -9.439512493063452, -13.951182533502232,\n",
            "      -12.05665465774798, -10.608063036244266, -14.777214783631942, -8.130244594842775,\n",
            "      -9.36960500636909, -6.872272505312526, -10.766278472460701, -7.929009053458728,\n",
            "      -13.373216398421135, -12.094972978989182, -10.533361932837124, -9.861880684544484,\n",
            "      -14.76786107475355, -9.637541311276973, -12.111276001648935, -7.659187504063094,\n",
            "      -10.654072144021406, -10.662708288246701, -12.157111678172402, -11.036723399105998,\n",
            "      -9.017724652339835, -10.383344214753725, -10.695380186558769, -9.479772956160454,\n",
            "      -9.285665126804158, -10.612019896672653, -10.867272274875523, -14.075178824852696,\n",
            "      -11.371093519317604, -12.521139836904533, -12.017776959414775, -9.161326716075497,\n",
            "      -12.177414047615514, -10.367973658502486, -9.778030988707343, -12.485545017073939,\n",
            "      -12.502553909244916, -14.804113064086984, -10.320976753197233, -10.589360505275465,\n",
            "      -13.457686499889114, -15.188115648480595, -13.451579699783416, -13.847447318744019,\n",
            "      -15.62213315314085, -14.898431366057483, -14.950176911069281, -12.878792173470302,\n",
            "      -14.191468213559183, -14.482777466918787, -11.96868688718963, -11.44340660485428,\n",
            "      -12.448758889616082, -12.990992413430979, -9.288925906749954, -12.958628622625204,\n",
            "      -12.106768301855102, -13.467092482188749, -9.248766661875116, -11.490535810078455,\n",
            "      -13.725009139429709, -13.4130069295191, -11.602683942103303, -14.50602504996919,\n",
            "      -12.681462746674505, -14.656985864996866, -12.660956971998862, -12.582657047159453,\n",
            "      -16.363436598998874, -13.530862506330191, -15.575662822289955, -14.478654851222322,\n",
            "      -13.271152354473637, -10.200992463005514, -11.172904321321422, -8.452631306227014,\n",
            "      -10.387820214495893, -14.092866281389728, -9.29984178372595, -10.216686893826026,\n",
            "      -9.931663013262247, -12.783466943021752, -13.781827702686751, -11.60078731948264,\n",
            "      -10.372853254784438, -6.955653676378858, -10.057021230324885, -9.956565312203937,\n",
            "      -10.605118753085424, -13.336397289312343, -12.701448365811682, -8.793272674064285,\n",
            "      -8.785132843290995, -7.8062557820477885, -10.059817316759577, -10.437985235611842]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25598793382952867\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.178683330445203\n",
            "    mean_inference_ms: 1.280845883566158\n",
            "    mean_raw_obs_processing_ms: 0.36708338745210595\n",
            "time_since_restore: 515.5954599380493\n",
            "time_this_iter_s: 1.070840835571289\n",
            "time_total_s: 515.5954599380493\n",
            "timers:\n",
            "  learn_throughput: 2388.722\n",
            "  learn_time_ms: 13.396\n",
            "  load_throughput: 191849.24\n",
            "  load_time_ms: 0.167\n",
            "  sample_time_ms: 90.053\n",
            "  synch_weights_time_ms: 0.023\n",
            "  training_iteration_time_ms: 119.1\n",
            "timestamp: 1703177892\n",
            "timesteps_total: 181400\n",
            "training_iteration: 482\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 181850\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.00673985481262207\n",
            "  StateBufferConnector_ms: 0.0050563812255859375\n",
            "  ViewRequirementAgentConnector_ms: 0.1349925994873047\n",
            "counters:\n",
            "  last_target_update_ts: 181850\n",
            "  num_agent_steps_sampled: 181850\n",
            "  num_agent_steps_trained: 115424\n",
            "  num_env_steps_sampled: 181850\n",
            "  num_env_steps_trained: 115424\n",
            "  num_target_updates: 3607\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-13\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.872272505312526\n",
            "episode_reward_mean: -11.589024016419158\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3637\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 181850\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1827.78125\n",
            "      learner_stats:\n",
            "        actor_loss: -20.883350372314453\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1275526285171509\n",
            "        alpha_value: 0.6986539959907532\n",
            "        critic_loss: 0.045523229986429214\n",
            "        grad_gnorm: 3.1443214416503906\n",
            "        log_alpha_value: -0.35859963297843933\n",
            "        max_q: 22.042932510375977\n",
            "        mean_q: 20.19498062133789\n",
            "        min_q: 17.069326400756836\n",
            "        policy_t: -0.15986476838588715\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.21144479513168335\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3607.0\n",
            "      td_error: [0.7407064437866211, 0.17885684967041016, 0.04964160919189453, 0.5305290222167969,\n",
            "        0.2709217071533203, 0.0189208984375, 0.1114950180053711, 0.4983816146850586,\n",
            "        0.03986835479736328, 0.040648460388183594, 0.05670166015625, 0.17926311492919922,\n",
            "        0.0264739990234375, 0.05686759948730469, 0.10925483703613281, 0.24053001403808594,\n",
            "        0.40371036529541016, 0.01565074920654297, 0.535578727722168, 0.071533203125,\n",
            "        0.2053661346435547, 0.5038909912109375, 0.37459468841552734, 0.13019847869873047,\n",
            "        0.03277111053466797, 0.2492046356201172, 0.07174110412597656, 0.18303298950195312,\n",
            "        0.2677488327026367, 0.4249248504638672, 0.05413055419921875, 0.0930948257446289]\n",
            "  num_agent_steps_sampled: 181850\n",
            "  num_agent_steps_trained: 115424\n",
            "  num_env_steps_sampled: 181850\n",
            "  num_env_steps_trained: 115424\n",
            "  num_target_updates: 3607\n",
            "iterations_since_restore: 483\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 181850\n",
            "num_agent_steps_trained: 115424\n",
            "num_env_steps_sampled: 181850\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 431.7513358433318\n",
            "num_env_steps_trained: 115424\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 276.3208549397324\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 55.4\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2559629094046512\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17866862797445748\n",
            "  mean_inference_ms: 1.2806235343987902\n",
            "  mean_raw_obs_processing_ms: 0.3670177775142369\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.00673985481262207\n",
            "    StateBufferConnector_ms: 0.0050563812255859375\n",
            "    ViewRequirementAgentConnector_ms: 0.1349925994873047\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.872272505312526\n",
            "  episode_reward_mean: -11.589024016419158\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-6.872272505312526, -10.766278472460701, -7.929009053458728,\n",
            "      -13.373216398421135, -12.094972978989182, -10.533361932837124, -9.861880684544484,\n",
            "      -14.76786107475355, -9.637541311276973, -12.111276001648935, -7.659187504063094,\n",
            "      -10.654072144021406, -10.662708288246701, -12.157111678172402, -11.036723399105998,\n",
            "      -9.017724652339835, -10.383344214753725, -10.695380186558769, -9.479772956160454,\n",
            "      -9.285665126804158, -10.612019896672653, -10.867272274875523, -14.075178824852696,\n",
            "      -11.371093519317604, -12.521139836904533, -12.017776959414775, -9.161326716075497,\n",
            "      -12.177414047615514, -10.367973658502486, -9.778030988707343, -12.485545017073939,\n",
            "      -12.502553909244916, -14.804113064086984, -10.320976753197233, -10.589360505275465,\n",
            "      -13.457686499889114, -15.188115648480595, -13.451579699783416, -13.847447318744019,\n",
            "      -15.62213315314085, -14.898431366057483, -14.950176911069281, -12.878792173470302,\n",
            "      -14.191468213559183, -14.482777466918787, -11.96868688718963, -11.44340660485428,\n",
            "      -12.448758889616082, -12.990992413430979, -9.288925906749954, -12.958628622625204,\n",
            "      -12.106768301855102, -13.467092482188749, -9.248766661875116, -11.490535810078455,\n",
            "      -13.725009139429709, -13.4130069295191, -11.602683942103303, -14.50602504996919,\n",
            "      -12.681462746674505, -14.656985864996866, -12.660956971998862, -12.582657047159453,\n",
            "      -16.363436598998874, -13.530862506330191, -15.575662822289955, -14.478654851222322,\n",
            "      -13.271152354473637, -10.200992463005514, -11.172904321321422, -8.452631306227014,\n",
            "      -10.387820214495893, -14.092866281389728, -9.29984178372595, -10.216686893826026,\n",
            "      -9.931663013262247, -12.783466943021752, -13.781827702686751, -11.60078731948264,\n",
            "      -10.372853254784438, -6.955653676378858, -10.057021230324885, -9.956565312203937,\n",
            "      -10.605118753085424, -13.336397289312343, -12.701448365811682, -8.793272674064285,\n",
            "      -8.785132843290995, -7.8062557820477885, -10.059817316759577, -10.437985235611842,\n",
            "      -9.001489063683344, -11.86083444784972, -11.671557779247351, -7.637880278577952,\n",
            "      -12.318618853337258, -8.521966043037677, -10.301633436987107, -11.473101558126032,\n",
            "      -12.265445812458726]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2559629094046512\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17866862797445748\n",
            "    mean_inference_ms: 1.2806235343987902\n",
            "    mean_raw_obs_processing_ms: 0.3670177775142369\n",
            "time_since_restore: 516.6388833522797\n",
            "time_this_iter_s: 1.0434234142303467\n",
            "time_total_s: 516.6388833522797\n",
            "timers:\n",
            "  learn_throughput: 2500.242\n",
            "  learn_time_ms: 12.799\n",
            "  load_throughput: 186258.296\n",
            "  load_time_ms: 0.172\n",
            "  sample_time_ms: 87.686\n",
            "  synch_weights_time_ms: 0.024\n",
            "  training_iteration_time_ms: 116.169\n",
            "timestamp: 1703177893\n",
            "timesteps_total: 181850\n",
            "training_iteration: 483\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 182300\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.0067098140716552734\n",
            "  StateBufferConnector_ms: 0.005042552947998047\n",
            "  ViewRequirementAgentConnector_ms: 0.1338353157043457\n",
            "counters:\n",
            "  last_target_update_ts: 182300\n",
            "  num_agent_steps_sampled: 182300\n",
            "  num_agent_steps_trained: 115712\n",
            "  num_env_steps_sampled: 182300\n",
            "  num_env_steps_trained: 115712\n",
            "  num_target_updates: 3616\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-14\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.955653676378858\n",
            "episode_reward_mean: -11.630823618002347\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3646\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 182300\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1453.8125\n",
            "      learner_stats:\n",
            "        actor_loss: -20.943302154541016\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.148857831954956\n",
            "        alpha_value: 0.6980360746383667\n",
            "        critic_loss: 0.0789346992969513\n",
            "        grad_gnorm: 3.195848226547241\n",
            "        log_alpha_value: -0.35948446393013\n",
            "        max_q: 22.351482391357422\n",
            "        mean_q: 20.248714447021484\n",
            "        min_q: 16.461984634399414\n",
            "        policy_t: -0.1919877827167511\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.3120456635951996\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3616.0\n",
            "      td_error: [0.11680889129638672, 0.26066017150878906, 0.17627716064453125, 0.1900959014892578,\n",
            "        0.759613037109375, 0.5382671356201172, 0.22077083587646484, 0.7960109710693359,\n",
            "        0.04303550720214844, 0.29337120056152344, 0.0901327133178711, 1.0432777404785156,\n",
            "        0.39827728271484375, 0.33817100524902344, 0.2614145278930664, 0.07263565063476562,\n",
            "        0.4837303161621094, 0.5201416015625, 0.06186962127685547, 0.20444679260253906,\n",
            "        0.4203977584838867, 0.4431734085083008, 0.44254207611083984, 0.12238693237304688,\n",
            "        0.40633296966552734, 0.12256336212158203, 0.21155357360839844, 0.2821979522705078,\n",
            "        0.1283130645751953, 0.3662586212158203, 0.12589073181152344, 0.04484272003173828]\n",
            "  num_agent_steps_sampled: 182300\n",
            "  num_agent_steps_trained: 115712\n",
            "  num_env_steps_sampled: 182300\n",
            "  num_env_steps_trained: 115712\n",
            "  num_target_updates: 3616\n",
            "iterations_since_restore: 484\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 182300\n",
            "num_agent_steps_trained: 115712\n",
            "num_env_steps_sampled: 182300\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 422.15895403960934\n",
            "num_env_steps_trained: 115712\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 270.18173058534995\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 54.65\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25593395066926905\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.1786500582847177\n",
            "  mean_inference_ms: 1.28037184934919\n",
            "  mean_raw_obs_processing_ms: 0.3669433173921393\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.0067098140716552734\n",
            "    StateBufferConnector_ms: 0.005042552947998047\n",
            "    ViewRequirementAgentConnector_ms: 0.1338353157043457\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.955653676378858\n",
            "  episode_reward_mean: -11.630823618002347\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-12.111276001648935, -7.659187504063094, -10.654072144021406,\n",
            "      -10.662708288246701, -12.157111678172402, -11.036723399105998, -9.017724652339835,\n",
            "      -10.383344214753725, -10.695380186558769, -9.479772956160454, -9.285665126804158,\n",
            "      -10.612019896672653, -10.867272274875523, -14.075178824852696, -11.371093519317604,\n",
            "      -12.521139836904533, -12.017776959414775, -9.161326716075497, -12.177414047615514,\n",
            "      -10.367973658502486, -9.778030988707343, -12.485545017073939, -12.502553909244916,\n",
            "      -14.804113064086984, -10.320976753197233, -10.589360505275465, -13.457686499889114,\n",
            "      -15.188115648480595, -13.451579699783416, -13.847447318744019, -15.62213315314085,\n",
            "      -14.898431366057483, -14.950176911069281, -12.878792173470302, -14.191468213559183,\n",
            "      -14.482777466918787, -11.96868688718963, -11.44340660485428, -12.448758889616082,\n",
            "      -12.990992413430979, -9.288925906749954, -12.958628622625204, -12.106768301855102,\n",
            "      -13.467092482188749, -9.248766661875116, -11.490535810078455, -13.725009139429709,\n",
            "      -13.4130069295191, -11.602683942103303, -14.50602504996919, -12.681462746674505,\n",
            "      -14.656985864996866, -12.660956971998862, -12.582657047159453, -16.363436598998874,\n",
            "      -13.530862506330191, -15.575662822289955, -14.478654851222322, -13.271152354473637,\n",
            "      -10.200992463005514, -11.172904321321422, -8.452631306227014, -10.387820214495893,\n",
            "      -14.092866281389728, -9.29984178372595, -10.216686893826026, -9.931663013262247,\n",
            "      -12.783466943021752, -13.781827702686751, -11.60078731948264, -10.372853254784438,\n",
            "      -6.955653676378858, -10.057021230324885, -9.956565312203937, -10.605118753085424,\n",
            "      -13.336397289312343, -12.701448365811682, -8.793272674064285, -8.785132843290995,\n",
            "      -7.8062557820477885, -10.059817316759577, -10.437985235611842, -9.001489063683344,\n",
            "      -11.86083444784972, -11.671557779247351, -7.637880278577952, -12.318618853337258,\n",
            "      -8.521966043037677, -10.301633436987107, -11.473101558126032, -12.265445812458726,\n",
            "      -7.986131024147747, -10.192114910569366, -13.384245530030281, -10.133525859099928,\n",
            "      -11.501064798051566, -9.679769061071617, -12.524546571843672, -10.20470978286785,\n",
            "      -14.410247032691405]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25593395066926905\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1786500582847177\n",
            "    mean_inference_ms: 1.28037184934919\n",
            "    mean_raw_obs_processing_ms: 0.3669433173921393\n",
            "time_since_restore: 517.7059693336487\n",
            "time_this_iter_s: 1.0670859813690186\n",
            "time_total_s: 517.7059693336487\n",
            "timers:\n",
            "  learn_throughput: 2501.575\n",
            "  learn_time_ms: 12.792\n",
            "  load_throughput: 189948.667\n",
            "  load_time_ms: 0.168\n",
            "  sample_time_ms: 88.683\n",
            "  synch_weights_time_ms: 0.026\n",
            "  training_iteration_time_ms: 117.729\n",
            "timestamp: 1703177894\n",
            "timesteps_total: 182300\n",
            "training_iteration: 484\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 182650\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.00661015510559082\n",
            "  StateBufferConnector_ms: 0.005036354064941406\n",
            "  ViewRequirementAgentConnector_ms: 0.13323688507080078\n",
            "counters:\n",
            "  last_target_update_ts: 182650\n",
            "  num_agent_steps_sampled: 182650\n",
            "  num_agent_steps_trained: 115936\n",
            "  num_env_steps_sampled: 182650\n",
            "  num_env_steps_trained: 115936\n",
            "  num_target_updates: 3623\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-15\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.955653676378858\n",
            "episode_reward_mean: -11.681207632143826\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 7\n",
            "episodes_total: 3653\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 182650\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1985.71875\n",
            "      learner_stats:\n",
            "        actor_loss: -20.604652404785156\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1603500843048096\n",
            "        alpha_value: 0.6975530385971069\n",
            "        critic_loss: 0.0645836889743805\n",
            "        grad_gnorm: 3.2216129302978516\n",
            "        log_alpha_value: -0.3601767122745514\n",
            "        max_q: 22.23540687561035\n",
            "        mean_q: 19.948843002319336\n",
            "        min_q: 16.972923278808594\n",
            "        policy_t: -0.19664502143859863\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.30156704783439636\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3623.0\n",
            "      td_error: [0.09315109252929688, 0.1878223419189453, 0.10869216918945312, 0.15320110321044922,\n",
            "        0.08815670013427734, 0.2771272659301758, 0.08316802978515625, 0.10063743591308594,\n",
            "        0.5813236236572266, 0.1466693878173828, 0.1450347900390625, 0.2376413345336914,\n",
            "        0.7356014251708984, 0.5348262786865234, 0.2913961410522461, 0.47212791442871094,\n",
            "        0.5567607879638672, 0.13054275512695312, 0.42296504974365234, 0.5995149612426758,\n",
            "        0.11753177642822266, 0.3419342041015625, 0.5700712203979492, 0.1737957000732422,\n",
            "        0.46182823181152344, 0.19199848175048828, 0.5499544143676758, 0.29416465759277344,\n",
            "        0.3045482635498047, 0.26203155517578125, 0.3375978469848633, 0.0983285903930664]\n",
            "  num_agent_steps_sampled: 182650\n",
            "  num_agent_steps_trained: 115936\n",
            "  num_env_steps_sampled: 182650\n",
            "  num_env_steps_trained: 115936\n",
            "  num_target_updates: 3623\n",
            "iterations_since_restore: 485\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 182650\n",
            "num_agent_steps_trained: 115936\n",
            "num_env_steps_sampled: 182650\n",
            "num_env_steps_sampled_this_iter: 350\n",
            "num_env_steps_sampled_throughput_per_sec: 323.8884090154431\n",
            "num_env_steps_trained: 115936\n",
            "num_env_steps_trained_this_iter: 224\n",
            "num_env_steps_trained_throughput_per_sec: 207.2885817698836\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 224\n",
            "perf:\n",
            "  cpu_util_percent: 76.85\n",
            "  ram_util_percent: 21.5\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2559081313477992\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17863386898660216\n",
            "  mean_inference_ms: 1.280164993168163\n",
            "  mean_raw_obs_processing_ms: 0.3668833824044496\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.00661015510559082\n",
            "    StateBufferConnector_ms: 0.005036354064941406\n",
            "    ViewRequirementAgentConnector_ms: 0.13323688507080078\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.955653676378858\n",
            "  episode_reward_mean: -11.681207632143826\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 7\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-10.383344214753725, -10.695380186558769, -9.479772956160454,\n",
            "      -9.285665126804158, -10.612019896672653, -10.867272274875523, -14.075178824852696,\n",
            "      -11.371093519317604, -12.521139836904533, -12.017776959414775, -9.161326716075497,\n",
            "      -12.177414047615514, -10.367973658502486, -9.778030988707343, -12.485545017073939,\n",
            "      -12.502553909244916, -14.804113064086984, -10.320976753197233, -10.589360505275465,\n",
            "      -13.457686499889114, -15.188115648480595, -13.451579699783416, -13.847447318744019,\n",
            "      -15.62213315314085, -14.898431366057483, -14.950176911069281, -12.878792173470302,\n",
            "      -14.191468213559183, -14.482777466918787, -11.96868688718963, -11.44340660485428,\n",
            "      -12.448758889616082, -12.990992413430979, -9.288925906749954, -12.958628622625204,\n",
            "      -12.106768301855102, -13.467092482188749, -9.248766661875116, -11.490535810078455,\n",
            "      -13.725009139429709, -13.4130069295191, -11.602683942103303, -14.50602504996919,\n",
            "      -12.681462746674505, -14.656985864996866, -12.660956971998862, -12.582657047159453,\n",
            "      -16.363436598998874, -13.530862506330191, -15.575662822289955, -14.478654851222322,\n",
            "      -13.271152354473637, -10.200992463005514, -11.172904321321422, -8.452631306227014,\n",
            "      -10.387820214495893, -14.092866281389728, -9.29984178372595, -10.216686893826026,\n",
            "      -9.931663013262247, -12.783466943021752, -13.781827702686751, -11.60078731948264,\n",
            "      -10.372853254784438, -6.955653676378858, -10.057021230324885, -9.956565312203937,\n",
            "      -10.605118753085424, -13.336397289312343, -12.701448365811682, -8.793272674064285,\n",
            "      -8.785132843290995, -7.8062557820477885, -10.059817316759577, -10.437985235611842,\n",
            "      -9.001489063683344, -11.86083444784972, -11.671557779247351, -7.637880278577952,\n",
            "      -12.318618853337258, -8.521966043037677, -10.301633436987107, -11.473101558126032,\n",
            "      -12.265445812458726, -7.986131024147747, -10.192114910569366, -13.384245530030281,\n",
            "      -10.133525859099928, -11.501064798051566, -9.679769061071617, -12.524546571843672,\n",
            "      -10.20470978286785, -14.410247032691405, -13.907524002295625, -10.942415536969854,\n",
            "      -7.494713511935938, -11.327591801455982, -12.335075489596791, -10.7559021248706,\n",
            "      -11.573982614621317]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2559081313477992\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17863386898660216\n",
            "    mean_inference_ms: 1.280164993168163\n",
            "    mean_raw_obs_processing_ms: 0.3668833824044496\n",
            "time_since_restore: 518.7881505489349\n",
            "time_this_iter_s: 1.0821812152862549\n",
            "time_total_s: 518.7881505489349\n",
            "timers:\n",
            "  learn_throughput: 2005.417\n",
            "  learn_time_ms: 15.957\n",
            "  load_throughput: 135191.104\n",
            "  load_time_ms: 0.237\n",
            "  sample_time_ms: 106.43\n",
            "  synch_weights_time_ms: 0.028\n",
            "  training_iteration_time_ms: 143.522\n",
            "timestamp: 1703177895\n",
            "timesteps_total: 182650\n",
            "training_iteration: 485\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 183000\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006809234619140625\n",
            "  StateBufferConnector_ms: 0.004941463470458984\n",
            "  ViewRequirementAgentConnector_ms: 0.12953686714172363\n",
            "counters:\n",
            "  last_target_update_ts: 183000\n",
            "  num_agent_steps_sampled: 183000\n",
            "  num_agent_steps_trained: 116160\n",
            "  num_env_steps_sampled: 183000\n",
            "  num_env_steps_trained: 116160\n",
            "  num_target_updates: 3630\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-16\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.955653676378858\n",
            "episode_reward_mean: -11.716302765858089\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 7\n",
            "episodes_total: 3660\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 183000\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1653.40625\n",
            "      learner_stats:\n",
            "        actor_loss: -20.159666061401367\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1604617834091187\n",
            "        alpha_value: 0.6970722675323486\n",
            "        critic_loss: 0.07618485391139984\n",
            "        grad_gnorm: 3.2157673835754395\n",
            "        log_alpha_value: -0.3608661890029907\n",
            "        max_q: 22.79654312133789\n",
            "        mean_q: 19.436281204223633\n",
            "        min_q: 12.078391075134277\n",
            "        policy_t: -0.23165695369243622\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.27651914954185486\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3630.0\n",
            "      td_error: [0.054581642150878906, 1.0421218872070312, 0.24209356307983398, 0.13242101669311523,\n",
            "        0.0756988525390625, 0.19454193115234375, 0.047158241271972656, 0.08181095123291016,\n",
            "        0.42865753173828125, 0.061331748962402344, 0.8833475112915039, 0.3569211959838867,\n",
            "        0.049556732177734375, 0.07456302642822266, 0.34772777557373047, 0.3517942428588867,\n",
            "        0.14796733856201172, 0.15297508239746094, 0.22633838653564453, 0.3522624969482422,\n",
            "        0.018182754516601562, 0.15184497833251953, 0.15387725830078125, 0.4998931884765625,\n",
            "        0.4105682373046875, 0.23494338989257812, 0.8835868835449219, 0.06511402130126953,\n",
            "        0.24017715454101562, 0.8002395629882812, 0.03808307647705078, 0.04823112487792969]\n",
            "  num_agent_steps_sampled: 183000\n",
            "  num_agent_steps_trained: 116160\n",
            "  num_env_steps_sampled: 183000\n",
            "  num_env_steps_trained: 116160\n",
            "  num_target_updates: 3630\n",
            "iterations_since_restore: 486\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 183000\n",
            "num_agent_steps_trained: 116160\n",
            "num_env_steps_sampled: 183000\n",
            "num_env_steps_sampled_this_iter: 350\n",
            "num_env_steps_sampled_throughput_per_sec: 303.400509661032\n",
            "num_env_steps_trained: 116160\n",
            "num_env_steps_trained_this_iter: 224\n",
            "num_env_steps_trained_throughput_per_sec: 194.1763261830605\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 224\n",
            "perf:\n",
            "  cpu_util_percent: 99.3\n",
            "  ram_util_percent: 21.6\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25588056970758283\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17861821787086649\n",
            "  mean_inference_ms: 1.2799544520502917\n",
            "  mean_raw_obs_processing_ms: 0.3668233883274216\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006809234619140625\n",
            "    StateBufferConnector_ms: 0.004941463470458984\n",
            "    ViewRequirementAgentConnector_ms: 0.12953686714172363\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.955653676378858\n",
            "  episode_reward_mean: -11.716302765858089\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 7\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-11.371093519317604, -12.521139836904533, -12.017776959414775,\n",
            "      -9.161326716075497, -12.177414047615514, -10.367973658502486, -9.778030988707343,\n",
            "      -12.485545017073939, -12.502553909244916, -14.804113064086984, -10.320976753197233,\n",
            "      -10.589360505275465, -13.457686499889114, -15.188115648480595, -13.451579699783416,\n",
            "      -13.847447318744019, -15.62213315314085, -14.898431366057483, -14.950176911069281,\n",
            "      -12.878792173470302, -14.191468213559183, -14.482777466918787, -11.96868688718963,\n",
            "      -11.44340660485428, -12.448758889616082, -12.990992413430979, -9.288925906749954,\n",
            "      -12.958628622625204, -12.106768301855102, -13.467092482188749, -9.248766661875116,\n",
            "      -11.490535810078455, -13.725009139429709, -13.4130069295191, -11.602683942103303,\n",
            "      -14.50602504996919, -12.681462746674505, -14.656985864996866, -12.660956971998862,\n",
            "      -12.582657047159453, -16.363436598998874, -13.530862506330191, -15.575662822289955,\n",
            "      -14.478654851222322, -13.271152354473637, -10.200992463005514, -11.172904321321422,\n",
            "      -8.452631306227014, -10.387820214495893, -14.092866281389728, -9.29984178372595,\n",
            "      -10.216686893826026, -9.931663013262247, -12.783466943021752, -13.781827702686751,\n",
            "      -11.60078731948264, -10.372853254784438, -6.955653676378858, -10.057021230324885,\n",
            "      -9.956565312203937, -10.605118753085424, -13.336397289312343, -12.701448365811682,\n",
            "      -8.793272674064285, -8.785132843290995, -7.8062557820477885, -10.059817316759577,\n",
            "      -10.437985235611842, -9.001489063683344, -11.86083444784972, -11.671557779247351,\n",
            "      -7.637880278577952, -12.318618853337258, -8.521966043037677, -10.301633436987107,\n",
            "      -11.473101558126032, -12.265445812458726, -7.986131024147747, -10.192114910569366,\n",
            "      -13.384245530030281, -10.133525859099928, -11.501064798051566, -9.679769061071617,\n",
            "      -12.524546571843672, -10.20470978286785, -14.410247032691405, -13.907524002295625,\n",
            "      -10.942415536969854, -7.494713511935938, -11.327591801455982, -12.335075489596791,\n",
            "      -10.7559021248706, -11.573982614621317, -7.768217495921097, -14.646346698979436,\n",
            "      -11.565832496338132, -12.599258405032321, -13.937999786108426, -8.695149631906453,\n",
            "      -9.695342337818554]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25588056970758283\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17861821787086649\n",
            "    mean_inference_ms: 1.2799544520502917\n",
            "    mean_raw_obs_processing_ms: 0.3668233883274216\n",
            "time_since_restore: 519.9433436393738\n",
            "time_this_iter_s: 1.1551930904388428\n",
            "time_total_s: 519.9433436393738\n",
            "timers:\n",
            "  learn_throughput: 1711.852\n",
            "  learn_time_ms: 18.693\n",
            "  load_throughput: 118493.624\n",
            "  load_time_ms: 0.27\n",
            "  sample_time_ms: 120.933\n",
            "  synch_weights_time_ms: 0.034\n",
            "  training_iteration_time_ms: 164.87\n",
            "timestamp: 1703177896\n",
            "timesteps_total: 183000\n",
            "training_iteration: 486\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 183300\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006794929504394531\n",
            "  StateBufferConnector_ms: 0.004914522171020508\n",
            "  ViewRequirementAgentConnector_ms: 0.13014507293701172\n",
            "counters:\n",
            "  last_target_update_ts: 183300\n",
            "  num_agent_steps_sampled: 183300\n",
            "  num_agent_steps_trained: 116352\n",
            "  num_env_steps_sampled: 183300\n",
            "  num_env_steps_trained: 116352\n",
            "  num_target_updates: 3636\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-17\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.955653676378858\n",
            "episode_reward_mean: -11.740381835990805\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 6\n",
            "episodes_total: 3666\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 183300\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1840.09375\n",
            "      learner_stats:\n",
            "        actor_loss: -20.1569881439209\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.2244703769683838\n",
            "        alpha_value: 0.6966623067855835\n",
            "        critic_loss: 0.043536752462387085\n",
            "        grad_gnorm: 3.387619972229004\n",
            "        log_alpha_value: -0.3614545166492462\n",
            "        max_q: 22.938039779663086\n",
            "        mean_q: 19.44666290283203\n",
            "        min_q: 14.04319953918457\n",
            "        policy_t: -0.10226217657327652\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.246368408203125\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3636.0\n",
            "      td_error: [0.2216329574584961, 0.2814760208129883, 0.026200294494628906, 0.32003307342529297,\n",
            "        0.13983440399169922, 0.3383340835571289, 0.2253427505493164, 0.2594490051269531,\n",
            "        0.3244915008544922, 0.1827373504638672, 0.04132843017578125, 0.10225677490234375,\n",
            "        0.2156696319580078, 0.37683773040771484, 0.06452274322509766, 0.5708818435668945,\n",
            "        0.21908283233642578, 0.1610736846923828, 0.3886556625366211, 0.31491661071777344,\n",
            "        0.1405935287475586, 0.5582847595214844, 0.4152669906616211, 0.18191242218017578,\n",
            "        0.3642292022705078, 0.12371158599853516, 0.1210012435913086, 0.1196279525756836,\n",
            "        0.022637367248535156, 0.45751953125, 0.03496551513671875, 0.5692815780639648]\n",
            "  num_agent_steps_sampled: 183300\n",
            "  num_agent_steps_trained: 116352\n",
            "  num_env_steps_sampled: 183300\n",
            "  num_env_steps_trained: 116352\n",
            "  num_target_updates: 3636\n",
            "iterations_since_restore: 487\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 183300\n",
            "num_agent_steps_trained: 116352\n",
            "num_env_steps_sampled: 183300\n",
            "num_env_steps_sampled_this_iter: 300\n",
            "num_env_steps_sampled_throughput_per_sec: 298.17355277114007\n",
            "num_env_steps_trained: 116352\n",
            "num_env_steps_trained_this_iter: 192\n",
            "num_env_steps_trained_throughput_per_sec: 190.83107377352965\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 192\n",
            "perf:\n",
            "  cpu_util_percent: 98.94999999999999\n",
            "  ram_util_percent: 21.6\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25585721655133403\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17860558025621454\n",
            "  mean_inference_ms: 1.27976943657395\n",
            "  mean_raw_obs_processing_ms: 0.3667699336003777\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006794929504394531\n",
            "    StateBufferConnector_ms: 0.004914522171020508\n",
            "    ViewRequirementAgentConnector_ms: 0.13014507293701172\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.955653676378858\n",
            "  episode_reward_mean: -11.740381835990805\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 6\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-9.778030988707343, -12.485545017073939, -12.502553909244916,\n",
            "      -14.804113064086984, -10.320976753197233, -10.589360505275465, -13.457686499889114,\n",
            "      -15.188115648480595, -13.451579699783416, -13.847447318744019, -15.62213315314085,\n",
            "      -14.898431366057483, -14.950176911069281, -12.878792173470302, -14.191468213559183,\n",
            "      -14.482777466918787, -11.96868688718963, -11.44340660485428, -12.448758889616082,\n",
            "      -12.990992413430979, -9.288925906749954, -12.958628622625204, -12.106768301855102,\n",
            "      -13.467092482188749, -9.248766661875116, -11.490535810078455, -13.725009139429709,\n",
            "      -13.4130069295191, -11.602683942103303, -14.50602504996919, -12.681462746674505,\n",
            "      -14.656985864996866, -12.660956971998862, -12.582657047159453, -16.363436598998874,\n",
            "      -13.530862506330191, -15.575662822289955, -14.478654851222322, -13.271152354473637,\n",
            "      -10.200992463005514, -11.172904321321422, -8.452631306227014, -10.387820214495893,\n",
            "      -14.092866281389728, -9.29984178372595, -10.216686893826026, -9.931663013262247,\n",
            "      -12.783466943021752, -13.781827702686751, -11.60078731948264, -10.372853254784438,\n",
            "      -6.955653676378858, -10.057021230324885, -9.956565312203937, -10.605118753085424,\n",
            "      -13.336397289312343, -12.701448365811682, -8.793272674064285, -8.785132843290995,\n",
            "      -7.8062557820477885, -10.059817316759577, -10.437985235611842, -9.001489063683344,\n",
            "      -11.86083444784972, -11.671557779247351, -7.637880278577952, -12.318618853337258,\n",
            "      -8.521966043037677, -10.301633436987107, -11.473101558126032, -12.265445812458726,\n",
            "      -7.986131024147747, -10.192114910569366, -13.384245530030281, -10.133525859099928,\n",
            "      -11.501064798051566, -9.679769061071617, -12.524546571843672, -10.20470978286785,\n",
            "      -14.410247032691405, -13.907524002295625, -10.942415536969854, -7.494713511935938,\n",
            "      -11.327591801455982, -12.335075489596791, -10.7559021248706, -11.573982614621317,\n",
            "      -7.768217495921097, -14.646346698979436, -11.565832496338132, -12.599258405032321,\n",
            "      -13.937999786108426, -8.695149631906453, -9.695342337818554, -10.51615949535363,\n",
            "      -14.74851655527017, -10.679266720574677, -12.074538371261678, -11.081020212110241,\n",
            "      -10.925130396531353]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25585721655133403\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17860558025621454\n",
            "    mean_inference_ms: 1.27976943657395\n",
            "    mean_raw_obs_processing_ms: 0.3667699336003777\n",
            "time_since_restore: 520.9510045051575\n",
            "time_this_iter_s: 1.0076608657836914\n",
            "time_total_s: 520.9510045051575\n",
            "timers:\n",
            "  learn_throughput: 1726.562\n",
            "  learn_time_ms: 18.534\n",
            "  load_throughput: 116215.887\n",
            "  load_time_ms: 0.275\n",
            "  sample_time_ms: 123.213\n",
            "  synch_weights_time_ms: 0.033\n",
            "  training_iteration_time_ms: 167.158\n",
            "timestamp: 1703177897\n",
            "timesteps_total: 183300\n",
            "training_iteration: 487\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 183650\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006837129592895508\n",
            "  StateBufferConnector_ms: 0.0049479007720947266\n",
            "  ViewRequirementAgentConnector_ms: 0.13202595710754395\n",
            "counters:\n",
            "  last_target_update_ts: 183650\n",
            "  num_agent_steps_sampled: 183650\n",
            "  num_agent_steps_trained: 116576\n",
            "  num_env_steps_sampled: 183650\n",
            "  num_env_steps_trained: 116576\n",
            "  num_target_updates: 3643\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-18\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.955653676378858\n",
            "episode_reward_mean: -11.657714465533576\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 7\n",
            "episodes_total: 3673\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 183650\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1857.53125\n",
            "      learner_stats:\n",
            "        actor_loss: -20.272478103637695\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.221754789352417\n",
            "        alpha_value: 0.6961830258369446\n",
            "        critic_loss: 0.10111185908317566\n",
            "        grad_gnorm: 3.3736841678619385\n",
            "        log_alpha_value: -0.3621426522731781\n",
            "        max_q: 22.15167236328125\n",
            "        mean_q: 19.49958610534668\n",
            "        min_q: 13.394567489624023\n",
            "        policy_t: -0.09334741532802582\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.3635321259498596\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3643.0\n",
            "      td_error: [0.2112874984741211, 0.5932836532592773, 0.781926155090332, 0.4579954147338867,\n",
            "        0.10813713073730469, 0.3636817932128906, 0.7903833389282227, 0.31936168670654297,\n",
            "        0.49305057525634766, 0.09249210357666016, 0.6984171867370605, 0.10667800903320312,\n",
            "        0.12729167938232422, 0.34865665435791016, 0.05671977996826172, 0.28952693939208984,\n",
            "        0.658355712890625, 0.7016744613647461, 0.10793495178222656, 0.8804826736450195,\n",
            "        0.4946479797363281, 0.31120777130126953, 0.23415470123291016, 0.11437225341796875,\n",
            "        0.15732860565185547, 0.6831684112548828, 0.11315345764160156, 0.26098060607910156,\n",
            "        0.5418882369995117, 0.14533233642578125, 0.11319637298583984, 0.2762603759765625]\n",
            "  num_agent_steps_sampled: 183650\n",
            "  num_agent_steps_trained: 116576\n",
            "  num_env_steps_sampled: 183650\n",
            "  num_env_steps_trained: 116576\n",
            "  num_target_updates: 3643\n",
            "iterations_since_restore: 488\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 183650\n",
            "num_agent_steps_trained: 116576\n",
            "num_env_steps_sampled: 183650\n",
            "num_env_steps_sampled_this_iter: 350\n",
            "num_env_steps_sampled_throughput_per_sec: 302.1186699674565\n",
            "num_env_steps_trained: 116576\n",
            "num_env_steps_trained_this_iter: 224\n",
            "num_env_steps_trained_throughput_per_sec: 193.35594877917217\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 224\n",
            "perf:\n",
            "  cpu_util_percent: 99.3\n",
            "  ram_util_percent: 21.45\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2558338838268708\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17859628849659934\n",
            "  mean_inference_ms: 1.2795720161189377\n",
            "  mean_raw_obs_processing_ms: 0.3667194000491645\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006837129592895508\n",
            "    StateBufferConnector_ms: 0.0049479007720947266\n",
            "    ViewRequirementAgentConnector_ms: 0.13202595710754395\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.955653676378858\n",
            "  episode_reward_mean: -11.657714465533576\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 7\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-15.188115648480595, -13.451579699783416, -13.847447318744019,\n",
            "      -15.62213315314085, -14.898431366057483, -14.950176911069281, -12.878792173470302,\n",
            "      -14.191468213559183, -14.482777466918787, -11.96868688718963, -11.44340660485428,\n",
            "      -12.448758889616082, -12.990992413430979, -9.288925906749954, -12.958628622625204,\n",
            "      -12.106768301855102, -13.467092482188749, -9.248766661875116, -11.490535810078455,\n",
            "      -13.725009139429709, -13.4130069295191, -11.602683942103303, -14.50602504996919,\n",
            "      -12.681462746674505, -14.656985864996866, -12.660956971998862, -12.582657047159453,\n",
            "      -16.363436598998874, -13.530862506330191, -15.575662822289955, -14.478654851222322,\n",
            "      -13.271152354473637, -10.200992463005514, -11.172904321321422, -8.452631306227014,\n",
            "      -10.387820214495893, -14.092866281389728, -9.29984178372595, -10.216686893826026,\n",
            "      -9.931663013262247, -12.783466943021752, -13.781827702686751, -11.60078731948264,\n",
            "      -10.372853254784438, -6.955653676378858, -10.057021230324885, -9.956565312203937,\n",
            "      -10.605118753085424, -13.336397289312343, -12.701448365811682, -8.793272674064285,\n",
            "      -8.785132843290995, -7.8062557820477885, -10.059817316759577, -10.437985235611842,\n",
            "      -9.001489063683344, -11.86083444784972, -11.671557779247351, -7.637880278577952,\n",
            "      -12.318618853337258, -8.521966043037677, -10.301633436987107, -11.473101558126032,\n",
            "      -12.265445812458726, -7.986131024147747, -10.192114910569366, -13.384245530030281,\n",
            "      -10.133525859099928, -11.501064798051566, -9.679769061071617, -12.524546571843672,\n",
            "      -10.20470978286785, -14.410247032691405, -13.907524002295625, -10.942415536969854,\n",
            "      -7.494713511935938, -11.327591801455982, -12.335075489596791, -10.7559021248706,\n",
            "      -11.573982614621317, -7.768217495921097, -14.646346698979436, -11.565832496338132,\n",
            "      -12.599258405032321, -13.937999786108426, -8.695149631906453, -9.695342337818554,\n",
            "      -10.51615949535363, -14.74851655527017, -10.679266720574677, -12.074538371261678,\n",
            "      -11.081020212110241, -10.925130396531353, -14.04570765821148, -11.804405236005765,\n",
            "      -14.402270786962982, -8.607784233073227, -10.463485862717148, -7.855281077299433,\n",
            "      -8.49259483748229]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2558338838268708\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17859628849659934\n",
            "    mean_inference_ms: 1.2795720161189377\n",
            "    mean_raw_obs_processing_ms: 0.3667194000491645\n",
            "time_since_restore: 522.1106219291687\n",
            "time_this_iter_s: 1.1596174240112305\n",
            "time_total_s: 522.1106219291687\n",
            "timers:\n",
            "  learn_throughput: 1793.688\n",
            "  learn_time_ms: 17.84\n",
            "  load_throughput: 118829.33\n",
            "  load_time_ms: 0.269\n",
            "  sample_time_ms: 121.226\n",
            "  synch_weights_time_ms: 0.031\n",
            "  training_iteration_time_ms: 164.786\n",
            "timestamp: 1703177898\n",
            "timesteps_total: 183650\n",
            "training_iteration: 488\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 184100\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.0068378448486328125\n",
            "  StateBufferConnector_ms: 0.0049707889556884766\n",
            "  ViewRequirementAgentConnector_ms: 0.13248205184936523\n",
            "counters:\n",
            "  last_target_update_ts: 184100\n",
            "  num_agent_steps_sampled: 184100\n",
            "  num_agent_steps_trained: 116864\n",
            "  num_env_steps_sampled: 184100\n",
            "  num_env_steps_trained: 116864\n",
            "  num_target_updates: 3652\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-20\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.955653676378858\n",
            "episode_reward_mean: -11.40590532724902\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3682\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 184100\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1821.0\n",
            "      learner_stats:\n",
            "        actor_loss: -21.35673713684082\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.196630597114563\n",
            "        alpha_value: 0.6955670118331909\n",
            "        critic_loss: 0.06723687797784805\n",
            "        grad_gnorm: 3.2962489128112793\n",
            "        log_alpha_value: -0.363027960062027\n",
            "        max_q: 23.487985610961914\n",
            "        mean_q: 20.74020767211914\n",
            "        min_q: 17.06476402282715\n",
            "        policy_t: -0.1295289695262909\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.2918180227279663\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3652.0\n",
            "      td_error: [0.2622346878051758, 0.34726524353027344, 0.27281951904296875, 0.1297903060913086,\n",
            "        0.31503772735595703, 0.27712059020996094, 0.03672027587890625, 0.12759113311767578,\n",
            "        0.5075960159301758, 0.14345550537109375, 0.0225830078125, 0.3495931625366211,\n",
            "        0.4699087142944336, 0.13007259368896484, 0.27350616455078125, 0.3148527145385742,\n",
            "        0.3267679214477539, 0.07110309600830078, 0.6402759552001953, 0.2866230010986328,\n",
            "        0.12266159057617188, 0.17755508422851562, 0.0723724365234375, 0.34812259674072266,\n",
            "        0.6934108734130859, 0.2791023254394531, 0.5008411407470703, 0.6925439834594727,\n",
            "        0.09157276153564453, 0.7355403900146484, 0.24559307098388672, 0.0739431381225586]\n",
            "  num_agent_steps_sampled: 184100\n",
            "  num_agent_steps_trained: 116864\n",
            "  num_env_steps_sampled: 184100\n",
            "  num_env_steps_trained: 116864\n",
            "  num_target_updates: 3652\n",
            "iterations_since_restore: 489\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 184100\n",
            "num_agent_steps_trained: 116864\n",
            "num_env_steps_sampled: 184100\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 413.67607388425364\n",
            "num_env_steps_trained: 116864\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 264.7526872859223\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 62.4\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25580356289598344\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17858333334007223\n",
            "  mean_inference_ms: 1.2793125114609005\n",
            "  mean_raw_obs_processing_ms: 0.3666521330808372\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.0068378448486328125\n",
            "    StateBufferConnector_ms: 0.0049707889556884766\n",
            "    ViewRequirementAgentConnector_ms: 0.13248205184936523\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.955653676378858\n",
            "  episode_reward_mean: -11.40590532724902\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-11.96868688718963, -11.44340660485428, -12.448758889616082,\n",
            "      -12.990992413430979, -9.288925906749954, -12.958628622625204, -12.106768301855102,\n",
            "      -13.467092482188749, -9.248766661875116, -11.490535810078455, -13.725009139429709,\n",
            "      -13.4130069295191, -11.602683942103303, -14.50602504996919, -12.681462746674505,\n",
            "      -14.656985864996866, -12.660956971998862, -12.582657047159453, -16.363436598998874,\n",
            "      -13.530862506330191, -15.575662822289955, -14.478654851222322, -13.271152354473637,\n",
            "      -10.200992463005514, -11.172904321321422, -8.452631306227014, -10.387820214495893,\n",
            "      -14.092866281389728, -9.29984178372595, -10.216686893826026, -9.931663013262247,\n",
            "      -12.783466943021752, -13.781827702686751, -11.60078731948264, -10.372853254784438,\n",
            "      -6.955653676378858, -10.057021230324885, -9.956565312203937, -10.605118753085424,\n",
            "      -13.336397289312343, -12.701448365811682, -8.793272674064285, -8.785132843290995,\n",
            "      -7.8062557820477885, -10.059817316759577, -10.437985235611842, -9.001489063683344,\n",
            "      -11.86083444784972, -11.671557779247351, -7.637880278577952, -12.318618853337258,\n",
            "      -8.521966043037677, -10.301633436987107, -11.473101558126032, -12.265445812458726,\n",
            "      -7.986131024147747, -10.192114910569366, -13.384245530030281, -10.133525859099928,\n",
            "      -11.501064798051566, -9.679769061071617, -12.524546571843672, -10.20470978286785,\n",
            "      -14.410247032691405, -13.907524002295625, -10.942415536969854, -7.494713511935938,\n",
            "      -11.327591801455982, -12.335075489596791, -10.7559021248706, -11.573982614621317,\n",
            "      -7.768217495921097, -14.646346698979436, -11.565832496338132, -12.599258405032321,\n",
            "      -13.937999786108426, -8.695149631906453, -9.695342337818554, -10.51615949535363,\n",
            "      -14.74851655527017, -10.679266720574677, -12.074538371261678, -11.081020212110241,\n",
            "      -10.925130396531353, -14.04570765821148, -11.804405236005765, -14.402270786962982,\n",
            "      -8.607784233073227, -10.463485862717148, -7.855281077299433, -8.49259483748229,\n",
            "      -11.80410971446314, -8.712235144825994, -8.526305236561203, -12.764316900048387,\n",
            "      -12.832345079106574, -14.767830427903197, -11.002082099570954, -10.80039839503756,\n",
            "      -13.120385125251268]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25580356289598344\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17858333334007223\n",
            "    mean_inference_ms: 1.2793125114609005\n",
            "    mean_raw_obs_processing_ms: 0.3666521330808372\n",
            "time_since_restore: 523.1995067596436\n",
            "time_this_iter_s: 1.0888848304748535\n",
            "time_total_s: 523.1995067596436\n",
            "timers:\n",
            "  learn_throughput: 2352.184\n",
            "  learn_time_ms: 13.604\n",
            "  load_throughput: 140822.294\n",
            "  load_time_ms: 0.227\n",
            "  sample_time_ms: 93.132\n",
            "  synch_weights_time_ms: 0.024\n",
            "  training_iteration_time_ms: 125.142\n",
            "timestamp: 1703177900\n",
            "timesteps_total: 184100\n",
            "training_iteration: 489\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 184550\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006764650344848633\n",
            "  StateBufferConnector_ms: 0.004945516586303711\n",
            "  ViewRequirementAgentConnector_ms: 0.13259577751159668\n",
            "counters:\n",
            "  last_target_update_ts: 184550\n",
            "  num_agent_steps_sampled: 184550\n",
            "  num_agent_steps_trained: 117152\n",
            "  num_env_steps_sampled: 184550\n",
            "  num_env_steps_trained: 117152\n",
            "  num_target_updates: 3661\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-21\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.955653676378858\n",
            "episode_reward_mean: -11.47276899234643\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3691\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 184550\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1911.9375\n",
            "      learner_stats:\n",
            "        actor_loss: -20.266138076782227\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.2067259550094604\n",
            "        alpha_value: 0.6949536800384521\n",
            "        critic_loss: 0.04857050999999046\n",
            "        grad_gnorm: 3.3160009384155273\n",
            "        log_alpha_value: -0.363910049200058\n",
            "        max_q: 22.448068618774414\n",
            "        mean_q: 19.601884841918945\n",
            "        min_q: 12.63731575012207\n",
            "        policy_t: -0.06726476550102234\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.24535247683525085\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3661.0\n",
            "      td_error: [0.51544189453125, 0.24312400817871094, 0.3882589340209961, 0.06956005096435547,\n",
            "        0.619964599609375, 0.3311586380004883, 0.16276931762695312, 0.05153369903564453,\n",
            "        0.30471229553222656, 0.19830036163330078, 0.07824897766113281, 0.16231918334960938,\n",
            "        0.4920034408569336, 0.46422576904296875, 0.06058311462402344, 0.02620220184326172,\n",
            "        0.0841217041015625, 0.29332637786865234, 0.24942779541015625, 0.23559951782226562,\n",
            "        0.15058040618896484, 0.14768409729003906, 0.4427204132080078, 0.30918407440185547,\n",
            "        0.29580211639404297, 0.49733638763427734, 0.29749298095703125, 0.2849435806274414,\n",
            "        0.07036304473876953, 0.08030891418457031, 0.1633443832397461, 0.08063697814941406]\n",
            "  num_agent_steps_sampled: 184550\n",
            "  num_agent_steps_trained: 117152\n",
            "  num_env_steps_sampled: 184550\n",
            "  num_env_steps_trained: 117152\n",
            "  num_target_updates: 3661\n",
            "iterations_since_restore: 490\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 184550\n",
            "num_agent_steps_trained: 117152\n",
            "num_env_steps_sampled: 184550\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 426.43935727619146\n",
            "num_env_steps_trained: 117152\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 272.9211886567625\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 54.15\n",
            "  ram_util_percent: 21.4\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.255770805242561\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17856891433853128\n",
            "  mean_inference_ms: 1.2790351268693587\n",
            "  mean_raw_obs_processing_ms: 0.36658154269021226\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006764650344848633\n",
            "    StateBufferConnector_ms: 0.004945516586303711\n",
            "    ViewRequirementAgentConnector_ms: 0.13259577751159668\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.955653676378858\n",
            "  episode_reward_mean: -11.47276899234643\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-11.490535810078455, -13.725009139429709, -13.4130069295191,\n",
            "      -11.602683942103303, -14.50602504996919, -12.681462746674505, -14.656985864996866,\n",
            "      -12.660956971998862, -12.582657047159453, -16.363436598998874, -13.530862506330191,\n",
            "      -15.575662822289955, -14.478654851222322, -13.271152354473637, -10.200992463005514,\n",
            "      -11.172904321321422, -8.452631306227014, -10.387820214495893, -14.092866281389728,\n",
            "      -9.29984178372595, -10.216686893826026, -9.931663013262247, -12.783466943021752,\n",
            "      -13.781827702686751, -11.60078731948264, -10.372853254784438, -6.955653676378858,\n",
            "      -10.057021230324885, -9.956565312203937, -10.605118753085424, -13.336397289312343,\n",
            "      -12.701448365811682, -8.793272674064285, -8.785132843290995, -7.8062557820477885,\n",
            "      -10.059817316759577, -10.437985235611842, -9.001489063683344, -11.86083444784972,\n",
            "      -11.671557779247351, -7.637880278577952, -12.318618853337258, -8.521966043037677,\n",
            "      -10.301633436987107, -11.473101558126032, -12.265445812458726, -7.986131024147747,\n",
            "      -10.192114910569366, -13.384245530030281, -10.133525859099928, -11.501064798051566,\n",
            "      -9.679769061071617, -12.524546571843672, -10.20470978286785, -14.410247032691405,\n",
            "      -13.907524002295625, -10.942415536969854, -7.494713511935938, -11.327591801455982,\n",
            "      -12.335075489596791, -10.7559021248706, -11.573982614621317, -7.768217495921097,\n",
            "      -14.646346698979436, -11.565832496338132, -12.599258405032321, -13.937999786108426,\n",
            "      -8.695149631906453, -9.695342337818554, -10.51615949535363, -14.74851655527017,\n",
            "      -10.679266720574677, -12.074538371261678, -11.081020212110241, -10.925130396531353,\n",
            "      -14.04570765821148, -11.804405236005765, -14.402270786962982, -8.607784233073227,\n",
            "      -10.463485862717148, -7.855281077299433, -8.49259483748229, -11.80410971446314,\n",
            "      -8.712235144825994, -8.526305236561203, -12.764316900048387, -12.832345079106574,\n",
            "      -14.767830427903197, -11.002082099570954, -10.80039839503756, -13.120385125251268,\n",
            "      -14.926260563116768, -11.008792190589409, -12.214165256820399, -13.70192092472168,\n",
            "      -13.636909134964915, -12.215434856204535, -13.546043556184435, -11.31315177013072,\n",
            "      -10.045715027393136]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.255770805242561\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17856891433853128\n",
            "    mean_inference_ms: 1.2790351268693587\n",
            "    mean_raw_obs_processing_ms: 0.36658154269021226\n",
            "time_since_restore: 524.2559440135956\n",
            "time_this_iter_s: 1.0564372539520264\n",
            "time_total_s: 524.2559440135956\n",
            "timers:\n",
            "  learn_throughput: 2422.187\n",
            "  learn_time_ms: 13.211\n",
            "  load_throughput: 197031.309\n",
            "  load_time_ms: 0.162\n",
            "  sample_time_ms: 88.304\n",
            "  synch_weights_time_ms: 0.023\n",
            "  training_iteration_time_ms: 116.639\n",
            "timestamp: 1703177901\n",
            "timesteps_total: 184550\n",
            "training_iteration: 490\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 185000\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.00671696662902832\n",
            "  StateBufferConnector_ms: 0.004925251007080078\n",
            "  ViewRequirementAgentConnector_ms: 0.1320488452911377\n",
            "counters:\n",
            "  last_target_update_ts: 185000\n",
            "  num_agent_steps_sampled: 185000\n",
            "  num_agent_steps_trained: 117440\n",
            "  num_env_steps_sampled: 185000\n",
            "  num_env_steps_trained: 117440\n",
            "  num_target_updates: 3670\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-22\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.955653676378858\n",
            "episode_reward_mean: -11.461019962955717\n",
            "episode_reward_min: -16.363436598998874\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3700\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 185000\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1673.1875\n",
            "      learner_stats:\n",
            "        actor_loss: -20.24966812133789\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1599221229553223\n",
            "        alpha_value: 0.6943355798721313\n",
            "        critic_loss: 0.023132437840104103\n",
            "        grad_gnorm: 3.179612636566162\n",
            "        log_alpha_value: -0.3647998571395874\n",
            "        max_q: 23.072265625\n",
            "        mean_q: 19.64983558654785\n",
            "        min_q: 14.280198097229004\n",
            "        policy_t: -0.0731707438826561\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.1720917969942093\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3670.0\n",
            "      td_error: [0.213531494140625, 0.22350215911865234, 0.09812545776367188, 0.03602027893066406,\n",
            "        0.17476654052734375, 0.21213102340698242, 0.025350570678710938, 0.07380390167236328,\n",
            "        0.15450477600097656, 0.3780345916748047, 0.1485147476196289, 0.09700870513916016,\n",
            "        0.0855865478515625, 0.06779766082763672, 0.20219039916992188, 0.06396675109863281,\n",
            "        0.027357101440429688, 0.38760948181152344, 0.20259380340576172, 0.07826900482177734,\n",
            "        0.24909114837646484, 0.19867515563964844, 0.2199869155883789, 0.048874855041503906,\n",
            "        0.3246908187866211, 0.4572725296020508, 0.239410400390625, 0.049304962158203125,\n",
            "        0.043189048767089844, 0.05738353729248047, 0.43561649322509766, 0.23277664184570312]\n",
            "  num_agent_steps_sampled: 185000\n",
            "  num_agent_steps_trained: 117440\n",
            "  num_env_steps_sampled: 185000\n",
            "  num_env_steps_trained: 117440\n",
            "  num_target_updates: 3670\n",
            "iterations_since_restore: 491\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 185000\n",
            "num_agent_steps_trained: 117440\n",
            "num_env_steps_sampled: 185000\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 424.2225117645618\n",
            "num_env_steps_trained: 117440\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 271.50240752931956\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 57.4\n",
            "  ram_util_percent: 21.5\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25573750896194236\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17855471577852725\n",
            "  mean_inference_ms: 1.2787537564272953\n",
            "  mean_raw_obs_processing_ms: 0.3665080604737966\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.00671696662902832\n",
            "    StateBufferConnector_ms: 0.004925251007080078\n",
            "    ViewRequirementAgentConnector_ms: 0.1320488452911377\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.955653676378858\n",
            "  episode_reward_mean: -11.461019962955717\n",
            "  episode_reward_min: -16.363436598998874\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-16.363436598998874, -13.530862506330191, -15.575662822289955,\n",
            "      -14.478654851222322, -13.271152354473637, -10.200992463005514, -11.172904321321422,\n",
            "      -8.452631306227014, -10.387820214495893, -14.092866281389728, -9.29984178372595,\n",
            "      -10.216686893826026, -9.931663013262247, -12.783466943021752, -13.781827702686751,\n",
            "      -11.60078731948264, -10.372853254784438, -6.955653676378858, -10.057021230324885,\n",
            "      -9.956565312203937, -10.605118753085424, -13.336397289312343, -12.701448365811682,\n",
            "      -8.793272674064285, -8.785132843290995, -7.8062557820477885, -10.059817316759577,\n",
            "      -10.437985235611842, -9.001489063683344, -11.86083444784972, -11.671557779247351,\n",
            "      -7.637880278577952, -12.318618853337258, -8.521966043037677, -10.301633436987107,\n",
            "      -11.473101558126032, -12.265445812458726, -7.986131024147747, -10.192114910569366,\n",
            "      -13.384245530030281, -10.133525859099928, -11.501064798051566, -9.679769061071617,\n",
            "      -12.524546571843672, -10.20470978286785, -14.410247032691405, -13.907524002295625,\n",
            "      -10.942415536969854, -7.494713511935938, -11.327591801455982, -12.335075489596791,\n",
            "      -10.7559021248706, -11.573982614621317, -7.768217495921097, -14.646346698979436,\n",
            "      -11.565832496338132, -12.599258405032321, -13.937999786108426, -8.695149631906453,\n",
            "      -9.695342337818554, -10.51615949535363, -14.74851655527017, -10.679266720574677,\n",
            "      -12.074538371261678, -11.081020212110241, -10.925130396531353, -14.04570765821148,\n",
            "      -11.804405236005765, -14.402270786962982, -8.607784233073227, -10.463485862717148,\n",
            "      -7.855281077299433, -8.49259483748229, -11.80410971446314, -8.712235144825994,\n",
            "      -8.526305236561203, -12.764316900048387, -12.832345079106574, -14.767830427903197,\n",
            "      -11.002082099570954, -10.80039839503756, -13.120385125251268, -14.926260563116768,\n",
            "      -11.008792190589409, -12.214165256820399, -13.70192092472168, -13.636909134964915,\n",
            "      -12.215434856204535, -13.546043556184435, -11.31315177013072, -10.045715027393136,\n",
            "      -13.743272042495654, -15.198364958709481, -10.670986585640316, -15.594481517749733,\n",
            "      -11.589679381334209, -10.142360894904348, -12.29459332434907, -16.297531580042055,\n",
            "      -10.613150277633306]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25573750896194236\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17855471577852725\n",
            "    mean_inference_ms: 1.2787537564272953\n",
            "    mean_raw_obs_processing_ms: 0.3665080604737966\n",
            "time_since_restore: 525.3178396224976\n",
            "time_this_iter_s: 1.0618956089019775\n",
            "time_total_s: 525.3178396224976\n",
            "timers:\n",
            "  learn_throughput: 2298.017\n",
            "  learn_time_ms: 13.925\n",
            "  load_throughput: 159365.623\n",
            "  load_time_ms: 0.201\n",
            "  sample_time_ms: 87.437\n",
            "  synch_weights_time_ms: 0.024\n",
            "  training_iteration_time_ms: 117.479\n",
            "timestamp: 1703177902\n",
            "timesteps_total: 185000\n",
            "training_iteration: 491\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 185450\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006693124771118164\n",
            "  StateBufferConnector_ms: 0.004909992218017578\n",
            "  ViewRequirementAgentConnector_ms: 0.13144516944885254\n",
            "counters:\n",
            "  last_target_update_ts: 185450\n",
            "  num_agent_steps_sampled: 185450\n",
            "  num_agent_steps_trained: 117728\n",
            "  num_env_steps_sampled: 185450\n",
            "  num_env_steps_trained: 117728\n",
            "  num_target_updates: 3679\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-23\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -6.955653676378858\n",
            "episode_reward_mean: -11.966361348976086\n",
            "episode_reward_min: -22.841573286392208\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3709\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 185450\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1551.375\n",
            "      learner_stats:\n",
            "        actor_loss: -21.460765838623047\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.0862222909927368\n",
            "        alpha_value: 0.6937189698219299\n",
            "        critic_loss: 0.12030372023582458\n",
            "        grad_gnorm: 2.9703500270843506\n",
            "        log_alpha_value: -0.36568835377693176\n",
            "        max_q: 23.32095718383789\n",
            "        mean_q: 20.79779815673828\n",
            "        min_q: 17.351688385009766\n",
            "        policy_t: -0.2976677715778351\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.37700343132019043\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3679.0\n",
            "      td_error: [0.5021648406982422, 0.042942047119140625, 0.013894081115722656, 0.4214954376220703,\n",
            "        0.08122062683105469, 0.28870487213134766, 0.2771492004394531, 0.36888980865478516,\n",
            "        1.32769775390625, 0.6857633590698242, 0.7783174514770508, 0.4976987838745117,\n",
            "        0.9649715423583984, 0.6562089920043945, 0.13531494140625, 0.6752796173095703,\n",
            "        0.34652137756347656, 0.1673736572265625, 0.07477474212646484, 0.19135189056396484,\n",
            "        0.5769386291503906, 0.3184795379638672, 0.14360618591308594, 0.6403999328613281,\n",
            "        0.14104080200195312, 0.445953369140625, 0.0657949447631836, 0.562535285949707,\n",
            "        0.2219409942626953, 0.29563331604003906, 0.09624385833740234, 0.05780792236328125]\n",
            "  num_agent_steps_sampled: 185450\n",
            "  num_agent_steps_trained: 117728\n",
            "  num_env_steps_sampled: 185450\n",
            "  num_env_steps_trained: 117728\n",
            "  num_target_updates: 3679\n",
            "iterations_since_restore: 492\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 185450\n",
            "num_agent_steps_trained: 117728\n",
            "num_env_steps_sampled: 185450\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 414.27370504129266\n",
            "num_env_steps_trained: 117728\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 265.1351712264273\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 54.150000000000006\n",
            "  ram_util_percent: 21.5\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2557051694137632\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17854087699372456\n",
            "  mean_inference_ms: 1.2784778877326528\n",
            "  mean_raw_obs_processing_ms: 0.36643826955357\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006693124771118164\n",
            "    StateBufferConnector_ms: 0.004909992218017578\n",
            "    ViewRequirementAgentConnector_ms: 0.13144516944885254\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -6.955653676378858\n",
            "  episode_reward_mean: -11.966361348976086\n",
            "  episode_reward_min: -22.841573286392208\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-14.092866281389728, -9.29984178372595, -10.216686893826026,\n",
            "      -9.931663013262247, -12.783466943021752, -13.781827702686751, -11.60078731948264,\n",
            "      -10.372853254784438, -6.955653676378858, -10.057021230324885, -9.956565312203937,\n",
            "      -10.605118753085424, -13.336397289312343, -12.701448365811682, -8.793272674064285,\n",
            "      -8.785132843290995, -7.8062557820477885, -10.059817316759577, -10.437985235611842,\n",
            "      -9.001489063683344, -11.86083444784972, -11.671557779247351, -7.637880278577952,\n",
            "      -12.318618853337258, -8.521966043037677, -10.301633436987107, -11.473101558126032,\n",
            "      -12.265445812458726, -7.986131024147747, -10.192114910569366, -13.384245530030281,\n",
            "      -10.133525859099928, -11.501064798051566, -9.679769061071617, -12.524546571843672,\n",
            "      -10.20470978286785, -14.410247032691405, -13.907524002295625, -10.942415536969854,\n",
            "      -7.494713511935938, -11.327591801455982, -12.335075489596791, -10.7559021248706,\n",
            "      -11.573982614621317, -7.768217495921097, -14.646346698979436, -11.565832496338132,\n",
            "      -12.599258405032321, -13.937999786108426, -8.695149631906453, -9.695342337818554,\n",
            "      -10.51615949535363, -14.74851655527017, -10.679266720574677, -12.074538371261678,\n",
            "      -11.081020212110241, -10.925130396531353, -14.04570765821148, -11.804405236005765,\n",
            "      -14.402270786962982, -8.607784233073227, -10.463485862717148, -7.855281077299433,\n",
            "      -8.49259483748229, -11.80410971446314, -8.712235144825994, -8.526305236561203,\n",
            "      -12.764316900048387, -12.832345079106574, -14.767830427903197, -11.002082099570954,\n",
            "      -10.80039839503756, -13.120385125251268, -14.926260563116768, -11.008792190589409,\n",
            "      -12.214165256820399, -13.70192092472168, -13.636909134964915, -12.215434856204535,\n",
            "      -13.546043556184435, -11.31315177013072, -10.045715027393136, -13.743272042495654,\n",
            "      -15.198364958709481, -10.670986585640316, -15.594481517749733, -11.589679381334209,\n",
            "      -10.142360894904348, -12.29459332434907, -16.297531580042055, -10.613150277633306,\n",
            "      -14.546847819571406, -14.928020843080681, -14.87214135978341, -18.451742149872533,\n",
            "      -18.323653736774904, -20.26580241822034, -20.34953820313229, -19.388936223573953,\n",
            "      -22.841573286392208]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2557051694137632\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17854087699372456\n",
            "    mean_inference_ms: 1.2784778877326528\n",
            "    mean_raw_obs_processing_ms: 0.36643826955357\n",
            "time_since_restore: 526.4053263664246\n",
            "time_this_iter_s: 1.087486743927002\n",
            "time_total_s: 526.4053263664246\n",
            "timers:\n",
            "  learn_throughput: 2377.862\n",
            "  learn_time_ms: 13.457\n",
            "  load_throughput: 175562.757\n",
            "  load_time_ms: 0.182\n",
            "  sample_time_ms: 90.806\n",
            "  synch_weights_time_ms: 0.025\n",
            "  training_iteration_time_ms: 120.35\n",
            "timestamp: 1703177903\n",
            "timesteps_total: 185450\n",
            "training_iteration: 492\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 185900\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006724119186401367\n",
            "  StateBufferConnector_ms: 0.004917621612548828\n",
            "  ViewRequirementAgentConnector_ms: 0.1311779022216797\n",
            "counters:\n",
            "  last_target_update_ts: 185900\n",
            "  num_agent_steps_sampled: 185900\n",
            "  num_agent_steps_trained: 118016\n",
            "  num_env_steps_sampled: 185900\n",
            "  num_env_steps_trained: 118016\n",
            "  num_target_updates: 3688\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-24\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -7.494713511935938\n",
            "episode_reward_mean: -12.547078699810163\n",
            "episode_reward_min: -25.131358270535983\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3718\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 185900\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1962.34375\n",
            "      learner_stats:\n",
            "        actor_loss: -20.56254768371582\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.1447665691375732\n",
            "        alpha_value: 0.6931135058403015\n",
            "        critic_loss: 0.06930079311132431\n",
            "        grad_gnorm: 3.122986316680908\n",
            "        log_alpha_value: -0.36656150221824646\n",
            "        max_q: 22.643840789794922\n",
            "        mean_q: 19.828052520751953\n",
            "        min_q: 11.152036666870117\n",
            "        policy_t: -0.23910921812057495\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.29570794105529785\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3688.0\n",
            "      td_error: [0.10239028930664062, 0.3126802444458008, 0.3415231704711914, 0.5529489517211914,\n",
            "        0.4367647171020508, 0.2646799087524414, 0.2355365753173828, 0.7937469482421875,\n",
            "        0.04154205322265625, 0.14513206481933594, 0.4676084518432617, 0.24744415283203125,\n",
            "        0.1562042236328125, 0.27794551849365234, 0.4815244674682617, 0.01657390594482422,\n",
            "        0.2604103088378906, 0.29126834869384766, 0.48136138916015625, 0.13150405883789062,\n",
            "        0.036853790283203125, 0.4897794723510742, 0.0073299407958984375, 0.12919998168945312,\n",
            "        0.6543331146240234, 0.4034757614135742, 0.2621145248413086, 0.753692626953125,\n",
            "        0.04542732238769531, 0.3586874008178711, 0.07422733306884766, 0.20874309539794922]\n",
            "  num_agent_steps_sampled: 185900\n",
            "  num_agent_steps_trained: 118016\n",
            "  num_env_steps_sampled: 185900\n",
            "  num_env_steps_trained: 118016\n",
            "  num_target_updates: 3688\n",
            "iterations_since_restore: 493\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 185900\n",
            "num_agent_steps_trained: 118016\n",
            "num_env_steps_sampled: 185900\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 428.7710340531703\n",
            "num_env_steps_trained: 118016\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 274.413461794029\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 55.0\n",
            "  ram_util_percent: 21.5\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.255672656083938\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17852645466201358\n",
            "  mean_inference_ms: 1.27820539595947\n",
            "  mean_raw_obs_processing_ms: 0.3663688121004616\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006724119186401367\n",
            "    StateBufferConnector_ms: 0.004917621612548828\n",
            "    ViewRequirementAgentConnector_ms: 0.1311779022216797\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -7.494713511935938\n",
            "  episode_reward_mean: -12.547078699810163\n",
            "  episode_reward_min: -25.131358270535983\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-10.057021230324885, -9.956565312203937, -10.605118753085424,\n",
            "      -13.336397289312343, -12.701448365811682, -8.793272674064285, -8.785132843290995,\n",
            "      -7.8062557820477885, -10.059817316759577, -10.437985235611842, -9.001489063683344,\n",
            "      -11.86083444784972, -11.671557779247351, -7.637880278577952, -12.318618853337258,\n",
            "      -8.521966043037677, -10.301633436987107, -11.473101558126032, -12.265445812458726,\n",
            "      -7.986131024147747, -10.192114910569366, -13.384245530030281, -10.133525859099928,\n",
            "      -11.501064798051566, -9.679769061071617, -12.524546571843672, -10.20470978286785,\n",
            "      -14.410247032691405, -13.907524002295625, -10.942415536969854, -7.494713511935938,\n",
            "      -11.327591801455982, -12.335075489596791, -10.7559021248706, -11.573982614621317,\n",
            "      -7.768217495921097, -14.646346698979436, -11.565832496338132, -12.599258405032321,\n",
            "      -13.937999786108426, -8.695149631906453, -9.695342337818554, -10.51615949535363,\n",
            "      -14.74851655527017, -10.679266720574677, -12.074538371261678, -11.081020212110241,\n",
            "      -10.925130396531353, -14.04570765821148, -11.804405236005765, -14.402270786962982,\n",
            "      -8.607784233073227, -10.463485862717148, -7.855281077299433, -8.49259483748229,\n",
            "      -11.80410971446314, -8.712235144825994, -8.526305236561203, -12.764316900048387,\n",
            "      -12.832345079106574, -14.767830427903197, -11.002082099570954, -10.80039839503756,\n",
            "      -13.120385125251268, -14.926260563116768, -11.008792190589409, -12.214165256820399,\n",
            "      -13.70192092472168, -13.636909134964915, -12.215434856204535, -13.546043556184435,\n",
            "      -11.31315177013072, -10.045715027393136, -13.743272042495654, -15.198364958709481,\n",
            "      -10.670986585640316, -15.594481517749733, -11.589679381334209, -10.142360894904348,\n",
            "      -12.29459332434907, -16.297531580042055, -10.613150277633306, -14.546847819571406,\n",
            "      -14.928020843080681, -14.87214135978341, -18.451742149872533, -18.323653736774904,\n",
            "      -20.26580241822034, -20.34953820313229, -19.388936223573953, -22.841573286392208,\n",
            "      -18.645616195213478, -25.131358270535983, -15.712139013860149, -17.42463623780536,\n",
            "      -16.676693780523774, -19.254531406474197, -16.085869703164743, -16.13864047235271,\n",
            "      -12.03789687203569]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.255672656083938\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17852645466201358\n",
            "    mean_inference_ms: 1.27820539595947\n",
            "    mean_raw_obs_processing_ms: 0.3663688121004616\n",
            "time_since_restore: 527.455952167511\n",
            "time_this_iter_s: 1.0506258010864258\n",
            "time_total_s: 527.455952167511\n",
            "timers:\n",
            "  learn_throughput: 2463.38\n",
            "  learn_time_ms: 12.99\n",
            "  load_throughput: 192924.72\n",
            "  load_time_ms: 0.166\n",
            "  sample_time_ms: 88.937\n",
            "  synch_weights_time_ms: 0.028\n",
            "  training_iteration_time_ms: 117.339\n",
            "timestamp: 1703177904\n",
            "timesteps_total: 185900\n",
            "training_iteration: 493\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 186350\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006686210632324219\n",
            "  StateBufferConnector_ms: 0.004907131195068359\n",
            "  ViewRequirementAgentConnector_ms: 0.13066530227661133\n",
            "counters:\n",
            "  last_target_update_ts: 186350\n",
            "  num_agent_steps_sampled: 186350\n",
            "  num_agent_steps_trained: 118304\n",
            "  num_env_steps_sampled: 186350\n",
            "  num_env_steps_trained: 118304\n",
            "  num_target_updates: 3697\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-25\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -7.494713511935938\n",
            "episode_reward_mean: -13.028833041496108\n",
            "episode_reward_min: -25.131358270535983\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3727\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 186350\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1954.6875\n",
            "      learner_stats:\n",
            "        actor_loss: -21.474971771240234\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.2384986877441406\n",
            "        alpha_value: 0.6925064325332642\n",
            "        critic_loss: 0.04484330862760544\n",
            "        grad_gnorm: 3.3706352710723877\n",
            "        log_alpha_value: -0.3674377202987671\n",
            "        max_q: 24.613927841186523\n",
            "        mean_q: 20.725181579589844\n",
            "        min_q: 16.554344177246094\n",
            "        policy_t: -0.05395924299955368\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.23829692602157593\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3697.0\n",
            "      td_error: [0.07327842712402344, 0.35006237030029297, 0.10367393493652344, 0.0785531997680664,\n",
            "        0.11787223815917969, 0.3239011764526367, 0.2941627502441406, 0.1840534210205078,\n",
            "        0.09950065612792969, 0.3155794143676758, 0.07906150817871094, 0.3350048065185547,\n",
            "        0.4210996627807617, 0.37398242950439453, 0.5818767547607422, 0.14015960693359375,\n",
            "        0.12210655212402344, 0.5819110870361328, 0.11674880981445312, 0.27524852752685547,\n",
            "        0.2129373550415039, 0.042380332946777344, 0.2283773422241211, 0.2311553955078125,\n",
            "        0.03907585144042969, 0.6346311569213867, 0.030094146728515625, 0.2019491195678711,\n",
            "        0.5000801086425781, 0.2785186767578125, 0.041230201721191406, 0.21723461151123047]\n",
            "  num_agent_steps_sampled: 186350\n",
            "  num_agent_steps_trained: 118304\n",
            "  num_env_steps_sampled: 186350\n",
            "  num_env_steps_trained: 118304\n",
            "  num_target_updates: 3697\n",
            "iterations_since_restore: 494\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 186350\n",
            "num_agent_steps_trained: 118304\n",
            "num_env_steps_sampled: 186350\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 426.9239590736424\n",
            "num_env_steps_trained: 118304\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 273.23133380713114\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 55.0\n",
            "  ram_util_percent: 21.5\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2556395404600711\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17851057004870427\n",
            "  mean_inference_ms: 1.2779313412067985\n",
            "  mean_raw_obs_processing_ms: 0.3662980852572447\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006686210632324219\n",
            "    StateBufferConnector_ms: 0.004907131195068359\n",
            "    ViewRequirementAgentConnector_ms: 0.13066530227661133\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -7.494713511935938\n",
            "  episode_reward_mean: -13.028833041496108\n",
            "  episode_reward_min: -25.131358270535983\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-10.437985235611842, -9.001489063683344, -11.86083444784972,\n",
            "      -11.671557779247351, -7.637880278577952, -12.318618853337258, -8.521966043037677,\n",
            "      -10.301633436987107, -11.473101558126032, -12.265445812458726, -7.986131024147747,\n",
            "      -10.192114910569366, -13.384245530030281, -10.133525859099928, -11.501064798051566,\n",
            "      -9.679769061071617, -12.524546571843672, -10.20470978286785, -14.410247032691405,\n",
            "      -13.907524002295625, -10.942415536969854, -7.494713511935938, -11.327591801455982,\n",
            "      -12.335075489596791, -10.7559021248706, -11.573982614621317, -7.768217495921097,\n",
            "      -14.646346698979436, -11.565832496338132, -12.599258405032321, -13.937999786108426,\n",
            "      -8.695149631906453, -9.695342337818554, -10.51615949535363, -14.74851655527017,\n",
            "      -10.679266720574677, -12.074538371261678, -11.081020212110241, -10.925130396531353,\n",
            "      -14.04570765821148, -11.804405236005765, -14.402270786962982, -8.607784233073227,\n",
            "      -10.463485862717148, -7.855281077299433, -8.49259483748229, -11.80410971446314,\n",
            "      -8.712235144825994, -8.526305236561203, -12.764316900048387, -12.832345079106574,\n",
            "      -14.767830427903197, -11.002082099570954, -10.80039839503756, -13.120385125251268,\n",
            "      -14.926260563116768, -11.008792190589409, -12.214165256820399, -13.70192092472168,\n",
            "      -13.636909134964915, -12.215434856204535, -13.546043556184435, -11.31315177013072,\n",
            "      -10.045715027393136, -13.743272042495654, -15.198364958709481, -10.670986585640316,\n",
            "      -15.594481517749733, -11.589679381334209, -10.142360894904348, -12.29459332434907,\n",
            "      -16.297531580042055, -10.613150277633306, -14.546847819571406, -14.928020843080681,\n",
            "      -14.87214135978341, -18.451742149872533, -18.323653736774904, -20.26580241822034,\n",
            "      -20.34953820313229, -19.388936223573953, -22.841573286392208, -18.645616195213478,\n",
            "      -25.131358270535983, -15.712139013860149, -17.42463623780536, -16.676693780523774,\n",
            "      -19.254531406474197, -16.085869703164743, -16.13864047235271, -12.03789687203569,\n",
            "      -16.099829458156083, -12.95468270484183, -16.07759398130344, -14.94672569999649,\n",
            "      -12.096699034764695, -14.941611166665089, -18.381354187014598, -16.671833583594672,\n",
            "      -18.10613391915888]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2556395404600711\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17851057004870427\n",
            "    mean_inference_ms: 1.2779313412067985\n",
            "    mean_raw_obs_processing_ms: 0.3662980852572447\n",
            "time_since_restore: 528.5111589431763\n",
            "time_this_iter_s: 1.0552067756652832\n",
            "time_total_s: 528.5111589431763\n",
            "timers:\n",
            "  learn_throughput: 2409.351\n",
            "  learn_time_ms: 13.282\n",
            "  load_throughput: 181326.301\n",
            "  load_time_ms: 0.176\n",
            "  sample_time_ms: 87.797\n",
            "  synch_weights_time_ms: 0.024\n",
            "  training_iteration_time_ms: 116.718\n",
            "timestamp: 1703177905\n",
            "timesteps_total: 186350\n",
            "training_iteration: 494\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 186800\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006673574447631836\n",
            "  StateBufferConnector_ms: 0.004990100860595703\n",
            "  ViewRequirementAgentConnector_ms: 0.13085651397705078\n",
            "counters:\n",
            "  last_target_update_ts: 186800\n",
            "  num_agent_steps_sampled: 186800\n",
            "  num_agent_steps_trained: 118592\n",
            "  num_env_steps_sampled: 186800\n",
            "  num_env_steps_trained: 118592\n",
            "  num_target_updates: 3706\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-26\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -7.494713511935938\n",
            "episode_reward_mean: -13.6712544724739\n",
            "episode_reward_min: -25.131358270535983\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3736\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 186800\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1977.96875\n",
            "      learner_stats:\n",
            "        actor_loss: -21.067951202392578\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.2053475379943848\n",
            "        alpha_value: 0.691897988319397\n",
            "        critic_loss: 0.05453431233763695\n",
            "        grad_gnorm: 3.2725837230682373\n",
            "        log_alpha_value: -0.36831676959991455\n",
            "        max_q: 23.791065216064453\n",
            "        mean_q: 20.442781448364258\n",
            "        min_q: 17.555004119873047\n",
            "        policy_t: -0.08210981637239456\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.25463569164276123\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3706.0\n",
            "      td_error: [0.4824409484863281, 0.6832447052001953, 0.11108589172363281, 0.16363143920898438,\n",
            "        0.1275644302368164, 0.09214401245117188, 0.10867500305175781, 0.4807701110839844,\n",
            "        0.34043407440185547, 0.4983339309692383, 0.11142921447753906, 0.5014276504516602,\n",
            "        0.4429445266723633, 0.07709980010986328, 0.050823211669921875, 0.18051624298095703,\n",
            "        0.06488800048828125, 0.2886180877685547, 0.07087516784667969, 0.6502323150634766,\n",
            "        0.25423240661621094, 0.13149547576904297, 0.24239540100097656, 0.10534191131591797,\n",
            "        0.366546630859375, 0.11102771759033203, 0.06655311584472656, 0.439208984375,\n",
            "        0.18024730682373047, 0.4422731399536133, 0.010456085205078125, 0.27138519287109375]\n",
            "  num_agent_steps_sampled: 186800\n",
            "  num_agent_steps_trained: 118592\n",
            "  num_env_steps_sampled: 186800\n",
            "  num_env_steps_trained: 118592\n",
            "  num_target_updates: 3706\n",
            "iterations_since_restore: 495\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 186800\n",
            "num_agent_steps_trained: 118592\n",
            "num_env_steps_sampled: 186800\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 414.0852949932724\n",
            "num_env_steps_trained: 118592\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 265.01458879569435\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 54.6\n",
            "  ram_util_percent: 21.5\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2556081596582614\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.1784936999734027\n",
            "  mean_inference_ms: 1.2776651341222292\n",
            "  mean_raw_obs_processing_ms: 0.366231064414969\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006673574447631836\n",
            "    StateBufferConnector_ms: 0.004990100860595703\n",
            "    ViewRequirementAgentConnector_ms: 0.13085651397705078\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -7.494713511935938\n",
            "  episode_reward_mean: -13.6712544724739\n",
            "  episode_reward_min: -25.131358270535983\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-12.265445812458726, -7.986131024147747, -10.192114910569366,\n",
            "      -13.384245530030281, -10.133525859099928, -11.501064798051566, -9.679769061071617,\n",
            "      -12.524546571843672, -10.20470978286785, -14.410247032691405, -13.907524002295625,\n",
            "      -10.942415536969854, -7.494713511935938, -11.327591801455982, -12.335075489596791,\n",
            "      -10.7559021248706, -11.573982614621317, -7.768217495921097, -14.646346698979436,\n",
            "      -11.565832496338132, -12.599258405032321, -13.937999786108426, -8.695149631906453,\n",
            "      -9.695342337818554, -10.51615949535363, -14.74851655527017, -10.679266720574677,\n",
            "      -12.074538371261678, -11.081020212110241, -10.925130396531353, -14.04570765821148,\n",
            "      -11.804405236005765, -14.402270786962982, -8.607784233073227, -10.463485862717148,\n",
            "      -7.855281077299433, -8.49259483748229, -11.80410971446314, -8.712235144825994,\n",
            "      -8.526305236561203, -12.764316900048387, -12.832345079106574, -14.767830427903197,\n",
            "      -11.002082099570954, -10.80039839503756, -13.120385125251268, -14.926260563116768,\n",
            "      -11.008792190589409, -12.214165256820399, -13.70192092472168, -13.636909134964915,\n",
            "      -12.215434856204535, -13.546043556184435, -11.31315177013072, -10.045715027393136,\n",
            "      -13.743272042495654, -15.198364958709481, -10.670986585640316, -15.594481517749733,\n",
            "      -11.589679381334209, -10.142360894904348, -12.29459332434907, -16.297531580042055,\n",
            "      -10.613150277633306, -14.546847819571406, -14.928020843080681, -14.87214135978341,\n",
            "      -18.451742149872533, -18.323653736774904, -20.26580241822034, -20.34953820313229,\n",
            "      -19.388936223573953, -22.841573286392208, -18.645616195213478, -25.131358270535983,\n",
            "      -15.712139013860149, -17.42463623780536, -16.676693780523774, -19.254531406474197,\n",
            "      -16.085869703164743, -16.13864047235271, -12.03789687203569, -16.099829458156083,\n",
            "      -12.95468270484183, -16.07759398130344, -14.94672569999649, -12.096699034764695,\n",
            "      -14.941611166665089, -18.381354187014598, -16.671833583594672, -18.10613391915888,\n",
            "      -17.214462818962808, -13.79750671099854, -18.468422118012402, -18.109416748855743,\n",
            "      -19.001127731959564, -19.896316196430107, -19.91085709342321, -18.31089548890767,\n",
            "      -12.758204886687192]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2556081596582614\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1784936999734027\n",
            "    mean_inference_ms: 1.2776651341222292\n",
            "    mean_raw_obs_processing_ms: 0.366231064414969\n",
            "time_since_restore: 529.5990490913391\n",
            "time_this_iter_s: 1.0878901481628418\n",
            "time_total_s: 529.5990490913391\n",
            "timers:\n",
            "  learn_throughput: 2309.102\n",
            "  learn_time_ms: 13.858\n",
            "  load_throughput: 179459.457\n",
            "  load_time_ms: 0.178\n",
            "  sample_time_ms: 89.412\n",
            "  synch_weights_time_ms: 0.025\n",
            "  training_iteration_time_ms: 119.709\n",
            "timestamp: 1703177906\n",
            "timesteps_total: 186800\n",
            "training_iteration: 495\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 187250\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006674528121948242\n",
            "  StateBufferConnector_ms: 0.004946231842041016\n",
            "  ViewRequirementAgentConnector_ms: 0.13057684898376465\n",
            "counters:\n",
            "  last_target_update_ts: 187250\n",
            "  num_agent_steps_sampled: 187250\n",
            "  num_agent_steps_trained: 118880\n",
            "  num_env_steps_sampled: 187250\n",
            "  num_env_steps_trained: 118880\n",
            "  num_target_updates: 3715\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-27\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -7.494713511935938\n",
            "episode_reward_mean: -13.828806497686394\n",
            "episode_reward_min: -25.131358270535983\n",
            "episodes_this_iter: 9\n",
            "episodes_total: 3745\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 187250\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1488.15625\n",
            "      learner_stats:\n",
            "        actor_loss: -20.80197525024414\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.2535600662231445\n",
            "        alpha_value: 0.691288411617279\n",
            "        critic_loss: 0.04268760606646538\n",
            "        grad_gnorm: 3.3953585624694824\n",
            "        log_alpha_value: -0.36919814348220825\n",
            "        max_q: 22.53289031982422\n",
            "        mean_q: 20.05047607421875\n",
            "        min_q: 13.89632511138916\n",
            "        policy_t: -0.006597957573831081\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.2413272112607956\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3715.0\n",
            "      td_error: [0.16394424438476562, 0.18638992309570312, 0.4500761032104492, 0.3557004928588867,\n",
            "        0.1585559844970703, 0.4919576644897461, 0.10310173034667969, 0.6194372177124023,\n",
            "        0.3540821075439453, 0.058167457580566406, 0.1070098876953125, 0.17523670196533203,\n",
            "        0.1769275665283203, 0.10927295684814453, 0.1789112091064453, 0.1491079330444336,\n",
            "        0.5159692764282227, 0.1952958106994629, 0.2097339630126953, 0.09419631958007812,\n",
            "        0.16039276123046875, 0.051207542419433594, 0.3932523727416992, 0.021799087524414062,\n",
            "        0.21845245361328125, 0.13710308074951172, 0.2303905487060547, 0.22937583923339844,\n",
            "        0.3797168731689453, 0.5222311019897461, 0.3337287902832031, 0.19174575805664062]\n",
            "  num_agent_steps_sampled: 187250\n",
            "  num_agent_steps_trained: 118880\n",
            "  num_env_steps_sampled: 187250\n",
            "  num_env_steps_trained: 118880\n",
            "  num_target_updates: 3715\n",
            "iterations_since_restore: 496\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 187250\n",
            "num_agent_steps_trained: 118880\n",
            "num_env_steps_sampled: 187250\n",
            "num_env_steps_sampled_this_iter: 450\n",
            "num_env_steps_sampled_throughput_per_sec: 424.3844757546371\n",
            "num_env_steps_trained: 118880\n",
            "num_env_steps_trained_this_iter: 288\n",
            "num_env_steps_trained_throughput_per_sec: 271.6060644829677\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 288\n",
            "perf:\n",
            "  cpu_util_percent: 56.8\n",
            "  ram_util_percent: 21.5\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2555770460450201\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17847731080786874\n",
            "  mean_inference_ms: 1.2773978322141095\n",
            "  mean_raw_obs_processing_ms: 0.36616233772350176\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006674528121948242\n",
            "    StateBufferConnector_ms: 0.004946231842041016\n",
            "    ViewRequirementAgentConnector_ms: 0.13057684898376465\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -7.494713511935938\n",
            "  episode_reward_mean: -13.828806497686394\n",
            "  episode_reward_min: -25.131358270535983\n",
            "  episodes_this_iter: 9\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-14.410247032691405, -13.907524002295625, -10.942415536969854,\n",
            "      -7.494713511935938, -11.327591801455982, -12.335075489596791, -10.7559021248706,\n",
            "      -11.573982614621317, -7.768217495921097, -14.646346698979436, -11.565832496338132,\n",
            "      -12.599258405032321, -13.937999786108426, -8.695149631906453, -9.695342337818554,\n",
            "      -10.51615949535363, -14.74851655527017, -10.679266720574677, -12.074538371261678,\n",
            "      -11.081020212110241, -10.925130396531353, -14.04570765821148, -11.804405236005765,\n",
            "      -14.402270786962982, -8.607784233073227, -10.463485862717148, -7.855281077299433,\n",
            "      -8.49259483748229, -11.80410971446314, -8.712235144825994, -8.526305236561203,\n",
            "      -12.764316900048387, -12.832345079106574, -14.767830427903197, -11.002082099570954,\n",
            "      -10.80039839503756, -13.120385125251268, -14.926260563116768, -11.008792190589409,\n",
            "      -12.214165256820399, -13.70192092472168, -13.636909134964915, -12.215434856204535,\n",
            "      -13.546043556184435, -11.31315177013072, -10.045715027393136, -13.743272042495654,\n",
            "      -15.198364958709481, -10.670986585640316, -15.594481517749733, -11.589679381334209,\n",
            "      -10.142360894904348, -12.29459332434907, -16.297531580042055, -10.613150277633306,\n",
            "      -14.546847819571406, -14.928020843080681, -14.87214135978341, -18.451742149872533,\n",
            "      -18.323653736774904, -20.26580241822034, -20.34953820313229, -19.388936223573953,\n",
            "      -22.841573286392208, -18.645616195213478, -25.131358270535983, -15.712139013860149,\n",
            "      -17.42463623780536, -16.676693780523774, -19.254531406474197, -16.085869703164743,\n",
            "      -16.13864047235271, -12.03789687203569, -16.099829458156083, -12.95468270484183,\n",
            "      -16.07759398130344, -14.94672569999649, -12.096699034764695, -14.941611166665089,\n",
            "      -18.381354187014598, -16.671833583594672, -18.10613391915888, -17.214462818962808,\n",
            "      -13.79750671099854, -18.468422118012402, -18.109416748855743, -19.001127731959564,\n",
            "      -19.896316196430107, -19.91085709342321, -18.31089548890767, -12.758204886687192,\n",
            "      -13.340392740155277, -17.473703942664443, -12.876385709479447, -15.211782848408903,\n",
            "      -11.67837612210638, -10.048714941426308, -10.308445736044748, -11.539657660092557,\n",
            "      -11.14929617101178]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2555770460450201\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17847731080786874\n",
            "    mean_inference_ms: 1.2773978322141095\n",
            "    mean_raw_obs_processing_ms: 0.36616233772350176\n",
            "time_since_restore: 530.6605055332184\n",
            "time_this_iter_s: 1.0614564418792725\n",
            "time_total_s: 530.6605055332184\n",
            "timers:\n",
            "  learn_throughput: 2210.593\n",
            "  learn_time_ms: 14.476\n",
            "  load_throughput: 169616.742\n",
            "  load_time_ms: 0.189\n",
            "  sample_time_ms: 87.945\n",
            "  synch_weights_time_ms: 0.026\n",
            "  training_iteration_time_ms: 119.057\n",
            "timestamp: 1703177907\n",
            "timesteps_total: 187250\n",
            "training_iteration: 496\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 187650\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006556272506713867\n",
            "  StateBufferConnector_ms: 0.004861593246459961\n",
            "  ViewRequirementAgentConnector_ms: 0.1285543441772461\n",
            "counters:\n",
            "  last_target_update_ts: 187650\n",
            "  num_agent_steps_sampled: 187650\n",
            "  num_agent_steps_trained: 119136\n",
            "  num_env_steps_sampled: 187650\n",
            "  num_env_steps_trained: 119136\n",
            "  num_target_updates: 3723\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-28\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -7.768217495921097\n",
            "episode_reward_mean: -13.892266983501315\n",
            "episode_reward_min: -25.131358270535983\n",
            "episodes_this_iter: 8\n",
            "episodes_total: 3753\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 187650\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1727.65625\n",
            "      learner_stats:\n",
            "        actor_loss: -21.259824752807617\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.2388631105422974\n",
            "        alpha_value: 0.6907403469085693\n",
            "        critic_loss: 0.04051484167575836\n",
            "        grad_gnorm: 3.3483574390411377\n",
            "        log_alpha_value: -0.3699913024902344\n",
            "        max_q: 23.520029067993164\n",
            "        mean_q: 20.558666229248047\n",
            "        min_q: 17.0228214263916\n",
            "        policy_t: -0.08269033581018448\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.211544007062912\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3723.0\n",
            "      td_error: [0.08785820007324219, 0.08979606628417969, 0.009191513061523438, 0.13653182983398438,\n",
            "        0.22950458526611328, 0.5301189422607422, 0.8680133819580078, 0.3236055374145508,\n",
            "        0.08893394470214844, 0.060565948486328125, 0.37077808380126953, 0.07260608673095703,\n",
            "        0.2533102035522461, 0.3270835876464844, 0.20648860931396484, 0.3903026580810547,\n",
            "        0.07891368865966797, 0.1645069122314453, 0.2557840347290039, 0.09873771667480469,\n",
            "        0.1519012451171875, 0.09304046630859375, 0.1359424591064453, 0.21799659729003906,\n",
            "        0.17988109588623047, 0.28069496154785156, 0.30809879302978516, 0.3086671829223633,\n",
            "        0.14626407623291016, 0.030597686767578125, 0.2158031463623047, 0.05788898468017578]\n",
            "  num_agent_steps_sampled: 187650\n",
            "  num_agent_steps_trained: 119136\n",
            "  num_env_steps_sampled: 187650\n",
            "  num_env_steps_trained: 119136\n",
            "  num_target_updates: 3723\n",
            "iterations_since_restore: 497\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 187650\n",
            "num_agent_steps_trained: 119136\n",
            "num_env_steps_sampled: 187650\n",
            "num_env_steps_sampled_this_iter: 400\n",
            "num_env_steps_sampled_throughput_per_sec: 385.3435811617028\n",
            "num_env_steps_trained: 119136\n",
            "num_env_steps_trained_this_iter: 256\n",
            "num_env_steps_trained_throughput_per_sec: 246.6198919434898\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 256\n",
            "perf:\n",
            "  cpu_util_percent: 54.150000000000006\n",
            "  ram_util_percent: 21.5\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25554491791163203\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.1784587085548025\n",
            "  mean_inference_ms: 1.2771365561890788\n",
            "  mean_raw_obs_processing_ms: 0.3660892717819793\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006556272506713867\n",
            "    StateBufferConnector_ms: 0.004861593246459961\n",
            "    ViewRequirementAgentConnector_ms: 0.1285543441772461\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -7.768217495921097\n",
            "  episode_reward_mean: -13.892266983501315\n",
            "  episode_reward_min: -25.131358270535983\n",
            "  episodes_this_iter: 8\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-7.768217495921097, -14.646346698979436, -11.565832496338132,\n",
            "      -12.599258405032321, -13.937999786108426, -8.695149631906453, -9.695342337818554,\n",
            "      -10.51615949535363, -14.74851655527017, -10.679266720574677, -12.074538371261678,\n",
            "      -11.081020212110241, -10.925130396531353, -14.04570765821148, -11.804405236005765,\n",
            "      -14.402270786962982, -8.607784233073227, -10.463485862717148, -7.855281077299433,\n",
            "      -8.49259483748229, -11.80410971446314, -8.712235144825994, -8.526305236561203,\n",
            "      -12.764316900048387, -12.832345079106574, -14.767830427903197, -11.002082099570954,\n",
            "      -10.80039839503756, -13.120385125251268, -14.926260563116768, -11.008792190589409,\n",
            "      -12.214165256820399, -13.70192092472168, -13.636909134964915, -12.215434856204535,\n",
            "      -13.546043556184435, -11.31315177013072, -10.045715027393136, -13.743272042495654,\n",
            "      -15.198364958709481, -10.670986585640316, -15.594481517749733, -11.589679381334209,\n",
            "      -10.142360894904348, -12.29459332434907, -16.297531580042055, -10.613150277633306,\n",
            "      -14.546847819571406, -14.928020843080681, -14.87214135978341, -18.451742149872533,\n",
            "      -18.323653736774904, -20.26580241822034, -20.34953820313229, -19.388936223573953,\n",
            "      -22.841573286392208, -18.645616195213478, -25.131358270535983, -15.712139013860149,\n",
            "      -17.42463623780536, -16.676693780523774, -19.254531406474197, -16.085869703164743,\n",
            "      -16.13864047235271, -12.03789687203569, -16.099829458156083, -12.95468270484183,\n",
            "      -16.07759398130344, -14.94672569999649, -12.096699034764695, -14.941611166665089,\n",
            "      -18.381354187014598, -16.671833583594672, -18.10613391915888, -17.214462818962808,\n",
            "      -13.79750671099854, -18.468422118012402, -18.109416748855743, -19.001127731959564,\n",
            "      -19.896316196430107, -19.91085709342321, -18.31089548890767, -12.758204886687192,\n",
            "      -13.340392740155277, -17.473703942664443, -12.876385709479447, -15.211782848408903,\n",
            "      -11.67837612210638, -10.048714941426308, -10.308445736044748, -11.539657660092557,\n",
            "      -11.14929617101178, -13.47352614311644, -9.938403618336626, -13.652619296352027,\n",
            "      -16.837592276396666, -9.388907355951929, -11.342797686701582, -10.271371948603479,\n",
            "      -14.188282370471459]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25554491791163203\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1784587085548025\n",
            "    mean_inference_ms: 1.2771365561890788\n",
            "    mean_raw_obs_processing_ms: 0.3660892717819793\n",
            "time_since_restore: 531.699827671051\n",
            "time_this_iter_s: 1.0393221378326416\n",
            "time_total_s: 531.699827671051\n",
            "timers:\n",
            "  learn_throughput: 2132.722\n",
            "  learn_time_ms: 15.004\n",
            "  load_throughput: 168108.377\n",
            "  load_time_ms: 0.19\n",
            "  sample_time_ms: 95.573\n",
            "  synch_weights_time_ms: 0.029\n",
            "  training_iteration_time_ms: 127.937\n",
            "timestamp: 1703177908\n",
            "timesteps_total: 187650\n",
            "training_iteration: 497\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 187950\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.0063669681549072266\n",
            "  StateBufferConnector_ms: 0.004875659942626953\n",
            "  ViewRequirementAgentConnector_ms: 0.13168692588806152\n",
            "counters:\n",
            "  last_target_update_ts: 187950\n",
            "  num_agent_steps_sampled: 187950\n",
            "  num_agent_steps_trained: 119328\n",
            "  num_env_steps_sampled: 187950\n",
            "  num_env_steps_trained: 119328\n",
            "  num_target_updates: 3729\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-29\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -7.855281077299433\n",
            "episode_reward_mean: -14.1874895122235\n",
            "episode_reward_min: -25.131358270535983\n",
            "episodes_this_iter: 6\n",
            "episodes_total: 3759\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 187950\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1611.78125\n",
            "      learner_stats:\n",
            "        actor_loss: -21.443424224853516\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.2225407361984253\n",
            "        alpha_value: 0.6903281211853027\n",
            "        critic_loss: 0.07101914286613464\n",
            "        grad_gnorm: 3.2989189624786377\n",
            "        log_alpha_value: -0.3705882728099823\n",
            "        max_q: 23.834997177124023\n",
            "        mean_q: 20.7890567779541\n",
            "        min_q: 18.335662841796875\n",
            "        policy_t: -0.11257427930831909\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.2417588233947754\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3729.0\n",
            "      td_error: [0.1423330307006836, 0.6059532165527344, 0.1397409439086914, 0.07049274444580078,\n",
            "        0.5392303466796875, 0.1824512481689453, 0.16587352752685547, 0.07911014556884766,\n",
            "        0.06385993957519531, 0.18151569366455078, 0.04521751403808594, 1.1612377166748047,\n",
            "        0.06011390686035156, 0.10122108459472656, 1.1158723831176758, 0.1564197540283203,\n",
            "        0.1339282989501953, 0.13466739654541016, 0.1397838592529297, 0.6811561584472656,\n",
            "        0.12373828887939453, 0.08926200866699219, 0.21206951141357422, 0.1944904327392578,\n",
            "        0.036665916442871094, 0.07361698150634766, 0.08865928649902344, 0.0659646987915039,\n",
            "        0.09383487701416016, 0.2887277603149414, 0.46673011779785156, 0.10234355926513672]\n",
            "  num_agent_steps_sampled: 187950\n",
            "  num_agent_steps_trained: 119328\n",
            "  num_env_steps_sampled: 187950\n",
            "  num_env_steps_trained: 119328\n",
            "  num_target_updates: 3729\n",
            "iterations_since_restore: 498\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 187950\n",
            "num_agent_steps_trained: 119328\n",
            "num_env_steps_sampled: 187950\n",
            "num_env_steps_sampled_this_iter: 300\n",
            "num_env_steps_sampled_throughput_per_sec: 293.09979601900466\n",
            "num_env_steps_trained: 119328\n",
            "num_env_steps_trained_this_iter: 192\n",
            "num_env_steps_trained_throughput_per_sec: 187.583869452163\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 192\n",
            "perf:\n",
            "  cpu_util_percent: 87.8\n",
            "  ram_util_percent: 21.7\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25552154079515466\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17844279773361904\n",
            "  mean_inference_ms: 1.276941894410216\n",
            "  mean_raw_obs_processing_ms: 0.36603210220565413\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.0063669681549072266\n",
            "    StateBufferConnector_ms: 0.004875659942626953\n",
            "    ViewRequirementAgentConnector_ms: 0.13168692588806152\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -7.855281077299433\n",
            "  episode_reward_mean: -14.1874895122235\n",
            "  episode_reward_min: -25.131358270535983\n",
            "  episodes_this_iter: 6\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-9.695342337818554, -10.51615949535363, -14.74851655527017, -10.679266720574677,\n",
            "      -12.074538371261678, -11.081020212110241, -10.925130396531353, -14.04570765821148,\n",
            "      -11.804405236005765, -14.402270786962982, -8.607784233073227, -10.463485862717148,\n",
            "      -7.855281077299433, -8.49259483748229, -11.80410971446314, -8.712235144825994,\n",
            "      -8.526305236561203, -12.764316900048387, -12.832345079106574, -14.767830427903197,\n",
            "      -11.002082099570954, -10.80039839503756, -13.120385125251268, -14.926260563116768,\n",
            "      -11.008792190589409, -12.214165256820399, -13.70192092472168, -13.636909134964915,\n",
            "      -12.215434856204535, -13.546043556184435, -11.31315177013072, -10.045715027393136,\n",
            "      -13.743272042495654, -15.198364958709481, -10.670986585640316, -15.594481517749733,\n",
            "      -11.589679381334209, -10.142360894904348, -12.29459332434907, -16.297531580042055,\n",
            "      -10.613150277633306, -14.546847819571406, -14.928020843080681, -14.87214135978341,\n",
            "      -18.451742149872533, -18.323653736774904, -20.26580241822034, -20.34953820313229,\n",
            "      -19.388936223573953, -22.841573286392208, -18.645616195213478, -25.131358270535983,\n",
            "      -15.712139013860149, -17.42463623780536, -16.676693780523774, -19.254531406474197,\n",
            "      -16.085869703164743, -16.13864047235271, -12.03789687203569, -16.099829458156083,\n",
            "      -12.95468270484183, -16.07759398130344, -14.94672569999649, -12.096699034764695,\n",
            "      -14.941611166665089, -18.381354187014598, -16.671833583594672, -18.10613391915888,\n",
            "      -17.214462818962808, -13.79750671099854, -18.468422118012402, -18.109416748855743,\n",
            "      -19.001127731959564, -19.896316196430107, -19.91085709342321, -18.31089548890767,\n",
            "      -12.758204886687192, -13.340392740155277, -17.473703942664443, -12.876385709479447,\n",
            "      -15.211782848408903, -11.67837612210638, -10.048714941426308, -10.308445736044748,\n",
            "      -11.539657660092557, -11.14929617101178, -13.47352614311644, -9.938403618336626,\n",
            "      -13.652619296352027, -16.837592276396666, -9.388907355951929, -11.342797686701582,\n",
            "      -10.271371948603479, -14.188282370471459, -13.976623926392133, -16.003506751405325,\n",
            "      -15.56504397848384, -18.760319029437248, -17.56425598637558, -16.86530771441015]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25552154079515466\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17844279773361904\n",
            "    mean_inference_ms: 1.276941894410216\n",
            "    mean_raw_obs_processing_ms: 0.36603210220565413\n",
            "time_since_restore: 532.724960565567\n",
            "time_this_iter_s: 1.0251328945159912\n",
            "time_total_s: 532.724960565567\n",
            "timers:\n",
            "  learn_throughput: 1704.794\n",
            "  learn_time_ms: 18.771\n",
            "  load_throughput: 128401.156\n",
            "  load_time_ms: 0.249\n",
            "  sample_time_ms: 115.009\n",
            "  synch_weights_time_ms: 0.03\n",
            "  training_iteration_time_ms: 157.421\n",
            "timestamp: 1703177909\n",
            "timesteps_total: 187950\n",
            "training_iteration: 498\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 188300\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006346940994262695\n",
            "  StateBufferConnector_ms: 0.004883527755737305\n",
            "  ViewRequirementAgentConnector_ms: 0.13265633583068848\n",
            "counters:\n",
            "  last_target_update_ts: 188300\n",
            "  num_agent_steps_sampled: 188300\n",
            "  num_agent_steps_trained: 119552\n",
            "  num_env_steps_sampled: 188300\n",
            "  num_env_steps_trained: 119552\n",
            "  num_target_updates: 3736\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-31\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -7.855281077299433\n",
            "episode_reward_mean: -14.3410627394334\n",
            "episode_reward_min: -25.131358270535983\n",
            "episodes_this_iter: 7\n",
            "episodes_total: 3766\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 188300\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 2093.40625\n",
            "      learner_stats:\n",
            "        actor_loss: -21.112628936767578\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.2015445232391357\n",
            "        alpha_value: 0.6898542642593384\n",
            "        critic_loss: 0.03019586391746998\n",
            "        grad_gnorm: 3.2362661361694336\n",
            "        log_alpha_value: -0.3712749183177948\n",
            "        max_q: 24.854711532592773\n",
            "        mean_q: 20.39613151550293\n",
            "        min_q: 17.05975914001465\n",
            "        policy_t: -0.1613137573003769\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.19673702120780945\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3736.0\n",
            "      td_error: [0.15321922302246094, 0.05402088165283203, 0.1115875244140625, 0.40457916259765625,\n",
            "        0.3849153518676758, 0.068115234375, 0.05232715606689453, 0.2188405990600586,\n",
            "        0.2996397018432617, 0.08026313781738281, 0.2866535186767578, 0.2076272964477539,\n",
            "        0.2226724624633789, 0.09753227233886719, 0.0976715087890625, 0.23702621459960938,\n",
            "        0.2160472869873047, 0.5143146514892578, 0.05808544158935547, 0.21660709381103516,\n",
            "        0.16141796112060547, 0.04834556579589844, 0.23712825775146484, 0.08640670776367188,\n",
            "        0.11997032165527344, 0.18731307983398438, 0.27100086212158203, 0.1414203643798828,\n",
            "        0.03694629669189453, 0.22260665893554688, 0.15781021118164062, 0.6434726715087891]\n",
            "  num_agent_steps_sampled: 188300\n",
            "  num_agent_steps_trained: 119552\n",
            "  num_env_steps_sampled: 188300\n",
            "  num_env_steps_trained: 119552\n",
            "  num_target_updates: 3736\n",
            "iterations_since_restore: 499\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 188300\n",
            "num_agent_steps_trained: 119552\n",
            "num_env_steps_sampled: 188300\n",
            "num_env_steps_sampled_this_iter: 350\n",
            "num_env_steps_sampled_throughput_per_sec: 303.9897654464126\n",
            "num_env_steps_trained: 119552\n",
            "num_env_steps_trained_this_iter: 224\n",
            "num_env_steps_trained_throughput_per_sec: 194.55344988570408\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 224\n",
            "perf:\n",
            "  cpu_util_percent: 99.65\n",
            "  ram_util_percent: 21.7\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.25549495323481985\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.17842409829914083\n",
            "  mean_inference_ms: 1.2767156412834113\n",
            "  mean_raw_obs_processing_ms: 0.3659679481414298\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006346940994262695\n",
            "    StateBufferConnector_ms: 0.004883527755737305\n",
            "    ViewRequirementAgentConnector_ms: 0.13265633583068848\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -7.855281077299433\n",
            "  episode_reward_mean: -14.3410627394334\n",
            "  episode_reward_min: -25.131358270535983\n",
            "  episodes_this_iter: 7\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-14.04570765821148, -11.804405236005765, -14.402270786962982,\n",
            "      -8.607784233073227, -10.463485862717148, -7.855281077299433, -8.49259483748229,\n",
            "      -11.80410971446314, -8.712235144825994, -8.526305236561203, -12.764316900048387,\n",
            "      -12.832345079106574, -14.767830427903197, -11.002082099570954, -10.80039839503756,\n",
            "      -13.120385125251268, -14.926260563116768, -11.008792190589409, -12.214165256820399,\n",
            "      -13.70192092472168, -13.636909134964915, -12.215434856204535, -13.546043556184435,\n",
            "      -11.31315177013072, -10.045715027393136, -13.743272042495654, -15.198364958709481,\n",
            "      -10.670986585640316, -15.594481517749733, -11.589679381334209, -10.142360894904348,\n",
            "      -12.29459332434907, -16.297531580042055, -10.613150277633306, -14.546847819571406,\n",
            "      -14.928020843080681, -14.87214135978341, -18.451742149872533, -18.323653736774904,\n",
            "      -20.26580241822034, -20.34953820313229, -19.388936223573953, -22.841573286392208,\n",
            "      -18.645616195213478, -25.131358270535983, -15.712139013860149, -17.42463623780536,\n",
            "      -16.676693780523774, -19.254531406474197, -16.085869703164743, -16.13864047235271,\n",
            "      -12.03789687203569, -16.099829458156083, -12.95468270484183, -16.07759398130344,\n",
            "      -14.94672569999649, -12.096699034764695, -14.941611166665089, -18.381354187014598,\n",
            "      -16.671833583594672, -18.10613391915888, -17.214462818962808, -13.79750671099854,\n",
            "      -18.468422118012402, -18.109416748855743, -19.001127731959564, -19.896316196430107,\n",
            "      -19.91085709342321, -18.31089548890767, -12.758204886687192, -13.340392740155277,\n",
            "      -17.473703942664443, -12.876385709479447, -15.211782848408903, -11.67837612210638,\n",
            "      -10.048714941426308, -10.308445736044748, -11.539657660092557, -11.14929617101178,\n",
            "      -13.47352614311644, -9.938403618336626, -13.652619296352027, -16.837592276396666,\n",
            "      -9.388907355951929, -11.342797686701582, -10.271371948603479, -14.188282370471459,\n",
            "      -13.976623926392133, -16.003506751405325, -15.56504397848384, -18.760319029437248,\n",
            "      -17.56425598637558, -16.86530771441015, -13.499945476791272, -12.709822312541258,\n",
            "      -13.987826942259273, -14.238097718678398, -12.033596991049004, -14.109377117877578,\n",
            "      -14.498630250713463]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.25549495323481985\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.17842409829914083\n",
            "    mean_inference_ms: 1.2767156412834113\n",
            "    mean_raw_obs_processing_ms: 0.3659679481414298\n",
            "time_since_restore: 533.8778455257416\n",
            "time_this_iter_s: 1.1528849601745605\n",
            "time_total_s: 533.8778455257416\n",
            "timers:\n",
            "  learn_throughput: 1728.071\n",
            "  learn_time_ms: 18.518\n",
            "  load_throughput: 123214.659\n",
            "  load_time_ms: 0.26\n",
            "  sample_time_ms: 119.642\n",
            "  synch_weights_time_ms: 0.03\n",
            "  training_iteration_time_ms: 163.807\n",
            "timestamp: 1703177911\n",
            "timesteps_total: 188300\n",
            "training_iteration: 499\n",
            "trial_id: default\n",
            "\n",
            "agent_timesteps_total: 188600\n",
            "connector_metrics:\n",
            "  ObsPreprocessorConnector_ms: 0.006334066390991211\n",
            "  StateBufferConnector_ms: 0.004891395568847656\n",
            "  ViewRequirementAgentConnector_ms: 0.1314709186553955\n",
            "counters:\n",
            "  last_target_update_ts: 188600\n",
            "  num_agent_steps_sampled: 188600\n",
            "  num_agent_steps_trained: 119744\n",
            "  num_env_steps_sampled: 188600\n",
            "  num_env_steps_trained: 119744\n",
            "  num_target_updates: 3742\n",
            "custom_metrics: {}\n",
            "date: 2023-12-21_16-58-32\n",
            "done: false\n",
            "episode_len_mean: 50.0\n",
            "episode_media: {}\n",
            "episode_reward_max: -8.49259483748229\n",
            "episode_reward_mean: -14.534371470891\n",
            "episode_reward_min: -25.131358270535983\n",
            "episodes_this_iter: 6\n",
            "episodes_total: 3772\n",
            "hostname: 4900223327f7\n",
            "info:\n",
            "  last_target_update_ts: 188600\n",
            "  learner:\n",
            "    default_policy:\n",
            "      custom_metrics: {}\n",
            "      diff_num_grad_updates_vs_sampler_policy: 1838.84375\n",
            "      learner_stats:\n",
            "        actor_loss: -21.219593048095703\n",
            "        allreduce_latency: 0.0\n",
            "        alpha_loss: -1.2233699560165405\n",
            "        alpha_value: 0.6894522905349731\n",
            "        critic_loss: 0.04154714569449425\n",
            "        grad_gnorm: 3.289886713027954\n",
            "        log_alpha_value: -0.37185776233673096\n",
            "        max_q: 23.638755798339844\n",
            "        mean_q: 20.512168884277344\n",
            "        min_q: 16.34865379333496\n",
            "        policy_t: -0.12245235592126846\n",
            "        target_entropy: -2.0\n",
            "      mean_td_error: 0.17675963044166565\n",
            "      model: {}\n",
            "      num_agent_steps_trained: 32.0\n",
            "      num_grad_updates_lifetime: 3742.0\n",
            "      td_error: [0.0915975570678711, 0.13893890380859375, 0.0654611587524414, 0.04379463195800781,\n",
            "        0.08953285217285156, 0.1542806625366211, 0.1576404571533203, 0.2432565689086914,\n",
            "        0.07642078399658203, 0.27353954315185547, 0.05898284912109375, 0.20685768127441406,\n",
            "        0.04409217834472656, 0.03446388244628906, 0.10618305206298828, 0.15145206451416016,\n",
            "        0.05474090576171875, 0.31597328186035156, 0.1217508316040039, 0.01700592041015625,\n",
            "        0.062274932861328125, 1.2070817947387695, 0.13727188110351562, 0.028525352478027344,\n",
            "        0.03676414489746094, 0.07012176513671875, 0.6647500991821289, 0.3069114685058594,\n",
            "        0.3197965621948242, 0.2623758316040039, 0.07521915435791016, 0.039249420166015625]\n",
            "  num_agent_steps_sampled: 188600\n",
            "  num_agent_steps_trained: 119744\n",
            "  num_env_steps_sampled: 188600\n",
            "  num_env_steps_trained: 119744\n",
            "  num_target_updates: 3742\n",
            "iterations_since_restore: 500\n",
            "node_ip: 172.28.0.12\n",
            "num_agent_steps_sampled: 188600\n",
            "num_agent_steps_trained: 119744\n",
            "num_env_steps_sampled: 188600\n",
            "num_env_steps_sampled_this_iter: 300\n",
            "num_env_steps_sampled_throughput_per_sec: 298.0034364301861\n",
            "num_env_steps_trained: 119744\n",
            "num_env_steps_trained_this_iter: 192\n",
            "num_env_steps_trained_throughput_per_sec: 190.7221993153191\n",
            "num_faulty_episodes: 0\n",
            "num_healthy_workers: 0\n",
            "num_in_flight_async_reqs: 0\n",
            "num_remote_worker_restarts: 0\n",
            "num_steps_trained_this_iter: 192\n",
            "perf:\n",
            "  cpu_util_percent: 99.3\n",
            "  ram_util_percent: 21.65\n",
            "pid: 1644\n",
            "policy_reward_max: {}\n",
            "policy_reward_mean: {}\n",
            "policy_reward_min: {}\n",
            "sampler_perf:\n",
            "  mean_action_processing_ms: 0.2554714287710461\n",
            "  mean_env_render_ms: 0.0\n",
            "  mean_env_wait_ms: 0.1784025656247876\n",
            "  mean_inference_ms: 1.2765243432546185\n",
            "  mean_raw_obs_processing_ms: 0.365908762552979\n",
            "sampler_results:\n",
            "  connector_metrics:\n",
            "    ObsPreprocessorConnector_ms: 0.006334066390991211\n",
            "    StateBufferConnector_ms: 0.004891395568847656\n",
            "    ViewRequirementAgentConnector_ms: 0.1314709186553955\n",
            "  custom_metrics: {}\n",
            "  episode_len_mean: 50.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: -8.49259483748229\n",
            "  episode_reward_mean: -14.534371470891\n",
            "  episode_reward_min: -25.131358270535983\n",
            "  episodes_this_iter: 6\n",
            "  hist_stats:\n",
            "    episode_lengths: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
            "      50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
            "    episode_reward: [-8.49259483748229, -11.80410971446314, -8.712235144825994, -8.526305236561203,\n",
            "      -12.764316900048387, -12.832345079106574, -14.767830427903197, -11.002082099570954,\n",
            "      -10.80039839503756, -13.120385125251268, -14.926260563116768, -11.008792190589409,\n",
            "      -12.214165256820399, -13.70192092472168, -13.636909134964915, -12.215434856204535,\n",
            "      -13.546043556184435, -11.31315177013072, -10.045715027393136, -13.743272042495654,\n",
            "      -15.198364958709481, -10.670986585640316, -15.594481517749733, -11.589679381334209,\n",
            "      -10.142360894904348, -12.29459332434907, -16.297531580042055, -10.613150277633306,\n",
            "      -14.546847819571406, -14.928020843080681, -14.87214135978341, -18.451742149872533,\n",
            "      -18.323653736774904, -20.26580241822034, -20.34953820313229, -19.388936223573953,\n",
            "      -22.841573286392208, -18.645616195213478, -25.131358270535983, -15.712139013860149,\n",
            "      -17.42463623780536, -16.676693780523774, -19.254531406474197, -16.085869703164743,\n",
            "      -16.13864047235271, -12.03789687203569, -16.099829458156083, -12.95468270484183,\n",
            "      -16.07759398130344, -14.94672569999649, -12.096699034764695, -14.941611166665089,\n",
            "      -18.381354187014598, -16.671833583594672, -18.10613391915888, -17.214462818962808,\n",
            "      -13.79750671099854, -18.468422118012402, -18.109416748855743, -19.001127731959564,\n",
            "      -19.896316196430107, -19.91085709342321, -18.31089548890767, -12.758204886687192,\n",
            "      -13.340392740155277, -17.473703942664443, -12.876385709479447, -15.211782848408903,\n",
            "      -11.67837612210638, -10.048714941426308, -10.308445736044748, -11.539657660092557,\n",
            "      -11.14929617101178, -13.47352614311644, -9.938403618336626, -13.652619296352027,\n",
            "      -16.837592276396666, -9.388907355951929, -11.342797686701582, -10.271371948603479,\n",
            "      -14.188282370471459, -13.976623926392133, -16.003506751405325, -15.56504397848384,\n",
            "      -18.760319029437248, -17.56425598637558, -16.86530771441015, -13.499945476791272,\n",
            "      -12.709822312541258, -13.987826942259273, -14.238097718678398, -12.033596991049004,\n",
            "      -14.109377117877578, -14.498630250713463, -17.087624109905015, -16.558516840604895,\n",
            "      -12.364353758427413, -9.137515800745406, -15.910417127039869, -15.45138036330708]\n",
            "  num_faulty_episodes: 0\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.2554714287710461\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1784025656247876\n",
            "    mean_inference_ms: 1.2765243432546185\n",
            "    mean_raw_obs_processing_ms: 0.365908762552979\n",
            "time_since_restore: 534.8861017227173\n",
            "time_this_iter_s: 1.008256196975708\n",
            "time_total_s: 534.8861017227173\n",
            "timers:\n",
            "  learn_throughput: 1703.217\n",
            "  learn_time_ms: 18.788\n",
            "  load_throughput: 120277.559\n",
            "  load_time_ms: 0.266\n",
            "  sample_time_ms: 122.657\n",
            "  synch_weights_time_ms: 0.033\n",
            "  training_iteration_time_ms: 167.436\n",
            "timestamp: 1703177912\n",
            "timesteps_total: 188600\n",
            "training_iteration: 500\n",
            "trial_id: default\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import ray\n",
        "from ray.rllib.algorithms.sac import SAC\n",
        "from ray.tune.logger import pretty_print\n",
        "\n",
        "def get_max_action(env_name):\n",
        "    import gym\n",
        "    env = gym.make(env_name)\n",
        "    return env.action_space.high[0]\n",
        "\n",
        "max_action = get_max_action(\"Reacher-v4\")\n",
        "\n",
        "config_dict_1 = {\n",
        "    \"env\": \"Reacher-v4\",\n",
        "    \"framework\": \"torch\",\n",
        "    \"num_gpus\": 1,\n",
        "    \"num_workers\": 0,\n",
        "    \"algorithm\": \"SAC\",\n",
        "    \"use_automatic_entropy_tuning\": True,\n",
        "    \"target_entropy\": \"auto\",\n",
        "    \"model\": {\n",
        "        \"fcnet_hiddens\": [256, 256],\n",
        "        \"fcnet_activation\": \"relu\",\n",
        "    },\n",
        "    \"optimization\": {\n",
        "        \"actor_learning_rate\": 0.0001,\n",
        "        \"critic_learning_rate\": 0.0001,\n",
        "        \"entropy_learning_rate\": 0.0003,\n",
        "        \"learning_starts\": 1000,\n",
        "    },\n",
        "    \"exploration_config\": {\n",
        "        \"type\": \"OrnsteinUhlenbeckNoise\",\n",
        "        \"ou_base_scale\": 0.5,\n",
        "        \"ou_theta\": 0.15,\n",
        "        \"ou_sigma\": 0.2,\n",
        "    },\n",
        "    \"replay_buffer_config\": {\n",
        "        \"capacity\": 1000000,\n",
        "        \"learning_starts\": 1000,\n",
        "        \"type\": \"MultiAgentReplayBuffer\",\n",
        "    },\n",
        "    \"target_network_update_freq\": 5,\n",
        "    \"tau\": 0.02,\n",
        "    \"gamma\": 0.95,\n",
        "    \"clip_actions\": True,\n",
        "    \"train_batch_size\": 128,\n",
        "    \"rollout_fragment_length\": 1,\n",
        "    \"batch_mode\": \"complete_episodes\",\n",
        "    \"stop\": {\n",
        "        \"training_iteration\": 10000,\n",
        "    },\n",
        "}\n",
        "\n",
        "config_dict_2 = {\n",
        "    \"env\": \"Reacher-v4\",\n",
        "    \"framework\": \"torch\",\n",
        "    \"num_gpus\": 0,\n",
        "    \"num_workers\": 0,\n",
        "    \"algorithm\": \"SAC\",\n",
        "    \"use_automatic_entropy_tuning\": False,\n",
        "    \"target_entropy\": -2,\n",
        "    \"model\": {\n",
        "        \"fcnet_hiddens\": [64, 64],\n",
        "        \"fcnet_activation\": \"tanh\",\n",
        "    },\n",
        "    \"optimization\": {\n",
        "        \"actor_learning_rate\": 0.0005,\n",
        "        \"critic_learning_rate\": 0.0005,\n",
        "        \"entropy_learning_rate\": 0.0001,\n",
        "        \"learning_starts\": 2000,\n",
        "    },\n",
        "    \"exploration_config\": {\n",
        "        \"type\": \"GaussianNoise\",\n",
        "        \"stddev\": 0.1,\n",
        "    },\n",
        "    \"replay_buffer_config\": {\n",
        "        \"capacity\": 500000,\n",
        "        \"learning_starts\": 2000,\n",
        "        \"type\": \"MultiAgentReplayBuffer\",\n",
        "    },\n",
        "    \"target_network_update_freq\": 20,\n",
        "    \"tau\": 0.01,\n",
        "    \"gamma\": 0.99,\n",
        "    \"clip_actions\": True,\n",
        "    \"train_batch_size\": 32,\n",
        "    \"rollout_fragment_length\": 1,\n",
        "    \"batch_mode\": \"complete_episodes\",\n",
        "    \"stop\": {\n",
        "        \"training_iteration\": 15000,\n",
        "    },\n",
        "}\n",
        "\n",
        "config_dict_3 = {\n",
        "    \"env\": \"Reacher-v4\",\n",
        "    \"framework\": \"torch\",\n",
        "    \"num_gpus\": 0,\n",
        "    \"num_workers\": 0,\n",
        "    \"algorithm\": \"SAC\",\n",
        "    \"use_automatic_entropy_tuning\": True,\n",
        "    \"target_entropy\": \"auto\",\n",
        "    \"model\": {\n",
        "        \"fcnet_hiddens\": [128, 128],\n",
        "        \"fcnet_activation\": \"relu\",\n",
        "    },\n",
        "    \"optimization\": {\n",
        "        \"actor_learning_rate\": 0.0005,\n",
        "        \"critic_learning_rate\": 0.0005,\n",
        "        \"entropy_learning_rate\": 0.0005,\n",
        "        \"learning_starts\": 500,\n",
        "    },\n",
        "    \"exploration_config\": {\n",
        "        \"type\": \"OrnsteinUhlenbeckNoise\",\n",
        "        \"ou_base_scale\": 0.3,\n",
        "        \"ou_theta\": 0.45,\n",
        "        \"ou_sigma\": 0.3,\n",
        "    },\n",
        "    \"replay_buffer_config\": {\n",
        "        \"capacity\": 500000,\n",
        "        \"learning_starts\": 500,\n",
        "        \"type\": \"MultiAgentReplayBuffer\",\n",
        "    },\n",
        "    \"target_network_update_freq\": 5,\n",
        "    \"tau\": 0.01,\n",
        "    \"gamma\": 0.95,\n",
        "    \"clip_actions\": True,\n",
        "    \"train_batch_size\": 256,\n",
        "    \"rollout_fragment_length\": 1,\n",
        "    \"batch_mode\": \"complete_episodes\",\n",
        "    \"stop\": {\n",
        "        \"training_iteration\": 10000,\n",
        "    },\n",
        "}\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "class CustomReacherEnv(gym.Wrapper):\n",
        "    def __init__(self, env_name):\n",
        "        super(CustomReacherEnv, self).__init__(gym.make(env_name))\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "\n",
        "        # Modify the reward using a custom function\n",
        "        modified_reward = self.custom_reward_function(obs, reward, info)\n",
        "\n",
        "        return obs, modified_reward, done, info\n",
        "\n",
        "    def custom_reward_function(self, obs, reward, info):\n",
        "        # Example of a custom reward function\n",
        "        # Extract relevant information for reward calculation\n",
        "        distance = np.linalg.norm(obs[-3:-1])  # Distance from fingertip to target\n",
        "        control_cost = np.sum(np.square(action))  # Cost of action magnitude\n",
        "\n",
        "        # Customize these weights as needed\n",
        "        distance_weight = 1.0\n",
        "        control_weight = 0.1\n",
        "\n",
        "        # Calculate the modified reward\n",
        "        modified_reward = -distance_weight * distance - control_weight * control_cost\n",
        "\n",
        "        return modified_reward\n",
        "\n",
        "config_dict_4 = {\n",
        "    \"env\": CustomReacherEnv(\"Reacher-v4\"),\n",
        "    \"framework\": \"torch\",\n",
        "    \"num_gpus\": 0,\n",
        "    \"num_workers\": 0,\n",
        "    \"algorithm\": \"SAC\",\n",
        "    \"use_automatic_entropy_tuning\": True,\n",
        "    \"target_entropy\": \"auto\",\n",
        "    \"model\": {\n",
        "        \"fcnet_hiddens\": [400, 300],  # More complex network\n",
        "        \"fcnet_activation\": \"relu\",\n",
        "    },\n",
        "    \"optimization\": {\n",
        "        \"actor_learning_rate\": 0.001,  # Slightly higher learning rate\n",
        "        \"critic_learning_rate\": 0.001,\n",
        "        \"entropy_learning_rate\": 0.001,\n",
        "        \"learning_starts\": 1500,  # Start learning after more initial exploration\n",
        "    },\n",
        "    \"exploration_config\": {\n",
        "        \"type\": \"OrnsteinUhlenbeckNoise\",\n",
        "        \"ou_base_scale\": 0.2,\n",
        "        \"ou_theta\": 0.3,\n",
        "        \"ou_sigma\": 0.2,\n",
        "    },\n",
        "    \"replay_buffer_config\": {\n",
        "        \"capacity\": 1000000,  # Larger replay buffer\n",
        "        \"learning_starts\": 1500,\n",
        "        \"type\": \"MultiAgentReplayBuffer\",\n",
        "    },\n",
        "    \"target_network_update_freq\": 1,\n",
        "    \"tau\": 0.005,\n",
        "    \"gamma\": 0.99,\n",
        "    \"clip_actions\": True,\n",
        "    \"train_batch_size\": 128,\n",
        "    \"rollout_fragment_length\": 1,\n",
        "    \"batch_mode\": \"complete_episodes\",\n",
        "    \"stop\": {\n",
        "        \"training_iteration\": 10000,\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init()\n",
        "\n",
        "# Create and configure the SAC algorithm\n",
        "sac_trainer = SAC(config=config_dict_2)\n",
        "\n",
        "# Training loop\n",
        "for i in range(500):  # Number of training iterations\n",
        "    result = sac_trainer.train()\n",
        "    print(pretty_print(result))\n",
        "\n",
        "# Cleanup\n",
        "ray.shutdown()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NOFVxmfUdRj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6c172e9-3fce-483b-e39c-12ce2fceaa99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = sac_trainer.save(\"/content\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwLpDpIG9DDM",
        "outputId": "bfdea006-c006-49af-bee9-1798205c9d98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [1 InRelease 2,585 B/110 kB 2%] [Connected to c\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Waiting for headers] [Wa\r                                                                                                    \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r                                                                                                    \r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Waiting for headers]\r                                                                                                \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r                                                                                                \r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:4 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers]\r                                              \rHit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "\r                                              \r0% [Waiting for headers]\r                        \rHit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,326 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,046 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,599 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,305 kB]\n",
            "Fetched 5,509 kB in 2s (2,839 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qlBjTfBngmWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ca07d6-25f8-4079-d7ac-b2b36c1a840c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 7,813 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.5 [28.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.5 [863 kB]\n",
            "Fetched 7,813 kB in 1s (7,716 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 121658 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.5_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.5) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.5_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.5) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.5) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install xvfb --fix-missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QeU9Xbpbo0LK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5cd3e5-47ee-47a6-9a34-faaab9e65fe0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "566GQSo2fLZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7d373a7-3f44-4375-fbad-e16a07a08550"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7a06bf6fe110>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Create a virtual display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VSppwCN6gunv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b7c402a-68c3-427e-f938-3123616d05df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7a06bf6fe110>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import gym\n",
        "import cv2\n",
        "\n",
        "\n",
        "# Load the trained model for testing\n",
        "#algo.restore(checkpoint_path)\n",
        "# Initialize the environment\n",
        "env = gym.make(\"Reacher-v4\")\n",
        "\n",
        "# Setup video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "out = cv2.VideoWriter('Reacher-v4_output_sac.avi', fourcc, 20.0, (500, 500))\n",
        "\n",
        "# Test the agent\n",
        "state = env.reset()\n",
        "for _ in range(100):\n",
        "    action = sac_trainer.compute_single_action(state)\n",
        "    next_state, _, _, _ = env.step(action)\n",
        "    frame = env.render(mode='rgb_array')\n",
        "    frame_resized = cv2.resize(frame, (500, 500))\n",
        "    out.write(cv2.cvtColor(frame_resized, cv2.COLOR_RGB2BGR))\n",
        "    state = next_state\n",
        "\n",
        "# Clean up\n",
        "env.close()\n",
        "out.release()\n",
        "\n",
        "# Close the virtual display\n",
        "virtual_display.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}